{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7ih7e5O6rX_",
    "tags": []
   },
   "source": [
    "# Running InstructLab with a GPU\n",
    "\n",
    "<ul>\n",
    "<li>Contributors: InstructLab team and IBM Research Technology Education team:\n",
    "<li>Questions and support: kochel@us.ibm.com, IBM.Research.JupyterLab@ibm.com\n",
    "<li>Version: 1.0.13\n",
    "<li>Release date: 2025-03-20\n",
    "<li>Compute requirements: Colab with GPU\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJjDTcrmAPVS"
   },
   "source": [
    "# Running this Notebook\n",
    "\n",
    "**IMPORTANT:** This notebook must be run within a Colab GPU runtime. You can check you are running with a GPU by selecting Runtime-> Change Runtime Type and confirming that a GPU Runtime is selected. While this notebook can be started on a free Colab account, the GPUs availabe with a free access do not have sufficient memory to run InstructLab training.\n",
    "\n",
    "You can run this notebook either:\n",
    "- Running All Cells by selecting Runtime->Run all\n",
    "- Cell by cell by selecting the arrow on each code cell and running them sequentially.\n",
    "\n",
    "Once the Configuring Instructlab section has been run, the other sections of this notebook can be repeatedly run on other data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlk9FjFB6rYD",
    "tags": []
   },
   "source": [
    "# Section 1. Configure InstructLab\n",
    "\n",
    "## Step 1.1 Environment Configuration\n",
    "Replicate the ilab data repository containing the pip requirements and data files and run the pip installs that require a reset.\n",
    "\n",
    "**IMPORTANT:** Run the next cell, allow it to complete running, then Restart the session , run the following cell to specify parameters and then you can run the remainder of the notebook.\n",
    "\n",
    "After selecting parameters, the remainder of this notebook can be run either:\n",
    "- Running All Cells by selecting Runtime->Run cell and below\n",
    "- Cell by cell by selecting the arrow on each code cell and running them sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2410,
     "status": "ok",
     "timestamp": 1744450518599,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "rde52-02CfiL",
    "outputId": "7dd1459a-3253-4ebb-8773-937e0e51dba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
      "Requirement already satisfied: psutil==7.0.0 in /usr/local/lib/python3.11/dist-packages (7.0.0)\n",
      "Requirement already satisfied: pillow==10.4.0 in /usr/local/lib/python3.11/dist-packages (10.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Run this cell, then perform the requested Reset\n",
    "import os\n",
    "if not os.path.exists(\"ilab\"):\n",
    "    !git clone https://github.com/KenOcheltree/ilab.git\n",
    "!pip install numpy==1.26.4 torch==2.5.1 psutil==7.0.0 pillow==10.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dmB_IVBPkZ1",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Step 1.2 Optionally, provide your own InstructLab QNA data set\n",
    "\n",
    "You can optionally provide your own InstructLab QNA file for processing in this step.\n",
    "\n",
    "**Note:** You may want to run this notebook with an existing dataset before creating your own to understand the InstructLab flow.\n",
    "\n",
    "Follow these steps to add your own dataset:\n",
    "1. Create your own qna.yaml file following the directions on the InstructLab taxonomy [readme](https://github.com/instructlab/taxonomy).\n",
    "1. Create a questions.txt file with related sample questions to use on inferencing.\n",
    "1. Add your qna.yaml and sample questions.txt files to the /content/ilab/data/your_content_1 folder or the /content/ilab/data/your_content_2 folder by dragging and dropping them in the desired folder.\n",
    "1. Double click on the /content/ilab/config.json file to edit and specify the qna_location where your data resides within the Dewey Decimal classification system. Close and save the config.json file.\n",
    "1. You can now specify to run with your own data by selecting **Your Content 1** or **Your Content 2** in the next code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OV_SZDBZgMVa"
   },
   "source": [
    "## Step 1.3 Select InstructLab Parameters\n",
    "Run this next cell, select the following parameters, then follow the direction in the next text cell to run the notebook.\n",
    "\n",
    "We've provided question-and-answer files for these datasets: \"2024 Oscar Awards Ceremony\" and \"Quantum Roadmap and Patterns\" and \"Artificial Intelligence Agents\". Feel free to choose one of these datasets, or select your own custom dataset in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401,
     "referenced_widgets": [
      "762e524f95c24b5e9c2d27dc7dc03397",
      "48b32b0ad9dc4596b01fa811ceef5dd7",
      "43e810ff41b34cc4b0fe54ead08af316",
      "ac4c3dd67d69464a8b42848a0039455b",
      "6ce6823c7dda4dd9ad234f77d498ae83",
      "d50074581d1b4733b05bb0b5d4a859cb",
      "de1abbcbf54942909e9e976f1cdb6284",
      "e349a0d750634cf08e27de9e9199a900",
      "d04db31d3e9e46a69f6332c03562f4fe",
      "efdae66b8ff84194b5b3f42869d5729d",
      "cb9a8f915633416ea6afb8041d6c9325",
      "93c8387fae0e4c929296b9e37ee26147",
      "01c1d7674c674814965f15c257155ea0",
      "5c2e9fc3b8614403b7b5eafd28ed0d49",
      "e745a9162f7441acbb580f00144517d5",
      "160d470d643e421e8acddf35fafc830f",
      "def0878813934f31a02af400a3424e83",
      "e7973af9f3894e38816567095419ae47",
      "26d6c19fe1bc406983b928881d3e237a",
      "a84167deb1ef4c5ca4ba5c2a250dc911",
      "819c4c8a9d984ebab8683fe7178f8cdc"
     ]
    },
    "executionInfo": {
     "elapsed": 1264,
     "status": "ok",
     "timestamp": 1744450525958,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "9E0Z6oO2L_3I",
    "outputId": "cbc0b98e-8a71-4084-8d52-325f033c3b70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select the Dataset for this run:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762e524f95c24b5e9c2d27dc7dc03397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Dataset:', options=('cybersecurity',), style=ToggleButtonsStyle(button_width='auto'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the Synthetic Data Generation parameter to use:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4c3dd67d69464a8b42848a0039455b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Processing:', options=('Simple', 'Full with GPU'), style=ToggleButtonsStyle(button_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1abbcbf54942909e9e976f1cdb6284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='# of QNAs:', options=('Default (>450)', '>15', '>50', '>200', '>500', '>1000'), sty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the Training parameters to use:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdae66b8ff84194b5b3f42869d5729d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Epochs:', index=2, options=('1', '2', '3', '4', '5', '10', '15'), style=ToggleButto…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c1d7674c674814965f15c257155ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Iterations:', index=2, options=('1', '3', '5', '10', '20', '50', '100', '200'), sty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select what to do with the model after training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160d470d643e421e8acddf35fafc830f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Live Q&A:', options=('Yes', 'No'), style=ToggleButtonsStyle(button_width='auto'), v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d6c19fe1bc406983b928881d3e237a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Download:', index=1, options=('Yes', 'No'), style=ToggleButtonsStyle(button_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After selecting the parameters, select the next cell and then choose Runtime->Run cell and below\n",
      "When that run completes, you can come here, choose different parameters and rerun at the next cell with Runtime->Run cell and below\n",
      "Note: You can also go back and rerun individual sections of the notebook with different parameters.\n"
     ]
    }
   ],
   "source": [
    "# Run this second cell to show parameters\n",
    "import ipywidgets as widgets\n",
    "#See instructions on placing your hf_token in colab userdata\n",
    "from google.colab import userdata\n",
    "hf_token=userdata.get('hf_token')\n",
    "data_set = widgets.ToggleButtons(\n",
    "    options=['logs-attacker-identification','logs-pattern-description','logs-service-identification','logs-sus-activity-duration'],\n",
    "    description='Dataset:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
    ")\n",
    "sdg_pipe = widgets.ToggleButtons(\n",
    "    options=['Simple', 'Full with GPU'],\n",
    "    description='Processing:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
    ")\n",
    "instr=widgets.ToggleButtons(\n",
    "    options=['Default (>450)','>15', '>50', '>200', '>500', '>1000'],\n",
    "    description='# of QNAs:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
    ")\n",
    "train_pipe = widgets.ToggleButtons(\n",
    "    options=['Simple with GPU','Accelerated GPU'],\n",
    "    description='Processing', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
    ")\n",
    "epoch=widgets.ToggleButtons(\n",
    "    options=['1', '2', '3', '4', '5', '10', '15'],\n",
    "    description='Epochs:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
    ")\n",
    "it=widgets.ToggleButtons(\n",
    "    options=['1', '3', '5','10','20','50','100','200'],\n",
    "    description='Iterations:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
    ")\n",
    "questions=widgets.ToggleButtons(\n",
    "    options=['Yes','No'],\n",
    "    description='Live Q&A:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
    ")\n",
    "download=widgets.ToggleButtons(\n",
    "    options=['Yes','No'],\n",
    "    description='Download:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
    ")\n",
    "print(\"\\nSelect the Dataset for this run:\")\n",
    "display(data_set)\n",
    "print(\"Select the Synthetic Data Generation parameter to use:\")\n",
    "sdg_pipe.value='Simple'\n",
    "display(sdg_pipe)\n",
    "instr.value = 'Default (>450)'\n",
    "display(instr)\n",
    "print(\"Select the Training parameters to use:\")\n",
    "train_pipe.value='Simple with GPU'\n",
    "#display(train_pipe)\n",
    "epoch.value=\"3\"\n",
    "display(epoch)\n",
    "it.value=\"5\"\n",
    "display(it)\n",
    "print(\"Select what to do with the model after training:\")\n",
    "questions.value=\"Yes\"\n",
    "display(questions)\n",
    "download.value=\"No\"\n",
    "display(download)\n",
    "print(\"After selecting the parameters, select the next cell and then choose Runtime->Run cell and below\")\n",
    "print(\"When that run completes, you can come here, choose different parameters and rerun at the next cell with Runtime->Run cell and below\")\n",
    "print(\"Note: You can also go back and rerun individual sections of the notebook with different parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVAfX3Q5CfiL",
    "tags": []
   },
   "source": [
    "## 1.4 Complete Environment Set Up and Optionally Run All\n",
    "This next code cell installs the remainder of the reuired pip packages and takes about 7 minutes to run.\n",
    "\n",
    "If you perform **Runtime->Run cell and below** on this cell, the rest of notebook will take about an hour to run. After running, it will present a prompt for providing questions to the pre-trained and trained models to test improvements in the model.\n",
    "\n",
    "**Note:** Please ignore the pip dependency errors that appear in the output of the pip installs. They do not affect the successful running of Instructlab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 425630,
     "status": "ok",
     "timestamp": 1744451010197,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "aFtTdSAbCfiL",
    "outputId": "c7d7f113-6671-46f0-81dd-3f43bf1250a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: No matching packages for pattern \"llama_cpp_python\"\u001b[0m\u001b[33m\n",
      "\u001b[0mFiles removed: 0\n",
      "Collecting llama_cpp_python==0.3.6 (from llama_cpp_python[server]==0.3.6->-r ilab/requirements_gpu.txt (line 3))\n",
      "  Downloading llama_cpp_python-0.3.6.tar.gz (66.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting instructlab==0.24.3 (from instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading instructlab-0.24.3-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting vllm==0.7.3 (from -r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading vllm-0.7.3-cp38-abi3-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama_cpp_python==0.3.6->llama_cpp_python[server]==0.3.6->-r ilab/requirements_gpu.txt (line 3)) (4.13.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama_cpp_python==0.3.6->llama_cpp_python[server]==0.3.6->-r ilab/requirements_gpu.txt (line 3)) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama_cpp_python==0.3.6->llama_cpp_python[server]==0.3.6->-r ilab/requirements_gpu.txt (line 3))\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama_cpp_python==0.3.6->llama_cpp_python[server]==0.3.6->-r ilab/requirements_gpu.txt (line 3)) (3.1.6)\n",
      "Collecting boto3>=1.35.96 (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading boto3-1.37.33-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (8.1.8)\n",
      "Collecting click-didyoumean>=0.3.0 (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading click_didyoumean-0.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting datasets>=2.18.0 (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (3.18.0)\n",
      "Collecting gguf>=0.6.0 (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading gguf-0.14.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: GitPython>=3.1.42 in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (3.1.44)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.28.1)\n",
      "Collecting instructlab-eval<0.6.0,>=0.5.1 (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading instructlab_eval-0.5.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting instructlab-quantize>=0.1.0 (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading instructlab_quantize-0.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting instructlab-schema>=0.4.2 (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading instructlab_schema-0.4.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting instructlab-sdg<0.8.0,>=0.7.0 (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading instructlab_sdg-0.7.3-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting instructlab-training<0.8.0,>=0.7.0 (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading instructlab_training-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: openai>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (1.70.0)\n",
      "Requirement already satisfied: peft>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.38 in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (3.0.50)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2.11.2)\n",
      "Collecting pydantic_yaml>=1.2.0 (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading pydantic_yaml-1.4.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (6.0.2)\n",
      "Requirement already satisfied: rich>=13.3.1 in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (13.9.4)\n",
      "Collecting rouge-score>=0.1.2 (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting ruamel.yaml>=0.17.0 (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading ruamel.yaml-0.18.10-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: tokenizers>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.21.1)\n",
      "Requirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.10.2)\n",
      "Requirement already satisfied: torch<2.7.0,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2.5.1)\n",
      "Requirement already satisfied: tqdm>=4.66.2 in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (4.67.1)\n",
      "Requirement already satisfied: transformers>=4.41.2 in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (4.50.3)\n",
      "Collecting trl<0.15.0,>=0.12.2 (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading trl-0.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: wandb>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.19.9)\n",
      "Collecting xdg-base-dirs>=6.0.1 (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading xdg_base_dirs-6.0.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: psutil>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (7.0.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]>=0.1.8->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.30.1)\n",
      "Collecting haystack-ai>=2.8 (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading haystack_ai-2.12.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting docling-core>=2.10.0 (from docling-core[chunking]>=2.10.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading docling_core-2.26.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: sentence-transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (3.4.1)\n",
      "Requirement already satisfied: numba==0.60.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (0.60.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (2.32.3)\n",
      "Collecting blake3 (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (9.0.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (5.29.4)\n",
      "Collecting fastapi!=0.113.*,!=0.114.0,>=0.107.0 (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (3.11.15)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (0.21.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (10.4.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lm-format-enforcer<0.11,>=0.10.9 (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting outlines==0.1.11 (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lark==1.2.2 (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xgrammar==0.1.11 (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading xgrammar-0.1.11-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting partial-json-parser (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (24.0.1)\n",
      "Collecting msgspec (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting gguf>=0.6.0 (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (8.6.1)\n",
      "Collecting mistral_common>=1.5.0 (from mistral_common[opencv]>=1.5.0->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (0.8.1)\n",
      "Collecting compressed-tensors==0.9.1 (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading compressed_tensors-0.9.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting depyf==0.18.0 (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (3.1.1)\n",
      "Collecting ray==2.40.0 (from ray[adag]==2.40.0->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading ray-2.40.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Collecting torchaudio==2.5.1 (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading torchaudio-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torchvision==0.20.1 (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading torchvision-0.20.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting xformers==0.0.28.post3 (from vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading xformers-0.0.28.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting uvicorn>=0.22.0 (from llama_cpp_python[server]==0.3.6->-r ilab/requirements_gpu.txt (line 3))\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pydantic-settings>=2.0.1 (from llama_cpp_python[server]==0.3.6->-r ilab/requirements_gpu.txt (line 3))\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting sse-starlette>=1.6.1 (from llama_cpp_python[server]==0.3.6->-r ilab/requirements_gpu.txt (line 3))\n",
      "  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting starlette-context<0.4,>=0.3.6 (from llama_cpp_python[server]==0.3.6->-r ilab/requirements_gpu.txt (line 3))\n",
      "  Downloading starlette_context-0.3.6-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting astor (from depyf==0.18.0->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting dill (from depyf==0.18.0->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba==0.60.0->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (0.43.0)\n",
      "Collecting interegular (from outlines==0.1.11->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (1.6.0)\n",
      "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (0.36.2)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (4.23.0)\n",
      "Collecting pycountry (from outlines==0.1.11->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting airportsdata (from outlines==0.1.11->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading airportsdata-20250224-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.40.0->ray[adag]==2.40.0->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray==2.40.0->ray[adag]==2.40.0->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (24.2)\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray==2.40.0->ray[adag]==2.40.0->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (1.3.2)\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray==2.40.0->ray[adag]==2.40.0->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (1.5.0)\n",
      "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[adag]==2.40.0->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (13.3.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (1.13.1)\n",
      "Collecting pybind11 (from xgrammar==0.1.11->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from xgrammar==0.1.11->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (8.3.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (1.3.0)\n",
      "Collecting botocore<1.38.0,>=1.37.33 (from boto3>=1.35.96->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading botocore-1.37.33-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.35.96->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3>=1.35.96->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.18.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (18.1.0)\n",
      "Collecting dill (from depyf==0.18.0->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.18.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2.2.2)\n",
      "Collecting xxhash (from datasets>=2.18.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.18.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core>=2.10.0->docling-core[chunking]>=2.10.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core>=2.10.0->docling-core[chunking]>=2.10.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading latex2mathml-3.77.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from docling-core>=2.10.0->docling-core[chunking]>=2.10.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.9.0)\n",
      "Collecting typer<0.13.0,>=0.12.5 (from docling-core>=2.10.0->docling-core[chunking]>=2.10.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]>=2.10.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting python-multipart>=0.0.18 (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting email-validator>=2.0.0 (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython>=3.1.42->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (4.0.12)\n",
      "Collecting haystack-experimental (from haystack-ai>=2.8->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading haystack_experimental-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting lazy-imports (from haystack-ai>=2.8->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading lazy_imports-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from haystack-ai>=2.8->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (10.6.0)\n",
      "Collecting posthog!=3.12.0 (from haystack-ai>=2.8->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading posthog-3.24.1-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from haystack-ai>=2.8->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0 in /usr/local/lib/python3.11/dist-packages (from haystack-ai>=2.8->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (9.1.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.14.0)\n",
      "Collecting hf-transfer>=0.1.4 (from huggingface_hub[hf_transfer]>=0.1.8->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting shortuuid (from instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (1.5.2)\n",
      "Requirement already satisfied: pandas-stubs in /usr/local/lib/python3.11/dist-packages (from instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2.2.2.240909)\n",
      "Collecting lm-eval>=0.4.4 (from instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading lm_eval-0.4.8-py3-none-any.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ragas>=0.2.11 (from instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading ragas-0.2.14-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting yamllint>=1.35.1 (from instructlab-schema>=0.4.2->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading yamllint-1.37.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting datasets>=2.18.0 (from instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting docling>=2.18.0 (from docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading docling-2.29.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.11/dist-packages (from instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.3.8)\n",
      "Collecting multiprocess (from datasets>=2.18.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch<2.7.0,>=2.3.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: wheel>=0.43 in /usr/local/lib/python3.11/dist-packages (from instructlab-training<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.45.1)\n",
      "Collecting instructlab-dolomite>=0.2.0 (from instructlab-training<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading instructlab_dolomite-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting aiofiles>=23.2.1 (from instructlab-training<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting flash-attn>=2.4.0 (from instructlab-training[cuda]>=0.7.0; extra == \"cuda\"->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting bitsandbytes>=0.43.1 (from instructlab-training[cuda]>=0.7.0; extra == \"cuda\"->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting deepspeed>=0.14.3 (from instructlab-training[cuda]>=0.7.0; extra == \"cuda\"->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading deepspeed-0.16.5.tar.gz (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting accelerate (from instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama_cpp_python==0.3.6->llama_cpp_python[server]==0.3.6->-r ilab/requirements_gpu.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: opencv-python-headless>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common[opencv]>=1.5.0->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (4.11.0.86)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.13.3->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.13.3->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.13.3->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft>=0.9.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.5.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit>=3.0.38->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.4.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings>=2.0.1->llama_cpp_python[server]==0.3.6->-r ilab/requirements_gpu.txt (line 3))\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (2.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2.18.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.1.2->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.1.2->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.1.2->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (1.17.0)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=3.0.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=3.0.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (1.14.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.6.0->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (2024.11.6)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.16.4->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.16.4->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (4.3.7)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.16.4->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2.25.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb>=0.16.4->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (1.3.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb>=0.16.4->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (75.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (25.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (1.18.3)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (3.21.0)\n",
      "Collecting hjson (from deepspeed>=0.14.3->instructlab-training[cuda]>=0.7.0; extra == \"cuda\"->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting ninja (from deepspeed>=0.14.3->instructlab-training[cuda]>=0.7.0; extra == \"cuda\"->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from deepspeed>=0.14.3->instructlab-training[cuda]>=0.7.0; extra == \"cuda\"->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (12.570.86)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (4.13.3)\n",
      "Collecting docling-ibm-models<4.0.0,>=3.4.0 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading docling_ibm_models-3.4.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting docling-parse<5.0.0,>=4.0.0 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading docling_parse-4.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting easyocr<2.0,>=1.7 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: lxml<6.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (5.3.1)\n",
      "Collecting marko<3.0.0,>=2.1.2 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading marko-2.1.3-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (3.1.5)\n",
      "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (1.5.0)\n",
      "Collecting pylatexenc<3.0,>=2.10 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pypdfium2<5.0.0,>=4.30.0 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-docx<2.0.0,>=1.1.2 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting python-pptx<2.0.0,>=1.0.2 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting rtree<2.0.0,>=1.3.0 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting tesserocr<3.0.0,>=2.7.1 (from docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading tesserocr-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading rich_toolkit-0.14.1-py3-none-any.whl.metadata (999 bytes)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.42->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (5.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (2024.10.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (0.24.0)\n",
      "Collecting evaluate (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting jsonlines (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2.10.2)\n",
      "Collecting pytablewriter (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting sacrebleu>=1.5.0 (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sqlitedict (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting tqdm-multiprocess (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.23.0)\n",
      "Collecting word2number (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.18.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.18.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2025.2)\n",
      "Collecting monotonic>=1.5 (from posthog!=3.12.0->haystack-ai>=2.8->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog!=3.12.0->haystack-ai>=2.8->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (from ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.3.23)\n",
      "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (from ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.3.51)\n",
      "Collecting langchain-community (from ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain_openai (from ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting appdirs (from ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=3.0.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=3.0.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (3.6.0)\n",
      "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]>=2.10.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->docling-core>=2.10.0->docling-core[chunking]>=2.10.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5))\n",
      "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (15.0.1)\n",
      "Collecting pathspec>=0.5.3 (from yamllint>=1.35.1->instructlab-schema>=0.4.2->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[adag]==2.40.0->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (0.8.3)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Requirement already satisfied: types-pytz>=2022.1.1 in /usr/local/lib/python3.11/dist-packages (from pandas-stubs->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2025.2.0.20250326)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->xgrammar==0.1.11->vllm==0.7.3->-r ilab/requirements_gpu.txt (line 5)) (2.1.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2.6)\n",
      "Collecting jsonlines (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading jsonlines-3.1.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.25.2)\n",
      "Collecting python-bidi (from easyocr<2.0,>=1.7->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2.1.0)\n",
      "Collecting pyclipper (from easyocr<2.0,>=1.7->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.3.24)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (1.33)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl<4.0.0,>=3.1.5->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2.0.0)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting portalocker (from sacrebleu>=1.5.0->lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting colorama (from sacrebleu>=1.5.0->lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2.0.40)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.11/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (5.2.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (3.1.1)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (2025.3.30)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4)) (0.4)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab==0.24.3->instructlab[cuda]==0.24.3->-r ilab/requirements_gpu.txt (line 4))\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading instructlab-0.24.3-py3-none-any.whl (285 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.5/285.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading vllm-0.7.3-cp38-abi3-manylinux1_x86_64.whl (264.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading compressed_tensors-0.9.1-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading depyf-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading gguf-0.10.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading outlines-0.1.11-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ray-2.40.0-cp311-cp311-manylinux2014_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.5.1-cp311-cp311-manylinux1_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.20.1-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.28.post3-cp311-cp311-manylinux_2_28_x86_64.whl (16.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xgrammar-0.1.11-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (396 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.9/396.9 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading boto3-1.37.33-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click_didyoumean-0.3.1-py3-none-any.whl (3.6 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docling_core-2.26.1-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading haystack_ai-2.12.1-py3-none-any.whl (482 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m483.0/483.0 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading instructlab_eval-0.5.1-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.9/70.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading instructlab_quantize-0.1.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading instructlab_schema-0.4.2-py3-none-any.whl (18 kB)\n",
      "Downloading instructlab_sdg-0.7.3-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading instructlab_training-0.7.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mistral_common-1.5.4-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Downloading pydantic_yaml-1.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading ruamel.yaml-0.18.10-py3-none-any.whl (117 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\n",
      "Downloading starlette_context-0.3.6-py3-none-any.whl (12 kB)\n",
      "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.14.0-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.9/313.9 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xdg_base_dirs-6.0.2-py3-none-any.whl (4.7 kB)\n",
      "Downloading blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
      "Downloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.37.33-py3-none-any.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docling-2.29.0-py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.2/162.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading instructlab_dolomite-0.2.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Downloading latex2mathml-3.77.0-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lm_eval-0.4.8-py3-none-any.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading posthog-3.24.1-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ragas-0.2.14-py3-none-any.whl (187 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.11.4-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
      "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yamllint-1.37.0-py3-none-any.whl (68 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading airportsdata-20250224-py3-none-any.whl (913 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading haystack_experimental-0.8.0-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lazy_imports-0.4.0-py3-none-any.whl (12 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docling_ibm_models-3.4.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docling_parse-4.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
      "Downloading marko-2.1.3-py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich_toolkit-0.14.1-py3-none-any.whl (24 kB)\n",
      "Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (541 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.1/541.1 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tesserocr-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.3.12-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
      "Downloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\n",
      "Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
      "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
      "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
      "Downloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading mpire-2.10.2-py3-none-any.whl (272 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Building wheels for collected packages: llama_cpp_python, rouge-score, deepspeed, flash-attn, pylatexenc, sqlitedict, word2number\n",
      "  Building wheel for llama_cpp_python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for llama_cpp_python: filename=llama_cpp_python-0.3.6-cp311-cp311-linux_x86_64.whl size=42788395 sha256=ea9a14a9ad236f89907cb2131cbc824937536f9abec92f5270ee3d52920e2eb5\n",
      "  Stored in directory: /root/.cache/pip/wheels/e8/96/d2/acfb576f7a58ef0580e2fec8096e5eefd17cc356017089337b\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=af1263b5decefa7a37973aed7f817eb2e66e59745f330895c874f5d0a5d3318c\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for deepspeed: filename=deepspeed-0.16.5-py3-none-any.whl size=1580582 sha256=ff238228507b2d6ab9c5b437ce5a877950a50f0b20e775a4f5cd1e6ad6aa83d5\n",
      "  Stored in directory: /root/.cache/pip/wheels/52/cc/7a/2a649465930859b5b227a30f75d9d19895f1d52f0b071a7486\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp311-cp311-linux_x86_64.whl size=187815463 sha256=d944fc7d2f962bce83fc4708c2fc0c21eaf8255962a0b350ae919362a51b7ef2\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/88/d8/284b89f56af7d5bf366b10d6b8e251ac8a7c7bf3f04203fb4f\n",
      "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136817 sha256=935b44f28c90e9951350d1579b0d4d7dc9ca46d1856b53af73475589b9ecd0c4\n",
      "  Stored in directory: /root/.cache/pip/wheels/b1/7a/33/9fdd892f784ed4afda62b685ae3703adf4c91aa0f524c28f03\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=b745ab38a2a8e9740183a9563ab0f559ef16d8fc5aec6c133ee29260993b3f4e\n",
      "  Stored in directory: /root/.cache/pip/wheels/73/63/89/7210274f9b7fb033b8f22671f64c0e0b55083d30c3c046a3ff\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=3f0803aea026bc8c5771f0b4a47adb0abd11a3b8afe3dab8c289f27fef72326e\n",
      "  Stored in directory: /root/.cache/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\n",
      "Successfully built llama_cpp_python rouge-score deepspeed flash-attn pylatexenc sqlitedict word2number\n",
      "Installing collected packages: word2number, tesserocr, sqlitedict, python-bidi, pylatexenc, pyclipper, monotonic, hjson, filetype, blake3, appdirs, xxhash, XlsxWriter, xdg-base-dirs, uvloop, uvicorn, tcolorpy, shortuuid, ruamel.yaml.clib, rtree, python-multipart, python-dotenv, python-docx, pypdfium2, pycountry, pybind11, portalocker, pathvalidate, pathspec, partial-json-parser, ninja, mypy-extensions, msgspec, mpire, mbstrdecoder, marshmallow, marko, lazy-imports, latex2mathml, lark, jsonref, jsonlines, jmespath, interegular, instructlab-quantize, httpx-sse, httptools, hf-transfer, gguf, fsspec, dnspython, diskcache, dill, colorama, click-didyoumean, backoff, astor, airportsdata, aiofiles, yamllint, watchfiles, typing-inspect, typepy, tqdm-multiprocess, tiktoken, starlette, sacrebleu, ruamel.yaml, rouge-score, python-pptx, posthog, multiprocess, llama_cpp_python, email-validator, depyf, botocore, typer, starlette-context, sse-starlette, s3transfer, rich-toolkit, pydantic_yaml, pydantic-settings, prometheus-fastapi-instrumentator, lm-format-enforcer, fastapi, dataclasses-json, xformers, torchvision, torchaudio, semchunk, ray, outlines_core, mistral_common, instructlab-schema, flash-attn, fastapi-cli, docling-core, deepspeed, datasets, DataProperty, boto3, bitsandbytes, accelerate, xgrammar, trl, tabledata, outlines, langchain_openai, instructlab-dolomite, evaluate, easyocr, docling-parse, docling-ibm-models, compressed-tensors, pytablewriter, instructlab-training, docling, vllm, lm-eval, langchain-community, ragas, instructlab-sdg, instructlab-eval, haystack-experimental, haystack-ai, instructlab\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.15.2\n",
      "    Uninstalling typer-0.15.2:\n",
      "      Successfully uninstalled typer-0.15.2\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.21.0+cu124\n",
      "    Uninstalling torchvision-0.21.0+cu124:\n",
      "      Successfully uninstalled torchvision-0.21.0+cu124\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.6.0+cu124\n",
      "    Uninstalling torchaudio-2.6.0+cu124:\n",
      "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.5.2\n",
      "    Uninstalling accelerate-1.5.2:\n",
      "      Successfully uninstalled accelerate-1.5.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed DataProperty-1.1.0 XlsxWriter-3.2.2 accelerate-1.0.1 aiofiles-24.1.0 airportsdata-20250224 appdirs-1.4.4 astor-0.8.1 backoff-2.2.1 bitsandbytes-0.45.5 blake3-1.0.4 boto3-1.37.33 botocore-1.37.33 click-didyoumean-0.3.1 colorama-0.4.6 compressed-tensors-0.9.1 dataclasses-json-0.6.7 datasets-2.21.0 deepspeed-0.16.5 depyf-0.18.0 dill-0.3.8 diskcache-5.6.3 dnspython-2.7.0 docling-2.29.0 docling-core-2.26.1 docling-ibm-models-3.4.1 docling-parse-4.0.1 easyocr-1.7.2 email-validator-2.2.0 evaluate-0.4.3 fastapi-0.115.12 fastapi-cli-0.0.7 filetype-1.2.0 flash-attn-2.7.4.post1 fsspec-2024.6.1 gguf-0.10.0 haystack-ai-2.12.1 haystack-experimental-0.8.0 hf-transfer-0.1.9 hjson-3.1.0 httptools-0.6.4 httpx-sse-0.4.0 instructlab-0.24.3 instructlab-dolomite-0.2.0 instructlab-eval-0.5.1 instructlab-quantize-0.1.0 instructlab-schema-0.4.2 instructlab-sdg-0.7.3 instructlab-training-0.7.0 interegular-0.3.3 jmespath-1.0.1 jsonlines-3.1.0 jsonref-1.1.0 langchain-community-0.3.21 langchain_openai-0.3.12 lark-1.2.2 latex2mathml-3.77.0 lazy-imports-0.4.0 llama_cpp_python-0.3.6 lm-eval-0.4.8 lm-format-enforcer-0.10.11 marko-2.1.3 marshmallow-3.26.1 mbstrdecoder-1.1.4 mistral_common-1.5.4 monotonic-1.6 mpire-2.10.2 msgspec-0.19.0 multiprocess-0.70.16 mypy-extensions-1.0.0 ninja-1.11.1.4 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post5 pathspec-0.12.1 pathvalidate-3.2.3 portalocker-3.1.1 posthog-3.24.1 prometheus-fastapi-instrumentator-7.1.0 pybind11-2.13.6 pyclipper-1.3.0.post6 pycountry-24.6.1 pydantic-settings-2.8.1 pydantic_yaml-1.4.0 pylatexenc-2.10 pypdfium2-4.30.1 pytablewriter-1.2.1 python-bidi-0.6.6 python-docx-1.1.2 python-dotenv-1.1.0 python-multipart-0.0.20 python-pptx-1.0.2 ragas-0.2.14 ray-2.40.0 rich-toolkit-0.14.1 rouge-score-0.1.2 rtree-1.4.0 ruamel.yaml-0.18.10 ruamel.yaml.clib-0.2.12 s3transfer-0.11.4 sacrebleu-2.5.1 semchunk-2.2.2 shortuuid-1.0.13 sqlitedict-2.1.0 sse-starlette-2.2.1 starlette-0.46.1 starlette-context-0.3.6 tabledata-1.3.4 tcolorpy-0.1.7 tesserocr-2.8.0 tiktoken-0.9.0 torchaudio-2.5.1 torchvision-0.20.1 tqdm-multiprocess-0.0.11 trl-0.14.0 typepy-1.3.4 typer-0.12.5 typing-inspect-0.9.0 uvicorn-0.34.0 uvloop-0.21.0 vllm-0.7.3 watchfiles-1.0.5 word2number-1.1 xdg-base-dirs-6.0.2 xformers-0.0.28.post3 xgrammar-0.1.11 xxhash-3.5.0 yamllint-1.37.0\n",
      "Package                            Version\n",
      "---------------------------------- -------------------\n",
      "absl-py                            1.4.0\n",
      "accelerate                         1.0.1\n",
      "aiofiles                           24.1.0\n",
      "aiohappyeyeballs                   2.6.1\n",
      "aiohttp                            3.11.15\n",
      "aiosignal                          1.3.2\n",
      "airportsdata                       20250224\n",
      "alabaster                          1.0.0\n",
      "albucore                           0.0.23\n",
      "albumentations                     2.0.5\n",
      "ale-py                             0.10.2\n",
      "altair                             5.5.0\n",
      "annotated-types                    0.7.0\n",
      "anyio                              4.9.0\n",
      "appdirs                            1.4.4\n",
      "argon2-cffi                        23.1.0\n",
      "argon2-cffi-bindings               21.2.0\n",
      "array_record                       0.7.1\n",
      "arviz                              0.21.0\n",
      "astor                              0.8.1\n",
      "astropy                            7.0.1\n",
      "astropy-iers-data                  0.2025.3.31.0.36.18\n",
      "astunparse                         1.6.3\n",
      "atpublic                           5.1\n",
      "attrs                              25.3.0\n",
      "audioread                          3.0.1\n",
      "autograd                           1.7.0\n",
      "babel                              2.17.0\n",
      "backcall                           0.2.0\n",
      "backoff                            2.2.1\n",
      "beautifulsoup4                     4.13.3\n",
      "betterproto                        2.0.0b6\n",
      "bigframes                          1.42.0\n",
      "bigquery-magics                    0.9.0\n",
      "bitsandbytes                       0.45.5\n",
      "blake3                             1.0.4\n",
      "bleach                             6.2.0\n",
      "blinker                            1.9.0\n",
      "blis                               1.3.0\n",
      "blosc2                             3.2.1\n",
      "bokeh                              3.6.3\n",
      "boto3                              1.37.33\n",
      "botocore                           1.37.33\n",
      "Bottleneck                         1.4.2\n",
      "bqplot                             0.12.44\n",
      "branca                             0.8.1\n",
      "CacheControl                       0.14.2\n",
      "cachetools                         5.5.2\n",
      "catalogue                          2.0.10\n",
      "certifi                            2025.1.31\n",
      "cffi                               1.17.1\n",
      "chardet                            5.2.0\n",
      "charset-normalizer                 3.4.1\n",
      "chex                               0.1.89\n",
      "clarabel                           0.10.0\n",
      "click                              8.1.8\n",
      "click-didyoumean                   0.3.1\n",
      "cloudpathlib                       0.21.0\n",
      "cloudpickle                        3.1.1\n",
      "cmake                              3.31.6\n",
      "cmdstanpy                          1.2.5\n",
      "colorama                           0.4.6\n",
      "colorcet                           3.1.0\n",
      "colorlover                         0.3.0\n",
      "colour                             0.1.5\n",
      "community                          1.0.0b1\n",
      "compressed-tensors                 0.9.1\n",
      "confection                         0.1.5\n",
      "cons                               0.4.6\n",
      "contourpy                          1.3.1\n",
      "cramjam                            2.9.1\n",
      "cryptography                       43.0.3\n",
      "cuda-python                        12.6.2.post1\n",
      "cudf-cu12                          25.2.1\n",
      "cudf-polars-cu12                   25.2.2\n",
      "cufflinks                          0.17.3\n",
      "cuml-cu12                          25.2.1\n",
      "cupy-cuda12x                       13.3.0\n",
      "cuvs-cu12                          25.2.1\n",
      "cvxopt                             1.3.2\n",
      "cvxpy                              1.6.4\n",
      "cycler                             0.12.1\n",
      "cyipopt                            1.5.0\n",
      "cymem                              2.0.11\n",
      "Cython                             3.0.12\n",
      "dask                               2024.12.1\n",
      "dask-cuda                          25.2.0\n",
      "dask-cudf-cu12                     25.2.2\n",
      "dask-expr                          1.1.21\n",
      "dataclasses-json                   0.6.7\n",
      "DataProperty                       1.1.0\n",
      "datascience                        0.17.6\n",
      "datasets                           2.21.0\n",
      "db-dtypes                          1.4.2\n",
      "dbus-python                        1.2.18\n",
      "debugpy                            1.8.0\n",
      "decorator                          4.4.2\n",
      "deepspeed                          0.16.5\n",
      "defusedxml                         0.7.1\n",
      "Deprecated                         1.2.18\n",
      "depyf                              0.18.0\n",
      "diffusers                          0.32.2\n",
      "dill                               0.3.8\n",
      "diskcache                          5.6.3\n",
      "distributed                        2024.12.1\n",
      "distributed-ucxx-cu12              0.42.0\n",
      "distro                             1.9.0\n",
      "dlib                               19.24.6\n",
      "dm-tree                            0.1.9\n",
      "dnspython                          2.7.0\n",
      "docker-pycreds                     0.4.0\n",
      "docling                            2.29.0\n",
      "docling-core                       2.26.1\n",
      "docling-ibm-models                 3.4.1\n",
      "docling-parse                      4.0.1\n",
      "docstring_parser                   0.16\n",
      "docutils                           0.21.2\n",
      "dopamine_rl                        4.1.2\n",
      "duckdb                             1.2.1\n",
      "earthengine-api                    1.5.9\n",
      "easydict                           1.13\n",
      "easyocr                            1.7.2\n",
      "editdistance                       0.8.1\n",
      "eerepr                             0.1.1\n",
      "einops                             0.8.1\n",
      "email_validator                    2.2.0\n",
      "en_core_web_sm                     3.8.0\n",
      "entrypoints                        0.4\n",
      "et_xmlfile                         2.0.0\n",
      "etils                              1.12.2\n",
      "etuples                            0.3.9\n",
      "evaluate                           0.4.3\n",
      "Farama-Notifications               0.0.4\n",
      "fastai                             2.7.19\n",
      "fastapi                            0.115.12\n",
      "fastapi-cli                        0.0.7\n",
      "fastcore                           1.7.29\n",
      "fastdownload                       0.0.7\n",
      "fastjsonschema                     2.21.1\n",
      "fastprogress                       1.0.3\n",
      "fastrlock                          0.8.3\n",
      "filelock                           3.18.0\n",
      "filetype                           1.2.0\n",
      "firebase-admin                     6.7.0\n",
      "flash_attn                         2.7.4.post1\n",
      "Flask                              3.1.0\n",
      "flatbuffers                        25.2.10\n",
      "flax                               0.10.5\n",
      "folium                             0.19.5\n",
      "fonttools                          4.57.0\n",
      "frozendict                         2.4.6\n",
      "frozenlist                         1.5.0\n",
      "fsspec                             2024.6.1\n",
      "future                             1.0.0\n",
      "gast                               0.6.0\n",
      "gcsfs                              2025.3.2\n",
      "GDAL                               3.6.4\n",
      "gdown                              5.2.0\n",
      "geemap                             0.35.3\n",
      "geocoder                           1.38.1\n",
      "geographiclib                      2.0\n",
      "geopandas                          1.0.1\n",
      "geopy                              2.4.1\n",
      "gguf                               0.10.0\n",
      "gin-config                         0.5.0\n",
      "gitdb                              4.0.12\n",
      "GitPython                          3.1.44\n",
      "glob2                              0.7\n",
      "google                             2.0.3\n",
      "google-ai-generativelanguage       0.6.15\n",
      "google-api-core                    2.24.2\n",
      "google-api-python-client           2.164.0\n",
      "google-auth                        2.38.0\n",
      "google-auth-httplib2               0.2.0\n",
      "google-auth-oauthlib               1.2.1\n",
      "google-cloud-aiplatform            1.87.0\n",
      "google-cloud-bigquery              3.31.0\n",
      "google-cloud-bigquery-connection   1.18.2\n",
      "google-cloud-bigquery-storage      2.30.0\n",
      "google-cloud-bigtable              2.30.0\n",
      "google-cloud-core                  2.4.3\n",
      "google-cloud-dataproc              5.18.1\n",
      "google-cloud-datastore             2.20.2\n",
      "google-cloud-firestore             2.20.1\n",
      "google-cloud-functions             1.20.2\n",
      "google-cloud-iam                   2.18.3\n",
      "google-cloud-language              2.17.1\n",
      "google-cloud-pubsub                2.29.0\n",
      "google-cloud-resource-manager      1.14.2\n",
      "google-cloud-spanner               3.53.0\n",
      "google-cloud-storage               2.19.0\n",
      "google-cloud-translate             3.20.2\n",
      "google-colab                       1.0.0\n",
      "google-crc32c                      1.7.1\n",
      "google-genai                       1.9.0\n",
      "google-generativeai                0.8.4\n",
      "google-pasta                       0.2.0\n",
      "google-resumable-media             2.7.2\n",
      "google-spark-connect               0.5.2\n",
      "googleapis-common-protos           1.69.2\n",
      "googledrivedownloader              1.1.0\n",
      "graphviz                           0.20.3\n",
      "greenlet                           3.1.1\n",
      "grpc-google-iam-v1                 0.14.2\n",
      "grpc-interceptor                   0.15.4\n",
      "grpcio                             1.71.0\n",
      "grpcio-status                      1.71.0\n",
      "grpclib                            0.4.7\n",
      "gspread                            6.2.0\n",
      "gspread-dataframe                  4.0.0\n",
      "gym                                0.25.2\n",
      "gym-notices                        0.0.8\n",
      "gymnasium                          1.1.1\n",
      "h11                                0.14.0\n",
      "h2                                 4.2.0\n",
      "h5netcdf                           1.6.1\n",
      "h5py                               3.13.0\n",
      "haystack-ai                        2.12.1\n",
      "haystack-experimental              0.8.0\n",
      "hdbscan                            0.8.40\n",
      "hf_transfer                        0.1.9\n",
      "highspy                            1.9.0\n",
      "hjson                              3.1.0\n",
      "holidays                           0.69\n",
      "holoviews                          1.20.2\n",
      "hpack                              4.1.0\n",
      "html5lib                           1.1\n",
      "httpcore                           1.0.7\n",
      "httpimport                         1.4.1\n",
      "httplib2                           0.22.0\n",
      "httptools                          0.6.4\n",
      "httpx                              0.28.1\n",
      "httpx-sse                          0.4.0\n",
      "huggingface-hub                    0.30.1\n",
      "humanize                           4.12.2\n",
      "hyperframe                         6.1.0\n",
      "hyperopt                           0.2.7\n",
      "ibis-framework                     9.5.0\n",
      "idna                               3.10\n",
      "imageio                            2.37.0\n",
      "imageio-ffmpeg                     0.6.0\n",
      "imagesize                          1.4.1\n",
      "imbalanced-learn                   0.13.0\n",
      "immutabledict                      4.2.1\n",
      "importlib_metadata                 8.6.1\n",
      "importlib_resources                6.5.2\n",
      "imutils                            0.5.4\n",
      "inflect                            7.5.0\n",
      "iniconfig                          2.1.0\n",
      "instructlab                        0.24.3\n",
      "instructlab-dolomite               0.2.0\n",
      "instructlab-eval                   0.5.1\n",
      "instructlab-quantize               0.1.0\n",
      "instructlab-schema                 0.4.2\n",
      "instructlab-sdg                    0.7.3\n",
      "instructlab-training               0.7.0\n",
      "intel-cmplr-lib-ur                 2025.1.0\n",
      "intel-openmp                       2025.1.0\n",
      "interegular                        0.3.3\n",
      "ipyevents                          2.0.2\n",
      "ipyfilechooser                     0.6.0\n",
      "ipykernel                          6.17.1\n",
      "ipyleaflet                         0.19.2\n",
      "ipyparallel                        8.8.0\n",
      "ipython                            7.34.0\n",
      "ipython-genutils                   0.2.0\n",
      "ipython-sql                        0.5.0\n",
      "ipytree                            0.2.2\n",
      "ipywidgets                         7.7.1\n",
      "itsdangerous                       2.2.0\n",
      "jax                                0.5.2\n",
      "jax-cuda12-pjrt                    0.5.1\n",
      "jax-cuda12-plugin                  0.5.1\n",
      "jaxlib                             0.5.1\n",
      "jeepney                            0.7.1\n",
      "jellyfish                          1.1.0\n",
      "jieba                              0.42.1\n",
      "Jinja2                             3.1.6\n",
      "jiter                              0.9.0\n",
      "jmespath                           1.0.1\n",
      "joblib                             1.4.2\n",
      "jsonlines                          3.1.0\n",
      "jsonpatch                          1.33\n",
      "jsonpickle                         4.0.5\n",
      "jsonpointer                        3.0.0\n",
      "jsonref                            1.1.0\n",
      "jsonschema                         4.23.0\n",
      "jsonschema-specifications          2024.10.1\n",
      "jupyter-client                     6.1.12\n",
      "jupyter-console                    6.1.0\n",
      "jupyter_core                       5.7.2\n",
      "jupyter-leaflet                    0.19.2\n",
      "jupyter-server                     1.16.0\n",
      "jupyterlab_pygments                0.3.0\n",
      "jupyterlab_widgets                 3.0.13\n",
      "kaggle                             1.7.4.2\n",
      "kagglehub                          0.3.11\n",
      "keras                              3.8.0\n",
      "keras-hub                          0.18.1\n",
      "keras-nlp                          0.18.1\n",
      "keyring                            23.5.0\n",
      "kiwisolver                         1.4.8\n",
      "langchain                          0.3.23\n",
      "langchain-community                0.3.21\n",
      "langchain-core                     0.3.51\n",
      "langchain-openai                   0.3.12\n",
      "langchain-text-splitters           0.3.8\n",
      "langcodes                          3.5.0\n",
      "langsmith                          0.3.24\n",
      "language_data                      1.3.0\n",
      "lark                               1.2.2\n",
      "latex2mathml                       3.77.0\n",
      "launchpadlib                       1.10.16\n",
      "lazr.restfulclient                 0.14.4\n",
      "lazr.uri                           1.0.6\n",
      "lazy_imports                       0.4.0\n",
      "lazy_loader                        0.4\n",
      "libclang                           18.1.1\n",
      "libcudf-cu12                       25.2.1\n",
      "libcugraph-cu12                    25.2.0\n",
      "libcuml-cu12                       25.2.1\n",
      "libcuvs-cu12                       25.2.1\n",
      "libkvikio-cu12                     25.2.1\n",
      "libraft-cu12                       25.2.0\n",
      "librosa                            0.11.0\n",
      "libucx-cu12                        1.18.0\n",
      "libucxx-cu12                       0.42.0\n",
      "lightgbm                           4.5.0\n",
      "linkify-it-py                      2.0.3\n",
      "llama_cpp_python                   0.3.6\n",
      "llvmlite                           0.43.0\n",
      "lm_eval                            0.4.8\n",
      "lm-format-enforcer                 0.10.11\n",
      "locket                             1.0.0\n",
      "logical-unification                0.4.6\n",
      "lxml                               5.3.1\n",
      "Mako                               1.1.3\n",
      "marisa-trie                        1.2.1\n",
      "Markdown                           3.7\n",
      "markdown-it-py                     3.0.0\n",
      "marko                              2.1.3\n",
      "MarkupSafe                         3.0.2\n",
      "marshmallow                        3.26.1\n",
      "matplotlib                         3.10.0\n",
      "matplotlib-inline                  0.1.7\n",
      "matplotlib-venn                    1.1.2\n",
      "mbstrdecoder                       1.1.4\n",
      "mdit-py-plugins                    0.4.2\n",
      "mdurl                              0.1.2\n",
      "miniKanren                         1.0.3\n",
      "missingno                          0.5.2\n",
      "mistral_common                     1.5.4\n",
      "mistune                            3.1.3\n",
      "mizani                             0.13.2\n",
      "mkl                                2025.0.1\n",
      "ml-dtypes                          0.4.1\n",
      "mlxtend                            0.23.4\n",
      "monotonic                          1.6\n",
      "more-itertools                     10.6.0\n",
      "moviepy                            1.0.3\n",
      "mpire                              2.10.2\n",
      "mpmath                             1.3.0\n",
      "msgpack                            1.1.0\n",
      "msgspec                            0.19.0\n",
      "multidict                          6.3.2\n",
      "multipledispatch                   1.0.0\n",
      "multiprocess                       0.70.16\n",
      "multitasking                       0.0.11\n",
      "murmurhash                         1.0.12\n",
      "music21                            9.3.0\n",
      "mypy-extensions                    1.0.0\n",
      "namex                              0.0.8\n",
      "narwhals                           1.33.0\n",
      "natsort                            8.4.0\n",
      "nbclassic                          1.2.0\n",
      "nbclient                           0.10.2\n",
      "nbconvert                          7.16.6\n",
      "nbformat                           5.10.4\n",
      "ndindex                            1.9.2\n",
      "nest-asyncio                       1.6.0\n",
      "networkx                           3.4.2\n",
      "nibabel                            5.3.2\n",
      "ninja                              1.11.1.4\n",
      "nltk                               3.9.1\n",
      "notebook                           6.5.7\n",
      "notebook_shim                      0.2.4\n",
      "numba                              0.60.0\n",
      "numba-cuda                         0.2.0\n",
      "numexpr                            2.10.2\n",
      "numpy                              1.26.4\n",
      "nvidia-cublas-cu12                 12.4.5.8\n",
      "nvidia-cuda-cupti-cu12             12.4.127\n",
      "nvidia-cuda-nvcc-cu12              12.5.82\n",
      "nvidia-cuda-nvrtc-cu12             12.4.127\n",
      "nvidia-cuda-runtime-cu12           12.4.127\n",
      "nvidia-cudnn-cu12                  9.1.0.70\n",
      "nvidia-cufft-cu12                  11.2.1.3\n",
      "nvidia-curand-cu12                 10.3.5.147\n",
      "nvidia-cusolver-cu12               11.6.1.9\n",
      "nvidia-cusparse-cu12               12.3.1.170\n",
      "nvidia-cusparselt-cu12             0.6.2\n",
      "nvidia-ml-py                       12.570.86\n",
      "nvidia-nccl-cu12                   2.21.5\n",
      "nvidia-nvcomp-cu12                 4.2.0.11\n",
      "nvidia-nvjitlink-cu12              12.4.127\n",
      "nvidia-nvtx-cu12                   12.4.127\n",
      "nvtx                               0.2.11\n",
      "nx-cugraph-cu12                    25.2.0\n",
      "oauth2client                       4.1.3\n",
      "oauthlib                           3.2.2\n",
      "openai                             1.70.0\n",
      "opencv-contrib-python              4.11.0.86\n",
      "opencv-python                      4.11.0.86\n",
      "opencv-python-headless             4.11.0.86\n",
      "openpyxl                           3.1.5\n",
      "opentelemetry-api                  1.31.1\n",
      "opentelemetry-sdk                  1.31.1\n",
      "opentelemetry-semantic-conventions 0.52b1\n",
      "opt_einsum                         3.4.0\n",
      "optax                              0.2.4\n",
      "optree                             0.14.1\n",
      "orbax-checkpoint                   0.11.10\n",
      "orjson                             3.10.16\n",
      "osqp                               1.0.3\n",
      "outlines                           0.1.11\n",
      "outlines_core                      0.1.26\n",
      "packaging                          24.2\n",
      "pandas                             2.2.2\n",
      "pandas-datareader                  0.10.0\n",
      "pandas-gbq                         0.28.0\n",
      "pandas-stubs                       2.2.2.240909\n",
      "pandocfilters                      1.5.1\n",
      "panel                              1.6.2\n",
      "param                              2.2.0\n",
      "parso                              0.8.4\n",
      "parsy                              2.1\n",
      "partd                              1.4.2\n",
      "partial-json-parser                0.2.1.1.post5\n",
      "pathlib                            1.0.1\n",
      "pathspec                           0.12.1\n",
      "pathvalidate                       3.2.3\n",
      "patsy                              1.0.1\n",
      "peewee                             3.17.9\n",
      "peft                               0.14.0\n",
      "pexpect                            4.9.0\n",
      "pickleshare                        0.7.5\n",
      "pillow                             10.4.0\n",
      "pip                                24.1.2\n",
      "platformdirs                       4.3.7\n",
      "plotly                             5.24.1\n",
      "plotnine                           0.14.5\n",
      "pluggy                             1.5.0\n",
      "ply                                3.11\n",
      "polars                             1.21.0\n",
      "pooch                              1.8.2\n",
      "portalocker                        3.1.1\n",
      "portpicker                         1.5.2\n",
      "posthog                            3.24.1\n",
      "preshed                            3.0.9\n",
      "prettytable                        3.16.0\n",
      "proglog                            0.1.11\n",
      "progressbar2                       4.5.0\n",
      "prometheus_client                  0.21.1\n",
      "prometheus-fastapi-instrumentator  7.1.0\n",
      "promise                            2.3\n",
      "prompt_toolkit                     3.0.50\n",
      "propcache                          0.3.1\n",
      "prophet                            1.1.6\n",
      "proto-plus                         1.26.1\n",
      "protobuf                           5.29.4\n",
      "psutil                             7.0.0\n",
      "psycopg2                           2.9.10\n",
      "ptyprocess                         0.7.0\n",
      "py-cpuinfo                         9.0.0\n",
      "py4j                               0.10.9.7\n",
      "pyarrow                            18.1.0\n",
      "pyasn1                             0.6.1\n",
      "pyasn1_modules                     0.4.2\n",
      "pybind11                           2.13.6\n",
      "pycairo                            1.27.0\n",
      "pyclipper                          1.3.0.post6\n",
      "pycocotools                        2.0.8\n",
      "pycountry                          24.6.1\n",
      "pycparser                          2.22\n",
      "pydantic                           2.11.2\n",
      "pydantic_core                      2.33.1\n",
      "pydantic-settings                  2.8.1\n",
      "pydantic_yaml                      1.4.0\n",
      "pydata-google-auth                 1.9.1\n",
      "pydot                              3.0.4\n",
      "pydotplus                          2.0.2\n",
      "PyDrive                            1.3.1\n",
      "PyDrive2                           1.21.3\n",
      "pyerfa                             2.0.1.5\n",
      "pygame                             2.6.1\n",
      "pygit2                             1.17.0\n",
      "Pygments                           2.18.0\n",
      "PyGObject                          3.42.0\n",
      "PyJWT                              2.10.1\n",
      "pylatexenc                         2.10\n",
      "pylibcudf-cu12                     25.2.1\n",
      "pylibcugraph-cu12                  25.2.0\n",
      "pylibraft-cu12                     25.2.0\n",
      "pymc                               5.21.2\n",
      "pymystem3                          0.2.0\n",
      "pynndescent                        0.5.13\n",
      "pynvjitlink-cu12                   0.5.2\n",
      "pynvml                             12.0.0\n",
      "pyogrio                            0.10.0\n",
      "Pyomo                              6.8.2\n",
      "PyOpenGL                           3.1.9\n",
      "pyOpenSSL                          24.2.1\n",
      "pyparsing                          3.2.3\n",
      "pypdfium2                          4.30.1\n",
      "pyperclip                          1.9.0\n",
      "pyproj                             3.7.1\n",
      "pyshp                              2.3.1\n",
      "PySocks                            1.7.1\n",
      "pyspark                            3.5.5\n",
      "pytablewriter                      1.2.1\n",
      "pytensor                           2.30.2\n",
      "pytest                             8.3.5\n",
      "python-apt                         0.0.0\n",
      "python-bidi                        0.6.6\n",
      "python-box                         7.3.2\n",
      "python-dateutil                    2.8.2\n",
      "python-docx                        1.1.2\n",
      "python-dotenv                      1.1.0\n",
      "python-louvain                     0.16\n",
      "python-multipart                   0.0.20\n",
      "python-pptx                        1.0.2\n",
      "python-slugify                     8.0.4\n",
      "python-snappy                      0.7.3\n",
      "python-utils                       3.9.1\n",
      "pytz                               2025.2\n",
      "pyviz_comms                        3.0.4\n",
      "PyYAML                             6.0.2\n",
      "pyzmq                              24.0.1\n",
      "raft-dask-cu12                     25.2.0\n",
      "ragas                              0.2.14\n",
      "rapids-dask-dependency             25.2.0\n",
      "ratelim                            0.1.6\n",
      "ray                                2.40.0\n",
      "referencing                        0.36.2\n",
      "regex                              2024.11.6\n",
      "requests                           2.32.3\n",
      "requests-oauthlib                  2.0.0\n",
      "requests-toolbelt                  1.0.0\n",
      "requirements-parser                0.9.0\n",
      "rich                               13.9.4\n",
      "rich-toolkit                       0.14.1\n",
      "rmm-cu12                           25.2.0\n",
      "roman-numerals-py                  3.1.0\n",
      "rouge_score                        0.1.2\n",
      "rpds-py                            0.24.0\n",
      "rpy2                               3.5.17\n",
      "rsa                                4.9\n",
      "rtree                              1.4.0\n",
      "ruamel.yaml                        0.18.10\n",
      "ruamel.yaml.clib                   0.2.12\n",
      "s3transfer                         0.11.4\n",
      "sacrebleu                          2.5.1\n",
      "safetensors                        0.5.3\n",
      "scikit-image                       0.25.2\n",
      "scikit-learn                       1.6.1\n",
      "scipy                              1.14.1\n",
      "scooby                             0.10.0\n",
      "scs                                3.2.7.post2\n",
      "seaborn                            0.13.2\n",
      "SecretStorage                      3.3.1\n",
      "semchunk                           2.2.2\n",
      "Send2Trash                         1.8.3\n",
      "sentence-transformers              3.4.1\n",
      "sentencepiece                      0.2.0\n",
      "sentry-sdk                         2.25.1\n",
      "setproctitle                       1.3.5\n",
      "setuptools                         75.2.0\n",
      "shap                               0.47.1\n",
      "shapely                            2.1.0\n",
      "shellingham                        1.5.4\n",
      "shortuuid                          1.0.13\n",
      "simple-parsing                     0.1.7\n",
      "simplejson                         3.20.1\n",
      "simsimd                            6.2.1\n",
      "six                                1.17.0\n",
      "sklearn-compat                     0.1.3\n",
      "sklearn-pandas                     2.2.0\n",
      "slicer                             0.0.8\n",
      "smart-open                         7.1.0\n",
      "smmap                              5.0.2\n",
      "sniffio                            1.3.1\n",
      "snowballstemmer                    2.2.0\n",
      "sortedcontainers                   2.4.0\n",
      "soundfile                          0.13.1\n",
      "soupsieve                          2.6\n",
      "soxr                               0.5.0.post1\n",
      "spacy                              3.8.5\n",
      "spacy-legacy                       3.0.12\n",
      "spacy-loggers                      1.0.5\n",
      "spanner-graph-notebook             1.1.6\n",
      "Sphinx                             8.2.3\n",
      "sphinxcontrib-applehelp            2.0.0\n",
      "sphinxcontrib-devhelp              2.0.0\n",
      "sphinxcontrib-htmlhelp             2.1.0\n",
      "sphinxcontrib-jsmath               1.0.1\n",
      "sphinxcontrib-qthelp               2.0.0\n",
      "sphinxcontrib-serializinghtml      2.0.0\n",
      "SQLAlchemy                         2.0.40\n",
      "sqlglot                            25.20.2\n",
      "sqlitedict                         2.1.0\n",
      "sqlparse                           0.5.3\n",
      "srsly                              2.5.1\n",
      "sse-starlette                      2.2.1\n",
      "stanio                             0.5.1\n",
      "starlette                          0.46.1\n",
      "starlette-context                  0.3.6\n",
      "statsmodels                        0.14.4\n",
      "stringzilla                        3.12.3\n",
      "sympy                              1.13.1\n",
      "tabledata                          1.3.4\n",
      "tables                             3.10.2\n",
      "tabulate                           0.9.0\n",
      "tbb                                2022.1.0\n",
      "tblib                              3.1.0\n",
      "tcmlib                             1.3.0\n",
      "tcolorpy                           0.1.7\n",
      "tenacity                           9.1.2\n",
      "tensorboard                        2.18.0\n",
      "tensorboard-data-server            0.7.2\n",
      "tensorflow                         2.18.0\n",
      "tensorflow-datasets                4.9.8\n",
      "tensorflow_decision_forests        1.11.0\n",
      "tensorflow-hub                     0.16.1\n",
      "tensorflow-io-gcs-filesystem       0.37.1\n",
      "tensorflow-metadata                1.17.0\n",
      "tensorflow-probability             0.25.0\n",
      "tensorflow-text                    2.18.1\n",
      "tensorstore                        0.1.73\n",
      "termcolor                          3.0.1\n",
      "terminado                          0.18.1\n",
      "tesserocr                          2.8.0\n",
      "text-unidecode                     1.3\n",
      "textblob                           0.19.0\n",
      "tf_keras                           2.18.0\n",
      "tf-slim                            1.1.0\n",
      "thinc                              8.3.6\n",
      "threadpoolctl                      3.6.0\n",
      "tifffile                           2025.3.30\n",
      "tiktoken                           0.9.0\n",
      "timm                               1.0.15\n",
      "tinycss2                           1.4.0\n",
      "tokenizers                         0.21.1\n",
      "toml                               0.10.2\n",
      "toolz                              0.12.1\n",
      "torch                              2.5.1\n",
      "torchaudio                         2.5.1\n",
      "torchsummary                       1.5.1\n",
      "torchvision                        0.20.1\n",
      "tornado                            6.4.2\n",
      "tqdm                               4.67.1\n",
      "tqdm-multiprocess                  0.0.11\n",
      "traitlets                          5.7.1\n",
      "traittypes                         0.2.1\n",
      "transformers                       4.50.3\n",
      "treelite                           4.4.1\n",
      "treescope                          0.1.9\n",
      "triton                             3.1.0\n",
      "trl                                0.14.0\n",
      "tweepy                             4.15.0\n",
      "typeguard                          4.4.2\n",
      "typepy                             1.3.4\n",
      "typer                              0.12.5\n",
      "types-pytz                         2025.2.0.20250326\n",
      "types-setuptools                   78.1.0.20250329\n",
      "typing_extensions                  4.13.1\n",
      "typing-inspect                     0.9.0\n",
      "typing-inspection                  0.4.0\n",
      "tzdata                             2025.2\n",
      "tzlocal                            5.3.1\n",
      "uc-micro-py                        1.0.3\n",
      "ucx-py-cu12                        0.42.0\n",
      "ucxx-cu12                          0.42.0\n",
      "umap-learn                         0.5.7\n",
      "umf                                0.10.0\n",
      "uritemplate                        4.1.1\n",
      "urllib3                            2.3.0\n",
      "uvicorn                            0.34.0\n",
      "uvloop                             0.21.0\n",
      "vega-datasets                      0.9.0\n",
      "vllm                               0.7.3\n",
      "wadllib                            1.3.6\n",
      "wandb                              0.19.9\n",
      "wasabi                             1.1.3\n",
      "watchfiles                         1.0.5\n",
      "wcwidth                            0.2.13\n",
      "weasel                             0.4.1\n",
      "webcolors                          24.11.1\n",
      "webencodings                       0.5.1\n",
      "websocket-client                   1.8.0\n",
      "websockets                         15.0.1\n",
      "Werkzeug                           3.1.3\n",
      "wheel                              0.45.1\n",
      "widgetsnbextension                 3.6.10\n",
      "word2number                        1.1\n",
      "wordcloud                          1.9.4\n",
      "wrapt                              1.17.2\n",
      "wurlitzer                          3.1.1\n",
      "xarray                             2025.1.2\n",
      "xarray-einstats                    0.8.0\n",
      "xdg-base-dirs                      6.0.2\n",
      "xformers                           0.0.28.post3\n",
      "xgboost                            2.1.4\n",
      "xgrammar                           0.1.11\n",
      "xlrd                               2.0.1\n",
      "XlsxWriter                         3.2.2\n",
      "xxhash                             3.5.0\n",
      "xyzservices                        2025.1.0\n",
      "yamllint                           1.37.0\n",
      "yarl                               1.18.3\n",
      "ydf                                0.11.0\n",
      "yellowbrick                        1.5\n",
      "yfinance                           0.2.55\n",
      "zict                               3.0.0\n",
      "zipp                               3.21.0\n",
      "zstandard                          0.23.0\n"
     ]
    }
   ],
   "source": [
    "# Run the rest of the notebook by selecting this third cell and choosing \"Runtime->Run cell and below\"\n",
    "!pip cache remove llama_cpp_python\n",
    "!pip install -r ilab/requirements_gpu.txt\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Mlth0Vetawj"
   },
   "source": [
    "Wrap code cell output for ease of reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1744451290352,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "abrPE0P18BWC"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_TTU5vUhXvM",
    "tags": []
   },
   "source": [
    "## Step 1.5 Check Starting Configuration\n",
    "### Check InstructLab Version\n",
    "\n",
    "Check that InstructLab is installed properly and is configured for using a GPU.\n",
    "\n",
    "The first line from 'InstructLab' section will give the InstructLab version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "executionInfo": {
     "elapsed": 3284,
     "status": "ok",
     "timestamp": 1744451297054,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "BqjqIGE06rYE",
    "outputId": "8de7c7ce-bf42-4b5b-995a-49ce78e9d4b0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes\n",
      "Platform:\n",
      "  sys.version: 3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]\n",
      "  sys.platform: linux\n",
      "  os.name: posix\n",
      "  platform.release: 6.1.85+\n",
      "  platform.machine: x86_64\n",
      "  platform.node: 607bb9a9d368\n",
      "  platform.python_version: 3.11.12\n",
      "  os-release.ID: ubuntu\n",
      "  os-release.VERSION_ID: 22.04\n",
      "  os-release.PRETTY_NAME: Ubuntu 22.04.4 LTS\n",
      "  memory.total: 83.48 GB\n",
      "  memory.available: 81.38 GB\n",
      "  memory.used: 1.23 GB\n",
      "\n",
      "InstructLab:\n",
      "  instructlab.version: 0.24.3\n",
      "  instructlab-dolomite.version: 0.2.0\n",
      "  instructlab-eval.version: 0.5.1\n",
      "  instructlab-quantize.version: 0.1.0\n",
      "  instructlab-schema.version: 0.4.2\n",
      "  instructlab-sdg.version: 0.7.3\n",
      "  instructlab-training.version: 0.7.0\n",
      "\n",
      "Torch:\n",
      "  torch.version: 2.5.1+cu124\n",
      "  torch.backends.cpu.capability: AVX512\n",
      "  torch.version.cuda: 12.4\n",
      "  torch.version.hip: None\n",
      "  torch.cuda.available: True\n",
      "  torch.backends.cuda.is_built: True\n",
      "  torch.backends.mps.is_built: False\n",
      "  torch.backends.mps.is_available: False\n",
      "  torch.cuda.bf16: True\n",
      "  torch.cuda.current.device: 0\n",
      "  torch.cuda.0.name: NVIDIA A100-SXM4-40GB\n",
      "  torch.cuda.0.free: 39.1 GB\n",
      "  torch.cuda.0.total: 39.6 GB\n",
      "  torch.cuda.0.capability: 8.0 (see https://developer.nvidia.com/cuda-gpus#compute)\n",
      "\n",
      "llama_cpp_python:\n",
      "  llama_cpp_python.version: 0.3.6\n",
      "  llama_cpp_python.supports_gpu_offload: True\n"
     ]
    }
   ],
   "source": [
    "!ilab system info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5EcJoSS6rYD",
    "tags": []
   },
   "source": [
    "<a id=\"IL1_check\"></a>\n",
    "## Perform Imports and Check for a GPU\n",
    "\n",
    "This code cell checks for a GPU in the configuration. This notebook requires a GPU in the configuration to run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 3978,
     "status": "ok",
     "timestamp": 1744451305597,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "kLDdSBiX6rYD",
    "outputId": "f56a1fce-2675-415f-c858-4e349845476b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch:  2.5 ; cuda:  cu124\n",
      "GPU(s) are Available\n",
      "One GPU of Type:  NVIDIA A100-SXM4-40GB\n",
      "Starting directory: /content/ilab\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from IPython.display import Image, display\n",
    "from datasets import load_dataset\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import json\n",
    "import subprocess\n",
    "import shutil\n",
    "import ruamel.yaml\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '64'\n",
    "Norm = \"<p style='font-family:IBM Plex Sans;font-size:20px'>\"\n",
    "\n",
    "notebook_dir='/content/ilab/'\n",
    "os.chdir(notebook_dir)\n",
    "\n",
    "with open('config.json', 'r') as f:\n",
    "    jsonData = json.load(f)\n",
    "\n",
    "## torch and cuda version check\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "\n",
    "if torch.cuda.is_available() is False:\n",
    "    print(\"No GPU in configuration\")\n",
    "else:\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    print(\"GPU(s) are Available\")\n",
    "    gpus=torch.cuda.device_count()\n",
    "    if gpus==1:\n",
    "      gpu_type=torch.cuda.get_device_name(0)\n",
    "      print(\"One GPU of Type: \", gpu_type)\n",
    "    else:\n",
    "      print(\"ERROR: More than 1 GPU in configuration: \",gpus)\n",
    "print(\"Starting directory: \"+ os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv5uMvQ-a-ZF",
    "tags": []
   },
   "source": [
    "<a id=\"IL1_config\"></a>\n",
    "## Step 1.6 Configure InstructLab\n",
    "\n",
    "### Create InstructLab config file\n",
    "The InstructLab configuration is captured in the *config.yaml* file. This step creates the config.yaml file and sets:\n",
    "- **taxomony_path = taxonomy** - the root location of the taxonomy is set to the taxonomy folder in instructlab-latest\n",
    "- **model_path = models/merlinite-7b-lab-Q4_K_M.gguf** - the default model is set to merlinite\n",
    "\n",
    "**Note:** The default directories for InstructLab are the following. If you initialize InstructLab on your own system, it will default to the following:\n",
    "* **Downloaded Models:**  ~/.cache/instructlab/models/ - Contains all downloaded large language models, including the saved output of ones you generate with ilab.\n",
    "* **Synthetic Data:** ~/.local/share/instructlab/datasets/ - Contains data output from the SDG phase, built on modifications to the taxonomy repository.\n",
    "* **Taxonomy:** ~/.local/share/instructlab/taxonomy/ - Contains the skill and knowledge data.\n",
    "* **Training Output:** ~/.local/share/instructlab/checkpoints/ - Contains the output of the training process.\n",
    "* **config.yaml:** ~/.config/instructlab/config.yaml - Contains the config.yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "executionInfo": {
     "elapsed": 4013,
     "status": "ok",
     "timestamp": 1744451317286,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "QX9s4XZx6rYF",
    "outputId": "67cfcf67-51eb-4eef-c5ca-b037767ea8f8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ilab was not initialized yet. config.yaml does not exist.\n",
      "ilab model is models/meta-llama/Llama-3.2-3B-Instruct.\n",
      "#############################################################\n",
      " \n",
      "Running ilab config init\n",
      "\n",
      "----------------------------------------------------\n",
      "         Welcome to the InstructLab CLI\n",
      "  This guide will help you to setup your environment\n",
      "----------------------------------------------------\n",
      "\n",
      "Please provide the following values to initiate the environment [press 'Enter' for default options when prompted]\n",
      "Path to taxonomy repo [/root/.local/share/instructlab/taxonomy]: `taxonomy` seems to not exist or is empty.\n",
      "Should I clone https://github.com/instructlab/taxonomy.git for you? [Y/n]: Cloning https://github.com/instructlab/taxonomy.git...\n",
      "Path to your model [/root/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf]: \n",
      "Generating config file:\n",
      "    /root/.config/instructlab/config.yaml\n",
      "\n",
      "INFO 2025-04-12 09:48:36,353 instructlab.config.init:262: Detecting hardware...\n",
      "\u001b[32mPlease choose a system profile.\n",
      "Profiles set hardware-specific defaults for all commands and sections of the configuration.\u001b[0m\n",
      "\u001b[37m\u001b[44mFirst, please select the hardware vendor your system falls into\u001b[0m\n",
      "[0] NO SYSTEM PROFILE\n",
      "[1] NVIDIA\n",
      "[2] AMD\n",
      "[3] APPLE\n",
      "[4] INTEL\n",
      "Enter the number of your choice [0]: No profile selected - ilab will use generic code defaults - these may not be optimized for your system.\n",
      "\n",
      "--------------------------------------------\n",
      "    Initialization completed successfully!\n",
      "  You're ready to start using `ilab`. Enjoy!\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Remove Colab Sample directory\n",
    "if os.path.exists(\"sample_data\"):\n",
    "    print(\"removing sample_data\")\n",
    "    shutil.rmtree(\"sample_data\")\n",
    "    os.chdir(\"ilab\")\n",
    "\n",
    "#Initialize ilab\n",
    "base_dir=\"/root/\"\n",
    "##Choose the base model as granite or mixtral\n",
    "model_dir=\"models\"\n",
    "model_name=\"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "taxonomy_path='taxonomy'\n",
    "\n",
    "## Define the file name\n",
    "file_name = \"config.yaml\"\n",
    "if os.path.exists(file_name):\n",
    "    os.remove(file_name)\n",
    "    print(f\"ilab was already initialized. {file_name} has been deleted. Reinitialized\")\n",
    "else:\n",
    "    print(f\"ilab was not initialized yet. {file_name} does not exist.\")\n",
    "\n",
    "##Remove old data\n",
    "if os.path.exists(\"taxonomy\"):\n",
    "    print(\"removing taxonomy\")\n",
    "    shutil.rmtree(\"taxonomy\")\n",
    "if os.path.exists(base_dir+\".cache/instructlab\"):\n",
    "    print(\"removing \" + base_dir+\".cache/instructlab\")\n",
    "    shutil.rmtree(base_dir+\".cache/instructlab\")\n",
    "if os.path.exists(base_dir+\".config/instructlab\"):\n",
    "    print(\"removing \" + base_dir+\".config/instructlab\")\n",
    "    shutil.rmtree(base_dir+\".config/instructlab\")\n",
    "if os.path.exists(base_dir+\".local/share/instructlab\"):\n",
    "    print(\"removing \" + base_dir+\".local/share/instructlab\")\n",
    "    shutil.rmtree(base_dir+\".local/share/instructlab\")\n",
    "\n",
    "print(f\"ilab model is {model_path}.\")\n",
    "print('#############################################################')\n",
    "print(' ')\n",
    "\n",
    "command = f\"\"\"\n",
    "ilab config init<<EOF\n",
    "{taxonomy_path}\n",
    "Y\n",
    "{model_path}\n",
    "0\n",
    "EOF\n",
    "\"\"\"\n",
    "\n",
    "## Using the ! operator to run the command\n",
    "!echo \"Running ilab config init\"\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8rbeRKE6rYF"
   },
   "source": [
    "### Display the config.yaml file\n",
    "We examine the base configuration for identifying parameters for changing in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1744451327441,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "ZflN-eeu6rYF",
    "outputId": "a3fb81e9-909f-4bdf-a20b-425fa8e5b2df",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Chat configuration section.\n",
      "chat:\n",
      "  # Predefined setting or environment that influences the behavior and responses of\n",
      "  # the chat assistant. Each context is associated with a specific prompt that\n",
      "  # guides the assistant on how to respond to user inputs. Available contexts:\n",
      "  # default, cli_helper.\n",
      "  # Default: default\n",
      "  context: default\n",
      "  # Directory where chat logs are stored.\n",
      "  # Default: /root/.local/share/instructlab/chatlogs\n",
      "  logs_dir: /root/.local/share/instructlab/chatlogs\n",
      "  # The maximum number of tokens that can be generated in the chat completion. Be\n",
      "  # aware that larger values use more memory.\n",
      "  # Default: None\n",
      "  max_tokens:\n",
      "  # Model to be used for chatting with.\n",
      "  # Default: /root/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf\n",
      "  model: models/meta-llama/Llama-3.2-3B-Instruct\n",
      "  # Filepath of a dialog session file.\n",
      "  # Default: None\n",
      "  session:\n",
      "  # Controls the randomness of the model's responses. Lower values make the output\n",
      "  # more deterministic, while higher values produce more random results.\n",
      "  # Default: 1.0\n",
      "  temperature: 1.0\n",
      "  # Enable vim keybindings for chat.\n",
      "  # Default: False\n",
      "  vi_mode: false\n",
      "  # Renders vertical overflow if enabled, displays ellipses otherwise.\n",
      "  # Default: True\n",
      "  visible_overflow: true\n",
      "# Evaluate configuration section.\n",
      "evaluate:\n",
      "  # Base taxonomy branch\n",
      "  # Default: None\n",
      "  base_branch:\n",
      "  # Base model to compare with 'model' for mt_bench_branch and mmlu_branch.\n",
      "  # Default: instructlab/granite-7b-lab\n",
      "  base_model: instructlab/granite-7b-lab\n",
      "  # Taxonomy branch containing custom skills/knowledge that should be used for\n",
      "  # evaluation runs.\n",
      "  # Default: None\n",
      "  branch:\n",
      "  # Settings to run DK-Bench against a file of user created questions, reference\n",
      "  # answers, and responses. If responses are not provided they are generated from a\n",
      "  # model\n",
      "  dk_bench:\n",
      "    # File with questions and reference answers used for evaluation during DK-Bench.\n",
      "    # The file must be valid a '.jsonl' file with the fields 'user_input' and\n",
      "    # 'reference' in each entry\n",
      "    # Default: None\n",
      "    input_questions:\n",
      "    # Judge model for DK-Bench.\n",
      "    # Default: gpt-4o\n",
      "    judge_model: gpt-4o\n",
      "    # Directory where DK-Bench evaluation results are stored.\n",
      "    # Default: /root/.local/share/instructlab/internal/eval_data/dk_bench\n",
      "    output_dir: /root/.local/share/instructlab/internal/eval_data/dk_bench\n",
      "    # Comma-separated list of file formats for results of the DK-Bench evaluation. Ex:\n",
      "    # 'csv,jsonl'. Valid options in the list are csv, jsonl, and xlsx. If this option\n",
      "    # is not provided the results are written as a .jsonl file\n",
      "    # Default: jsonl\n",
      "    output_file_formats: jsonl\n",
      "  # Number of GPUs to use for running evaluation.\n",
      "  # Default: None\n",
      "  gpus:\n",
      "  # MMLU benchmarking settings\n",
      "  mmlu:\n",
      "    # Batch size for evaluation. Valid values are a positive integer or 'auto' to\n",
      "    # select the largest batch size that will fit in memory.\n",
      "    # Default: auto\n",
      "    batch_size: auto\n",
      "    # Number of question-answer pairs provided in the context preceding the question\n",
      "    # used for evaluation.\n",
      "    # Default: 5\n",
      "    few_shots: 5\n",
      "  # Settings to run MMLU against a branch of taxonomy containing custom\n",
      "  # skills/knowledge used for training.\n",
      "  mmlu_branch:\n",
      "    # Directory where custom MMLU tasks are stored.\n",
      "    # Default: /root/.local/share/instructlab/datasets\n",
      "    tasks_dir: /root/.local/share/instructlab/datasets\n",
      "  # Model to be evaluated\n",
      "  # Default: None\n",
      "  model: models/meta-llama/Llama-3.2-3B-Instruct\n",
      "  # Multi-turn benchmarking settings for skills.\n",
      "  mt_bench:\n",
      "    # Judge model for MT-Bench.\n",
      "    # Default: prometheus-eval/prometheus-8x7b-v2.0\n",
      "    judge_model: prometheus-eval/prometheus-8x7b-v2.0\n",
      "    # Number of workers to use for evaluation with mt_bench or mt_bench_branch. Must\n",
      "    # be a positive integer or 'auto'.\n",
      "    # Default: auto\n",
      "    max_workers: auto\n",
      "    # Directory where MT-Bench evaluation results are stored.\n",
      "    # Default: /root/.local/share/instructlab/internal/eval_data/mt_bench\n",
      "    output_dir: /root/.local/share/instructlab/internal/eval_data/mt_bench\n",
      "  # Settings to run MT-Bench against a branch of taxonomy containing custom\n",
      "  # skills/knowledge used for training\n",
      "  mt_bench_branch:\n",
      "    # Judge model for MT-Bench-Branch.\n",
      "    # Default: prometheus-eval/prometheus-8x7b-v2.0\n",
      "    judge_model: prometheus-eval/prometheus-8x7b-v2.0\n",
      "    # Directory where MT-Bench-Branch evaluation results are stored.\n",
      "    # Default: /root/.local/share/instructlab/internal/eval_data/mt_bench_branch\n",
      "    output_dir: /root/.local/share/instructlab/internal/eval_data/mt_bench_branch\n",
      "    # Path to where base taxonomy is stored.\n",
      "    # Default: /root/.local/share/instructlab/taxonomy\n",
      "    taxonomy_path: taxonomy\n",
      "  # System prompt for model getting responses during DK-Bench.\n",
      "  # Default: None\n",
      "  system_prompt:\n",
      "  # Temperature for model getting responses during DK-Bench. Temperature controls\n",
      "  # the randomness of the model's responses. Lower values make the output more\n",
      "  # deterministic, while higher values produce more random results.\n",
      "  # Default: 0.0\n",
      "  temperature: 0.0\n",
      "# General configuration section.\n",
      "general:\n",
      "  # Debug level for logging.\n",
      "  # Default: 0\n",
      "  debug_level: 0\n",
      "  # Log format. https://docs.python.org/3/library/logging.html#logrecord-attributes\n",
      "  # Default: %(levelname)s %(asctime)s %(name)s:%(lineno)d: %(message)s\n",
      "  log_format: '%(levelname)s %(asctime)s %(name)s:%(lineno)d: %(message)s'\n",
      "  # Log level for logging.\n",
      "  # Default: INFO\n",
      "  log_level: INFO\n",
      "  # Use legacy IBM Granite chat template (default uses 3.0 Instruct template)\n",
      "  # Default: False\n",
      "  use_legacy_tmpl: false\n",
      "# Generate configuration section.\n",
      "generate:\n",
      "  # Number of Batches to send for generation on each core.\n",
      "  # Default: 8\n",
      "  batch_size: 8\n",
      "  # Maximum number of words per chunk.\n",
      "  # Default: 1000\n",
      "  chunk_word_count: 1000\n",
      "  # The maximum amount of tokens for the model to generate during knowledge\n",
      "  # generation. A lower number yields less data but a faster SDG run. It is\n",
      "  # reccomended to use this on consumer hardware\n",
      "  # Default: 4096\n",
      "  max_num_tokens: 4096\n",
      "  # Teacher model that will be used to synthetically generate training data.\n",
      "  # Default: /root/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
      "  model: /root/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
      "  # Number of CPU cores to use for generation.\n",
      "  # Default: 10\n",
      "  num_cpus: 10\n",
      "  # Number of instructions to use\n",
      "  # Default: -1\n",
      "  # Deprecated: see 'sdg_scale_factor' instead\n",
      "  num_instructions: -1\n",
      "  # Directory where generated datasets are stored.\n",
      "  # Default: /root/.local/share/instructlab/datasets\n",
      "  output_dir: /root/.local/share/instructlab/datasets\n",
      "  # Data generation pipeline to use. Available: 'simple', 'full', or a valid path to\n",
      "  # a directory of pipeline workflow YAML files. Note that 'full' requires a larger\n",
      "  # teacher model, Mixtral-8x7b.\n",
      "  # Default: full\n",
      "  pipeline: full\n",
      "  # The total number of instructions to be generated.\n",
      "  # Default: 30\n",
      "  sdg_scale_factor: 30\n",
      "  # Branch of taxonomy used to calculate diff against.\n",
      "  # Default: origin/main\n",
      "  taxonomy_base: origin/main\n",
      "  # Directory where taxonomy is stored and accessed from.\n",
      "  # Default: /root/.local/share/instructlab/taxonomy\n",
      "  taxonomy_path: taxonomy\n",
      "  # Teacher configuration\n",
      "  teacher:\n",
      "    # Serving backend to use to host the model.\n",
      "    # Default: None\n",
      "    # Examples:\n",
      "    #   - vllm\n",
      "    #   - llama-cpp\n",
      "    backend:\n",
      "    # Chat template to supply to the model. Possible values: 'auto'(default),\n",
      "    # 'tokenizer', a path to a jinja2 file.\n",
      "    # Default: None\n",
      "    # Examples:\n",
      "    #   - auto\n",
      "    #   - tokenizer\n",
      "    #   - A filesystem path expressing the location of a custom template\n",
      "    chat_template:\n",
      "    # llama-cpp serving settings.\n",
      "    llama_cpp:\n",
      "      # Number of model layers to offload to GPU. -1 means all layers.\n",
      "      # Default: -1\n",
      "      gpu_layers: -1\n",
      "      # Large Language Model Family\n",
      "      # Default: ''\n",
      "      # Examples:\n",
      "      #   - granite\n",
      "      #   - mixtral\n",
      "      llm_family: ''\n",
      "      # Maximum number of tokens that can be processed by the model.\n",
      "      # Default: 4096\n",
      "      max_ctx_size: 4096\n",
      "    # Directory where model to be served is stored.\n",
      "    # Default: /root/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf\n",
      "    model_path: /root/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
      "    # Server configuration including host and port.\n",
      "    # Default: host='127.0.0.1' port=8000 backend_type='' current_max_ctx_size=4096\n",
      "    server:\n",
      "      # Backend Instance Type\n",
      "      # Default: ''\n",
      "      # Examples:\n",
      "      #   - llama-cpp\n",
      "      #   - vllm\n",
      "      backend_type: ''\n",
      "      # Maximum number of tokens that can be processed by the currently served model.\n",
      "      # Default: 4096\n",
      "      current_max_ctx_size: 4096\n",
      "      # Host to serve on.\n",
      "      # Default: 127.0.0.1\n",
      "      host: 127.0.0.1\n",
      "      # Port to serve on.\n",
      "      # Default: 8000\n",
      "      port: 8000\n",
      "    # vLLM serving settings.\n",
      "    vllm:\n",
      "      # Number of GPUs to use.\n",
      "      # Default: None\n",
      "      gpus:\n",
      "      # Large Language Model Family\n",
      "      # Default: ''\n",
      "      # Examples:\n",
      "      #   - granite\n",
      "      #   - mixtral\n",
      "      llm_family: ''\n",
      "      # Maximum number of attempts to start the vLLM server.\n",
      "      # Default: 120\n",
      "      max_startup_attempts: 120\n",
      "      # vLLM specific arguments. All settings can be passed as a list of strings, see:\n",
      "      # https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n",
      "      # Default: []\n",
      "      # Examples:\n",
      "      #   - ['--dtype', 'auto']\n",
      "      #   - ['--lora-alpha', '32']\n",
      "      vllm_args: []\n",
      "# Metadata pertaining to the specifics of the system which the Configuration is\n",
      "# meant to be applied to.\n",
      "metadata:\n",
      "  # Manufacturer, Family, and SKU of the system CPU, ex: Apple M3 Max\n",
      "  # Default: None\n",
      "  cpu_info:\n",
      "  # Amount of GPUs on the system, ex: 8\n",
      "  # Default: None\n",
      "  gpu_count:\n",
      "  # Family of the system GPU, ex: H100\n",
      "  # Default: None\n",
      "  gpu_family:\n",
      "  # Manufacturer of the system GPU, ex: Nvidia\n",
      "  # Default: None\n",
      "  gpu_manufacturer:\n",
      "  # Specific SKU related information about the given GPU, ex: PCIe, NVL\n",
      "  # Default: None\n",
      "  gpu_sku:\n",
      "# RAG configuration section.\n",
      "rag:\n",
      "  # RAG convert configuration section.\n",
      "  convert:\n",
      "    # Directory where converted documents are stored.\n",
      "    # Default: /root/.local/share/instructlab/converted_documents\n",
      "    output_dir: /root/.local/share/instructlab/converted_documents\n",
      "    # Branch of taxonomy used to calculate diff against.\n",
      "    # Default: origin/main\n",
      "    taxonomy_base: origin/main\n",
      "    # Directory where taxonomy is stored and accessed from.\n",
      "    # Default: /root/.local/share/instructlab/taxonomy\n",
      "    taxonomy_path: /root/.local/share/instructlab/taxonomy\n",
      "  # Document store configuration for RAG.\n",
      "  document_store:\n",
      "    # Document store collection name.\n",
      "    # Default: ilab\n",
      "    collection_name: ilab\n",
      "    # Document store service URI.\n",
      "    # Default: /root/.local/share/instructlab/embeddings.db\n",
      "    uri: /root/.local/share/instructlab/embeddings.db\n",
      "  # Embedding model configuration for RAG\n",
      "  embedding_model:\n",
      "    # Embedding model to use for RAG.\n",
      "    # Default: /root/.cache/instructlab/models/ibm-granite/granite-embedding-125m-english\n",
      "    embedding_model_path: /root/.cache/instructlab/models/ibm-granite/granite-embedding-125m-english\n",
      "  # Flag for enabling RAG functionality.\n",
      "  # Default: False\n",
      "  enabled: false\n",
      "  # Retrieval configuration parameters for RAG\n",
      "  retriever:\n",
      "    # The maximum number of documents to retrieve.\n",
      "    # Default: 3\n",
      "    top_k: 3\n",
      "# Serve configuration section.\n",
      "serve:\n",
      "  # Serving backend to use to host the model.\n",
      "  # Default: None\n",
      "  # Examples:\n",
      "  #   - vllm\n",
      "  #   - llama-cpp\n",
      "  backend:\n",
      "  # Chat template to supply to the model. Possible values: 'auto'(default),\n",
      "  # 'tokenizer', a path to a jinja2 file.\n",
      "  # Default: None\n",
      "  # Examples:\n",
      "  #   - auto\n",
      "  #   - tokenizer\n",
      "  #   - A filesystem path expressing the location of a custom template\n",
      "  chat_template:\n",
      "  # llama-cpp serving settings.\n",
      "  llama_cpp:\n",
      "    # Number of model layers to offload to GPU. -1 means all layers.\n",
      "    # Default: -1\n",
      "    gpu_layers: -1\n",
      "    # Large Language Model Family\n",
      "    # Default: ''\n",
      "    # Examples:\n",
      "    #   - granite\n",
      "    #   - mixtral\n",
      "    llm_family: ''\n",
      "    # Maximum number of tokens that can be processed by the model.\n",
      "    # Default: 4096\n",
      "    max_ctx_size: 4096\n",
      "  # Directory where model to be served is stored.\n",
      "  # Default: /root/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf\n",
      "  model_path: models/meta-llama/Llama-3.2-3B-Instruct\n",
      "  # Server configuration including host and port.\n",
      "  # Default: host='127.0.0.1' port=8000 backend_type='' current_max_ctx_size=4096\n",
      "  server:\n",
      "    # Backend Instance Type\n",
      "    # Default: ''\n",
      "    # Examples:\n",
      "    #   - llama-cpp\n",
      "    #   - vllm\n",
      "    backend_type: ''\n",
      "    # Maximum number of tokens that can be processed by the currently served model.\n",
      "    # Default: 4096\n",
      "    current_max_ctx_size: 4096\n",
      "    # Host to serve on.\n",
      "    # Default: 127.0.0.1\n",
      "    host: 127.0.0.1\n",
      "    # Port to serve on.\n",
      "    # Default: 8000\n",
      "    port: 8000\n",
      "  # vLLM serving settings.\n",
      "  vllm:\n",
      "    # Number of GPUs to use.\n",
      "    # Default: None\n",
      "    gpus:\n",
      "    # Large Language Model Family\n",
      "    # Default: ''\n",
      "    # Examples:\n",
      "    #   - granite\n",
      "    #   - mixtral\n",
      "    llm_family: ''\n",
      "    # Maximum number of attempts to start the vLLM server.\n",
      "    # Default: 120\n",
      "    max_startup_attempts: 120\n",
      "    # vLLM specific arguments. All settings can be passed as a list of strings, see:\n",
      "    # https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n",
      "    # Default: []\n",
      "    # Examples:\n",
      "    #   - ['--dtype', 'auto']\n",
      "    #   - ['--lora-alpha', '32']\n",
      "    vllm_args: []\n",
      "# Train configuration section.\n",
      "train:\n",
      "  # Additional arguments to pass to the training script. These arguments are passed\n",
      "  # as key-value pairs to the training script.\n",
      "  # Default: {}\n",
      "  additional_args: {}\n",
      "  # Save a checkpoint at the end of each epoch.\n",
      "  # Default: True\n",
      "  checkpoint_at_epoch: true\n",
      "  # Directory where periodic training checkpoints are stored.\n",
      "  # Default: /root/.local/share/instructlab/checkpoints\n",
      "  ckpt_output_dir: /root/.local/share/instructlab/checkpoints\n",
      "  # Directory where the processed training data is stored (post\n",
      "  # filtering/tokenization/masking).\n",
      "  # Default: /root/.local/share/instructlab/internal\n",
      "  data_output_dir: /root/.local/share/instructlab/internal\n",
      "  # For the training library (primary training method), this specifies the path to\n",
      "  # the dataset file. For legacy training (MacOS/Linux), this specifies the path to\n",
      "  # the directory.\n",
      "  # Default: /root/.local/share/instructlab/datasets\n",
      "  data_path: /root/.local/share/instructlab/datasets\n",
      "  # Allow CPU offload for deepspeed optimizer.\n",
      "  # Default: False\n",
      "  deepspeed_cpu_offload_optimizer: false\n",
      "  # PyTorch device to use. Use 'cpu' for 'simple' and 'full' training on Linux. Use\n",
      "  # 'mps' for 'full' training on MacOS Metal Performance Shader. Use 'cuda' for\n",
      "  # Nvidia CUDA / AMD ROCm GPUs. Use 'hpu' for Intel Gaudi GPUs.\n",
      "  # Default: cpu\n",
      "  # Examples:\n",
      "  #   - cpu\n",
      "  #   - mps\n",
      "  #   - cuda\n",
      "  #   - hpu\n",
      "  device: cpu\n",
      "  # Whether or not we should disable the use of flash-attention during training.\n",
      "  # This is useful when using older GPUs.\n",
      "  # Default: False\n",
      "  disable_flash_attn: false\n",
      "  # Pick a distributed training backend framework for GPU accelerated full fine-\n",
      "  # tuning.\n",
      "  # Default: fsdp\n",
      "  distributed_backend: fsdp\n",
      "  # The number of samples in a batch that the model should see before its parameters\n",
      "  # are updated.\n",
      "  # Default: 64\n",
      "  effective_batch_size: 64\n",
      "  # Allow CPU offload for FSDP optimizer.\n",
      "  # Default: False\n",
      "  fsdp_cpu_offload_optimizer: false\n",
      "  # Boolean to indicate if the model being trained is a padding-free transformer\n",
      "  # model such as Granite.\n",
      "  # Default: False\n",
      "  is_padding_free: false\n",
      "  # The data type for quantization in LoRA training. Valid options are 'None' and\n",
      "  # 'nf4'.\n",
      "  # Default: nf4\n",
      "  # Examples:\n",
      "  #   - nf4\n",
      "  lora_quantize_dtype: nf4\n",
      "  # Rank of low rank matrices to be used during training.\n",
      "  # Default: 0\n",
      "  lora_rank: 0\n",
      "  # Maximum tokens per gpu for each batch that will be handled in a single step. If\n",
      "  # running into out-of-memory errors, this value can be lowered but not below the\n",
      "  # `max_seq_len`.\n",
      "  # Default: 5000\n",
      "  max_batch_len: 5000\n",
      "  # Maximum sequence length to be included in the training set. Samples exceeding\n",
      "  # this length will be dropped.\n",
      "  # Default: 4096\n",
      "  max_seq_len: 4096\n",
      "  # Directory where the model to be trained is stored.\n",
      "  # Default: instructlab/granite-7b-lab\n",
      "  model_path: instructlab/granite-7b-lab\n",
      "  # Number of GPUs to use for training. This value is not supported in legacy\n",
      "  # training or MacOS.\n",
      "  # Default: 1\n",
      "  nproc_per_node: 1\n",
      "  # Number of epochs to run training for.\n",
      "  # Default: 10\n",
      "  num_epochs: 10\n",
      "  # Base directory for organization of end-to-end intermediate outputs.\n",
      "  # Default: /root/.local/share/instructlab/phased\n",
      "  phased_base_dir: /root/.local/share/instructlab/phased\n",
      "  # Judge model path for phased MT-Bench evaluation.\n",
      "  # Default: /root/.cache/instructlab/models/prometheus-eval/prometheus-8x7b-v2.0\n",
      "  phased_mt_bench_judge: /root/.cache/instructlab/models/prometheus-eval/prometheus-8x7b-v2.0\n",
      "  # Phased phase1 effective batch size.\n",
      "  # Default: 128\n",
      "  phased_phase1_effective_batch_size: 128\n",
      "  # Learning rate for phase1 knowledge training.\n",
      "  # Default: 2e-05\n",
      "  phased_phase1_learning_rate: 2e-05\n",
      "  # Number of epochs to run training for during phase1 (experimentally optimal\n",
      "  # number is 7).\n",
      "  # Default: 7\n",
      "  phased_phase1_num_epochs: 7\n",
      "  # Number of samples the model should see before saving a checkpoint during phase1.\n",
      "  # Disabled when set to 0.\n",
      "  # Default: 0\n",
      "  phased_phase1_samples_per_save: 0\n",
      "  # Phased phase2 effective batch size.\n",
      "  # Default: 3840\n",
      "  phased_phase2_effective_batch_size: 3840\n",
      "  # Learning rate for phase2 skills training.\n",
      "  # Default: 6e-06\n",
      "  phased_phase2_learning_rate: 6e-06\n",
      "  # Number of epochs to run training for during phase2.\n",
      "  # Default: 10\n",
      "  phased_phase2_num_epochs: 10\n",
      "  # Number of samples the model should see before saving a checkpoint during phase2.\n",
      "  # Disabled when set to 0.\n",
      "  # Default: 0\n",
      "  phased_phase2_samples_per_save: 0\n",
      "  # Training pipeline to use. Simple is for systems with limited resources, full is\n",
      "  # for more capable consumer systems (64 GB of RAM), and accelerated is for systems\n",
      "  # with a dedicated GPU.\n",
      "  # Default: full\n",
      "  # Examples:\n",
      "  #   - simple\n",
      "  #   - full\n",
      "  #   - accelerated\n",
      "  pipeline: full\n",
      "  # Number of samples the model should see before saving a checkpoint.\n",
      "  # Default: 250000\n",
      "  save_samples: 250000\n",
      "  # Optional path to a yaml file that tracks the progress of multiphase training.\n",
      "  # Default: None\n",
      "  training_journal:\n",
      "# Configuration file structure version.\n",
      "# Default: 1.0.0\n",
      "version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "##to copy config.yaml to local directory\n",
    "!cp /root/.config/instructlab/config.yaml .\n",
    "!cat config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAL743VA6rYH"
   },
   "source": [
    "### Customize LLM Models and copy to notebook for use\n",
    "\n",
    "This cell changes the models to use for the generate stage. The mistral model as the teacher model in the generate step and as the student model to be trained.\n",
    "\n",
    "If you want to customize other models for generation or the training phase, you would specify the models in this step.\n",
    "\n",
    "This step specifies that the models to be used will be from this notebook's models directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1744451332390,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "QILAQZYY6rYH",
    "outputId": "608f241f-16a7-415c-98d9-072a7362efd3",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated config.yaml successfully.\n",
      "\n",
      "# Chat configuration section.\n",
      "chat:\n",
      "  # Predefined setting or environment that influences the behavior and responses of\n",
      "  # the chat assistant. Each context is associated with a specific prompt that\n",
      "  # guides the assistant on how to respond to user inputs. Available contexts:\n",
      "  # default, cli_helper.\n",
      "  # Default: default\n",
      "  context: default\n",
      "  # Directory where chat logs are stored.\n",
      "  # Default: /root/.local/share/instructlab/chatlogs\n",
      "  logs_dir: /root/.local/share/instructlab/chatlogs\n",
      "  # The maximum number of tokens that can be generated in the chat completion. Be\n",
      "  # aware that larger values use more memory.\n",
      "  # Default: None\n",
      "  max_tokens:\n",
      "  # Model to be used for chatting with.\n",
      "  # Default: /root/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf\n",
      "  model: models/meta-llama/Llama-3.2-3B-Instruct\n",
      "  # Filepath of a dialog session file.\n",
      "  # Default: None\n",
      "  session:\n",
      "  # Controls the randomness of the model's responses. Lower values make the output\n",
      "  # more deterministic, while higher values produce more random results.\n",
      "  # Default: 1.0\n",
      "  temperature: 1.0\n",
      "  # Enable vim keybindings for chat.\n",
      "  # Default: False\n",
      "  vi_mode: false\n",
      "  # Renders vertical overflow if enabled, displays ellipses otherwise.\n",
      "  # Default: True\n",
      "  visible_overflow: true\n",
      "# Evaluate configuration section.\n",
      "evaluate:\n",
      "  # Base taxonomy branch\n",
      "  # Default: None\n",
      "  base_branch:\n",
      "  # Base model to compare with 'model' for mt_bench_branch and mmlu_branch.\n",
      "  # Default: instructlab/granite-7b-lab\n",
      "  base_model: instructlab/granite-7b-lab\n",
      "  # Taxonomy branch containing custom skills/knowledge that should be used for\n",
      "  # evaluation runs.\n",
      "  # Default: None\n",
      "  branch:\n",
      "  # Settings to run DK-Bench against a file of user created questions, reference\n",
      "  # answers, and responses. If responses are not provided they are generated from a\n",
      "  # model\n",
      "  dk_bench:\n",
      "    # File with questions and reference answers used for evaluation during DK-Bench.\n",
      "    # The file must be valid a '.jsonl' file with the fields 'user_input' and\n",
      "    # 'reference' in each entry\n",
      "    # Default: None\n",
      "    input_questions:\n",
      "    # Judge model for DK-Bench.\n",
      "    # Default: gpt-4o\n",
      "    judge_model: gpt-4o\n",
      "    # Directory where DK-Bench evaluation results are stored.\n",
      "    # Default: /root/.local/share/instructlab/internal/eval_data/dk_bench\n",
      "    output_dir: /root/.local/share/instructlab/internal/eval_data/dk_bench\n",
      "    # Comma-separated list of file formats for results of the DK-Bench evaluation. Ex:\n",
      "    # 'csv,jsonl'. Valid options in the list are csv, jsonl, and xlsx. If this option\n",
      "    # is not provided the results are written as a .jsonl file\n",
      "    # Default: jsonl\n",
      "    output_file_formats: jsonl\n",
      "  # Number of GPUs to use for running evaluation.\n",
      "  # Default: None\n",
      "  gpus: 1\n",
      "  # MMLU benchmarking settings\n",
      "  mmlu:\n",
      "    # Batch size for evaluation. Valid values are a positive integer or 'auto' to\n",
      "    # select the largest batch size that will fit in memory.\n",
      "    # Default: auto\n",
      "    batch_size: auto\n",
      "    # Number of question-answer pairs provided in the context preceding the question\n",
      "    # used for evaluation.\n",
      "    # Default: 5\n",
      "    few_shots: 5\n",
      "  # Settings to run MMLU against a branch of taxonomy containing custom\n",
      "  # skills/knowledge used for training.\n",
      "  mmlu_branch:\n",
      "    # Directory where custom MMLU tasks are stored.\n",
      "    # Default: /root/.local/share/instructlab/datasets\n",
      "    tasks_dir: /root/.local/share/instructlab/datasets\n",
      "  # Model to be evaluated\n",
      "  # Default: None\n",
      "  model: models/meta-llama/Llama-3.2-3B-Instruct\n",
      "  # Multi-turn benchmarking settings for skills.\n",
      "  mt_bench:\n",
      "    # Judge model for MT-Bench.\n",
      "    # Default: prometheus-eval/prometheus-8x7b-v2.0\n",
      "    judge_model: prometheus-eval/prometheus-8x7b-v2.0\n",
      "    # Number of workers to use for evaluation with mt_bench or mt_bench_branch. Must\n",
      "    # be a positive integer or 'auto'.\n",
      "    # Default: auto\n",
      "    max_workers: auto\n",
      "    # Directory where MT-Bench evaluation results are stored.\n",
      "    # Default: /root/.local/share/instructlab/internal/eval_data/mt_bench\n",
      "    output_dir: /root/.local/share/instructlab/internal/eval_data/mt_bench\n",
      "  # Settings to run MT-Bench against a branch of taxonomy containing custom\n",
      "  # skills/knowledge used for training\n",
      "  mt_bench_branch:\n",
      "    # Judge model for MT-Bench-Branch.\n",
      "    # Default: prometheus-eval/prometheus-8x7b-v2.0\n",
      "    judge_model: prometheus-eval/prometheus-8x7b-v2.0\n",
      "    # Directory where MT-Bench-Branch evaluation results are stored.\n",
      "    # Default: /root/.local/share/instructlab/internal/eval_data/mt_bench_branch\n",
      "    output_dir: /root/.local/share/instructlab/internal/eval_data/mt_bench_branch\n",
      "    # Path to where base taxonomy is stored.\n",
      "    # Default: /root/.local/share/instructlab/taxonomy\n",
      "    taxonomy_path: taxonomy\n",
      "  # System prompt for model getting responses during DK-Bench.\n",
      "  # Default: None\n",
      "  system_prompt:\n",
      "  # Temperature for model getting responses during DK-Bench. Temperature controls\n",
      "  # the randomness of the model's responses. Lower values make the output more\n",
      "  # deterministic, while higher values produce more random results.\n",
      "  # Default: 0.0\n",
      "  temperature: 0.0\n",
      "# General configuration section.\n",
      "general:\n",
      "  # Debug level for logging.\n",
      "  # Default: 0\n",
      "  debug_level: 0\n",
      "  # Log format. https://docs.python.org/3/library/logging.html#logrecord-attributes\n",
      "  # Default: %(levelname)s %(asctime)s %(name)s:%(lineno)d: %(message)s\n",
      "  log_format: '%(levelname)s %(asctime)s %(name)s:%(lineno)d: %(message)s'\n",
      "  # Log level for logging.\n",
      "  # Default: INFO\n",
      "  log_level: INFO\n",
      "  # Use legacy IBM Granite chat template (default uses 3.0 Instruct template)\n",
      "  # Default: False\n",
      "  use_legacy_tmpl: false\n",
      "# Generate configuration section.\n",
      "generate:\n",
      "  # Number of Batches to send for generation on each core.\n",
      "  # Default: 8\n",
      "  batch_size: 8\n",
      "  # Maximum number of words per chunk.\n",
      "  # Default: 1000\n",
      "  chunk_word_count: 1000\n",
      "  # The maximum amount of tokens for the model to generate during knowledge\n",
      "  # generation. A lower number yields less data but a faster SDG run. It is\n",
      "  # reccomended to use this on consumer hardware\n",
      "  # Default: 4096\n",
      "  max_num_tokens: 4096\n",
      "  # Teacher model that will be used to synthetically generate training data.\n",
      "  # Default: /root/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
      "  model: models/mistralai/Mistral-7B-Instruct-v0.2\n",
      "  # Number of CPU cores to use for generation.\n",
      "  # Default: 10\n",
      "  num_cpus: 10\n",
      "  # Number of instructions to use\n",
      "  # Default: -1\n",
      "  # Deprecated: see 'sdg_scale_factor' instead\n",
      "  num_instructions: -1\n",
      "  # Directory where generated datasets are stored.\n",
      "  # Default: /root/.local/share/instructlab/datasets\n",
      "  output_dir: /root/.local/share/instructlab/datasets\n",
      "  # Data generation pipeline to use. Available: 'simple', 'full', or a valid path to\n",
      "  # a directory of pipeline workflow YAML files. Note that 'full' requires a larger\n",
      "  # teacher model, Mixtral-8x7b.\n",
      "  # Default: full\n",
      "  pipeline: full\n",
      "  # The total number of instructions to be generated.\n",
      "  # Default: 30\n",
      "  sdg_scale_factor: 30\n",
      "  # Branch of taxonomy used to calculate diff against.\n",
      "  # Default: origin/main\n",
      "  taxonomy_base: origin/main\n",
      "  # Directory where taxonomy is stored and accessed from.\n",
      "  # Default: /root/.local/share/instructlab/taxonomy\n",
      "  taxonomy_path: taxonomy\n",
      "  # Teacher configuration\n",
      "  teacher:\n",
      "    # Serving backend to use to host the model.\n",
      "    # Default: None\n",
      "    # Examples:\n",
      "    #   - vllm\n",
      "    #   - llama-cpp\n",
      "    backend:\n",
      "    # Chat template to supply to the model. Possible values: 'auto'(default),\n",
      "    # 'tokenizer', a path to a jinja2 file.\n",
      "    # Default: None\n",
      "    # Examples:\n",
      "    #   - auto\n",
      "    #   - tokenizer\n",
      "    #   - A filesystem path expressing the location of a custom template\n",
      "    chat_template:\n",
      "    # llama-cpp serving settings.\n",
      "    llama_cpp:\n",
      "      # Number of model layers to offload to GPU. -1 means all layers.\n",
      "      # Default: -1\n",
      "      gpu_layers: -1\n",
      "      # Large Language Model Family\n",
      "      # Default: ''\n",
      "      # Examples:\n",
      "      #   - granite\n",
      "      #   - mixtral\n",
      "      llm_family: ''\n",
      "      # Maximum number of tokens that can be processed by the model.\n",
      "      # Default: 4096\n",
      "      max_ctx_size: 4096\n",
      "    # Directory where model to be served is stored.\n",
      "    # Default: /root/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf\n",
      "    model_path: models/mistralai/Mistral-7B-Instruct-v0.2\n",
      "    # Server configuration including host and port.\n",
      "    # Default: host='127.0.0.1' port=8000 backend_type='' current_max_ctx_size=4096\n",
      "    server:\n",
      "      # Backend Instance Type\n",
      "      # Default: ''\n",
      "      # Examples:\n",
      "      #   - llama-cpp\n",
      "      #   - vllm\n",
      "      backend_type: ''\n",
      "      # Maximum number of tokens that can be processed by the currently served model.\n",
      "      # Default: 4096\n",
      "      current_max_ctx_size: 4096\n",
      "      # Host to serve on.\n",
      "      # Default: 127.0.0.1\n",
      "      host: 127.0.0.1\n",
      "      # Port to serve on.\n",
      "      # Default: 8000\n",
      "      port: 8000\n",
      "    # vLLM serving settings.\n",
      "    vllm:\n",
      "      # Number of GPUs to use.\n",
      "      # Default: None\n",
      "      gpus: 1\n",
      "      # Large Language Model Family\n",
      "      # Default: ''\n",
      "      # Examples:\n",
      "      #   - granite\n",
      "      #   - mixtral\n",
      "      llm_family: ''\n",
      "      # Maximum number of attempts to start the vLLM server.\n",
      "      # Default: 120\n",
      "      max_startup_attempts: 120\n",
      "      # vLLM specific arguments. All settings can be passed as a list of strings, see:\n",
      "      # https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n",
      "      # Default: []\n",
      "      # Examples:\n",
      "      #   - ['--dtype', 'auto']\n",
      "      #   - ['--lora-alpha', '32']\n",
      "      vllm_args: []\n",
      "# Metadata pertaining to the specifics of the system which the Configuration is\n",
      "# meant to be applied to.\n",
      "metadata:\n",
      "  # Manufacturer, Family, and SKU of the system CPU, ex: Apple M3 Max\n",
      "  # Default: None\n",
      "  cpu_info:\n",
      "  # Amount of GPUs on the system, ex: 8\n",
      "  # Default: None\n",
      "  gpu_count: 1\n",
      "  # Family of the system GPU, ex: H100\n",
      "  # Default: None\n",
      "  gpu_family: A100-SXM4-40GB\n",
      "  # Manufacturer of the system GPU, ex: Nvidia\n",
      "  # Default: None\n",
      "  gpu_manufacturer: Nvidia\n",
      "  # Specific SKU related information about the given GPU, ex: PCIe, NVL\n",
      "  # Default: None\n",
      "  gpu_sku:\n",
      "# RAG configuration section.\n",
      "rag:\n",
      "  # RAG convert configuration section.\n",
      "  convert:\n",
      "    # Directory where converted documents are stored.\n",
      "    # Default: /root/.local/share/instructlab/converted_documents\n",
      "    output_dir: /root/.local/share/instructlab/converted_documents\n",
      "    # Branch of taxonomy used to calculate diff against.\n",
      "    # Default: origin/main\n",
      "    taxonomy_base: origin/main\n",
      "    # Directory where taxonomy is stored and accessed from.\n",
      "    # Default: /root/.local/share/instructlab/taxonomy\n",
      "    taxonomy_path: /root/.local/share/instructlab/taxonomy\n",
      "  # Document store configuration for RAG.\n",
      "  document_store:\n",
      "    # Document store collection name.\n",
      "    # Default: ilab\n",
      "    collection_name: ilab\n",
      "    # Document store service URI.\n",
      "    # Default: /root/.local/share/instructlab/embeddings.db\n",
      "    uri: /root/.local/share/instructlab/embeddings.db\n",
      "  # Embedding model configuration for RAG\n",
      "  embedding_model:\n",
      "    # Embedding model to use for RAG.\n",
      "    # Default: /root/.cache/instructlab/models/ibm-granite/granite-embedding-125m-english\n",
      "    embedding_model_path: /root/.cache/instructlab/models/ibm-granite/granite-embedding-125m-english\n",
      "  # Flag for enabling RAG functionality.\n",
      "  # Default: False\n",
      "  enabled: false\n",
      "  # Retrieval configuration parameters for RAG\n",
      "  retriever:\n",
      "    # The maximum number of documents to retrieve.\n",
      "    # Default: 3\n",
      "    top_k: 3\n",
      "# Serve configuration section.\n",
      "serve:\n",
      "  # Serving backend to use to host the model.\n",
      "  # Default: None\n",
      "  # Examples:\n",
      "  #   - vllm\n",
      "  #   - llama-cpp\n",
      "  backend:\n",
      "  # Chat template to supply to the model. Possible values: 'auto'(default),\n",
      "  # 'tokenizer', a path to a jinja2 file.\n",
      "  # Default: None\n",
      "  # Examples:\n",
      "  #   - auto\n",
      "  #   - tokenizer\n",
      "  #   - A filesystem path expressing the location of a custom template\n",
      "  chat_template:\n",
      "  # llama-cpp serving settings.\n",
      "  llama_cpp:\n",
      "    # Number of model layers to offload to GPU. -1 means all layers.\n",
      "    # Default: -1\n",
      "    gpu_layers: -1\n",
      "    # Large Language Model Family\n",
      "    # Default: ''\n",
      "    # Examples:\n",
      "    #   - granite\n",
      "    #   - mixtral\n",
      "    llm_family: ''\n",
      "    # Maximum number of tokens that can be processed by the model.\n",
      "    # Default: 4096\n",
      "    max_ctx_size: 4096\n",
      "  # Directory where model to be served is stored.\n",
      "  # Default: /root/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf\n",
      "  model_path: models/meta-llama/Llama-3.2-3B-Instruct\n",
      "  # Server configuration including host and port.\n",
      "  # Default: host='127.0.0.1' port=8000 backend_type='' current_max_ctx_size=4096\n",
      "  server:\n",
      "    # Backend Instance Type\n",
      "    # Default: ''\n",
      "    # Examples:\n",
      "    #   - llama-cpp\n",
      "    #   - vllm\n",
      "    backend_type: ''\n",
      "    # Maximum number of tokens that can be processed by the currently served model.\n",
      "    # Default: 4096\n",
      "    current_max_ctx_size: 4096\n",
      "    # Host to serve on.\n",
      "    # Default: 127.0.0.1\n",
      "    host: 127.0.0.1\n",
      "    # Port to serve on.\n",
      "    # Default: 8000\n",
      "    port: 8000\n",
      "  # vLLM serving settings.\n",
      "  vllm:\n",
      "    # Number of GPUs to use.\n",
      "    # Default: None\n",
      "    gpus: 1\n",
      "    # Large Language Model Family\n",
      "    # Default: ''\n",
      "    # Examples:\n",
      "    #   - granite\n",
      "    #   - mixtral\n",
      "    llm_family: ''\n",
      "    # Maximum number of attempts to start the vLLM server.\n",
      "    # Default: 120\n",
      "    max_startup_attempts: 120\n",
      "    # vLLM specific arguments. All settings can be passed as a list of strings, see:\n",
      "    # https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n",
      "    # Default: []\n",
      "    # Examples:\n",
      "    #   - ['--dtype', 'auto']\n",
      "    #   - ['--lora-alpha', '32']\n",
      "    vllm_args: []\n",
      "# Train configuration section.\n",
      "train:\n",
      "  # Additional arguments to pass to the training script. These arguments are passed\n",
      "  # as key-value pairs to the training script.\n",
      "  # Default: {}\n",
      "  additional_args: {}\n",
      "  # Save a checkpoint at the end of each epoch.\n",
      "  # Default: True\n",
      "  checkpoint_at_epoch: true\n",
      "  # Directory where periodic training checkpoints are stored.\n",
      "  # Default: /root/.local/share/instructlab/checkpoints\n",
      "  ckpt_output_dir: /root/.local/share/instructlab/checkpoints\n",
      "  # Directory where the processed training data is stored (post\n",
      "  # filtering/tokenization/masking).\n",
      "  # Default: /root/.local/share/instructlab/internal\n",
      "  data_output_dir: /root/.local/share/instructlab/internal\n",
      "  # For the training library (primary training method), this specifies the path to\n",
      "  # the dataset file. For legacy training (MacOS/Linux), this specifies the path to\n",
      "  # the directory.\n",
      "  # Default: /root/.local/share/instructlab/datasets\n",
      "  data_path: /root/.local/share/instructlab/datasets\n",
      "  # Allow CPU offload for deepspeed optimizer.\n",
      "  # Default: False\n",
      "  deepspeed_cpu_offload_optimizer: false\n",
      "  # PyTorch device to use. Use 'cpu' for 'simple' and 'full' training on Linux. Use\n",
      "  # 'mps' for 'full' training on MacOS Metal Performance Shader. Use 'cuda' for\n",
      "  # Nvidia CUDA / AMD ROCm GPUs. Use 'hpu' for Intel Gaudi GPUs.\n",
      "  # Default: cpu\n",
      "  # Examples:\n",
      "  #   - cpu\n",
      "  #   - mps\n",
      "  #   - cuda\n",
      "  #   - hpu\n",
      "  device: cuda\n",
      "  # Whether or not we should disable the use of flash-attention during training.\n",
      "  # This is useful when using older GPUs.\n",
      "  # Default: False\n",
      "  disable_flash_attn: false\n",
      "  # Pick a distributed training backend framework for GPU accelerated full fine-\n",
      "  # tuning.\n",
      "  # Default: fsdp\n",
      "  distributed_backend: fsdp\n",
      "  # The number of samples in a batch that the model should see before its parameters\n",
      "  # are updated.\n",
      "  # Default: 64\n",
      "  effective_batch_size: 64\n",
      "  # Allow CPU offload for FSDP optimizer.\n",
      "  # Default: False\n",
      "  fsdp_cpu_offload_optimizer: false\n",
      "  # Boolean to indicate if the model being trained is a padding-free transformer\n",
      "  # model such as Granite.\n",
      "  # Default: False\n",
      "  is_padding_free: false\n",
      "  # The data type for quantization in LoRA training. Valid options are 'None' and\n",
      "  # 'nf4'.\n",
      "  # Default: nf4\n",
      "  # Examples:\n",
      "  #   - nf4\n",
      "  lora_quantize_dtype: nf4\n",
      "  # Rank of low rank matrices to be used during training.\n",
      "  # Default: 0\n",
      "  lora_rank: 0\n",
      "  # Maximum tokens per gpu for each batch that will be handled in a single step. If\n",
      "  # running into out-of-memory errors, this value can be lowered but not below the\n",
      "  # `max_seq_len`.\n",
      "  # Default: 5000\n",
      "  max_batch_len: 5000\n",
      "  # Maximum sequence length to be included in the training set. Samples exceeding\n",
      "  # this length will be dropped.\n",
      "  # Default: 4096\n",
      "  max_seq_len: 4096\n",
      "  # Directory where the model to be trained is stored.\n",
      "  # Default: instructlab/granite-7b-lab\n",
      "  model_path: instructlab/granite-7b-lab\n",
      "  # Number of GPUs to use for training. This value is not supported in legacy\n",
      "  # training or MacOS.\n",
      "  # Default: 1\n",
      "  nproc_per_node: 1\n",
      "  # Number of epochs to run training for.\n",
      "  # Default: 10\n",
      "  num_epochs: 10\n",
      "  # Base directory for organization of end-to-end intermediate outputs.\n",
      "  # Default: /root/.local/share/instructlab/phased\n",
      "  phased_base_dir: /root/.local/share/instructlab/phased\n",
      "  # Judge model path for phased MT-Bench evaluation.\n",
      "  # Default: /root/.cache/instructlab/models/prometheus-eval/prometheus-8x7b-v2.0\n",
      "  phased_mt_bench_judge: /root/.cache/instructlab/models/prometheus-eval/prometheus-8x7b-v2.0\n",
      "  # Phased phase1 effective batch size.\n",
      "  # Default: 128\n",
      "  phased_phase1_effective_batch_size: 128\n",
      "  # Learning rate for phase1 knowledge training.\n",
      "  # Default: 2e-05\n",
      "  phased_phase1_learning_rate: 2e-05\n",
      "  # Number of epochs to run training for during phase1 (experimentally optimal\n",
      "  # number is 7).\n",
      "  # Default: 7\n",
      "  phased_phase1_num_epochs: 7\n",
      "  # Number of samples the model should see before saving a checkpoint during phase1.\n",
      "  # Disabled when set to 0.\n",
      "  # Default: 0\n",
      "  phased_phase1_samples_per_save: 0\n",
      "  # Phased phase2 effective batch size.\n",
      "  # Default: 3840\n",
      "  phased_phase2_effective_batch_size: 3840\n",
      "  # Learning rate for phase2 skills training.\n",
      "  # Default: 6e-06\n",
      "  phased_phase2_learning_rate: 6e-06\n",
      "  # Number of epochs to run training for during phase2.\n",
      "  # Default: 10\n",
      "  phased_phase2_num_epochs: 10\n",
      "  # Number of samples the model should see before saving a checkpoint during phase2.\n",
      "  # Disabled when set to 0.\n",
      "  # Default: 0\n",
      "  phased_phase2_samples_per_save: 0\n",
      "  # Training pipeline to use. Simple is for systems with limited resources, full is\n",
      "  # for more capable consumer systems (64 GB of RAM), and accelerated is for systems\n",
      "  # with a dedicated GPU.\n",
      "  # Default: full\n",
      "  # Examples:\n",
      "  #   - simple\n",
      "  #   - full\n",
      "  #   - accelerated\n",
      "  pipeline: full\n",
      "  # Number of samples the model should see before saving a checkpoint.\n",
      "  # Default: 250000\n",
      "  save_samples: 250000\n",
      "  # Optional path to a yaml file that tracks the progress of multiphase training.\n",
      "  # Default: None\n",
      "  training_journal:\n",
      "# Configuration file structure version.\n",
      "# Default: 1.0.0\n",
      "version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "##Use ruamel.yaml to load the yaml file to preserve comments\n",
    "yaml = ruamel.yaml.YAML()\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.load(file)\n",
    "\n",
    "##Upate to use the same models and just change the directory\n",
    "teacher_model_path = \"models/mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "base_model_path = \"models/meta-llama/Llama-3.2-3B-Instruct\"\n",
    "##judge_model_path = \"models/prometheus-eval/prometheus-8x7b-v2.0\"\n",
    "\n",
    "##config['evaluate']['mt_bench']['judge_model'] = judge_model_path\n",
    "##config['evaluate']['mt_bench_branch']['judge_model'] = judge_model_path\n",
    "config['generate']['model'] = teacher_model_path\n",
    "config['generate']['teacher']['model_path']= teacher_model_path\n",
    "##config['train']['phased_mt_bench_judge']=judge_model_path\n",
    "\n",
    "#Update GPU information\n",
    "config['evaluate']['gpus']=gpus\n",
    "config['generate']['teacher']['vllm']['gpus']=gpus\n",
    "config['serve']['vllm']['gpus']=gpus\n",
    "config['train']['nproc_per_node']=gpus\n",
    "config['metadata']['gpu_count']=gpus\n",
    "if gpus==1:\n",
    "  config['train']['device']=\"cuda\"\n",
    "  if gpu_type[:6]==\"NVIDIA\":\n",
    "    config['metadata']['gpu_manufacturer']=\"Nvidia\"\n",
    "    config['metadata']['gpu_family']=gpu_type[7:]\n",
    "\n",
    "## Save the updated config.yaml file\n",
    "yaml.default_flow_style=False\n",
    "with open('config.yaml', 'w') as file:\n",
    "    yaml.dump(config, file)\n",
    "\n",
    "##copy the config file to the .config/instructlab/ where it is used by InstructLab\n",
    "!cp config.yaml {base_dir}.config/instructlab/\n",
    "\n",
    "print(\"Updated config.yaml successfully.\\n\")\n",
    "!cat config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dZgO-FZ6rYH"
   },
   "source": [
    "<a id=\"IL1_down\"></a>\n",
    "## Step 1.7 Download Models\n",
    "The models that will be used in the InstructLab processing are downloaded in this step. Additional steps can be added if other models are used in processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fI-25zVd6rYH"
   },
   "source": [
    "The merlinite model will be used as the teacher model for the simple pipeline in the **Training with InstructLab** section.\n",
    "\n",
    "The mistral-7b-instruct-v0.2.Q4_K_M model will be used as the teacher model for the full pipeline in that section.\n",
    "\n",
    "The granite07b-lab.gguf model is a quantized version of the granite-7b-lab model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 149001,
     "status": "ok",
     "timestamp": 1744451488819,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "rCa2va8r6rYH",
    "outputId": "b58e27c7-f42c-44e7-c5fe-21d771524877"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-04-12 09:49:00,518 instructlab.model.download:77: Downloading model from Hugging Face:\n",
      "    Model: meta-llama/Llama-3.2-3B-Instruct@main\n",
      "    Destination: models\n",
      "Downloading '.gitattributes' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
      "INFO 2025-04-12 09:49:00,835 huggingface_hub.file_download:1652: Downloading '.gitattributes' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
      ".gitattributes: 100% 1.52k/1.52k [00:00<00:00, 11.1MB/s]\n",
      "Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/.gitattributes\n",
      "INFO 2025-04-12 09:49:00,897 huggingface_hub.file_download:1684: Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/.gitattributes\n",
      "Downloading 'LICENSE.txt' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/cyBuwAu93UXke23CJCWORBYR70A=.085b47c1575cb889b7024030e60b78f54f0b8c9e.incomplete'\n",
      "INFO 2025-04-12 09:49:00,950 huggingface_hub.file_download:1652: Downloading 'LICENSE.txt' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/cyBuwAu93UXke23CJCWORBYR70A=.085b47c1575cb889b7024030e60b78f54f0b8c9e.incomplete'\n",
      "LICENSE.txt: 100% 7.71k/7.71k [00:00<00:00, 39.3MB/s]\n",
      "Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/LICENSE.txt\n",
      "INFO 2025-04-12 09:49:01,003 huggingface_hub.file_download:1684: Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/LICENSE.txt\n",
      "Downloading 'README.md' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.55ce1d9728044d12f0856a0fcd715ee2ec2e449b.incomplete'\n",
      "INFO 2025-04-12 09:49:01,071 huggingface_hub.file_download:1652: Downloading 'README.md' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.55ce1d9728044d12f0856a0fcd715ee2ec2e449b.incomplete'\n",
      "README.md: 100% 41.7k/41.7k [00:00<00:00, 12.4MB/s]\n",
      "Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/README.md\n",
      "INFO 2025-04-12 09:49:01,129 huggingface_hub.file_download:1684: Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/README.md\n",
      "Downloading 'USE_POLICY.md' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/UcPfAI5B08awK9_TiALuc0iOThI=.ac3c5f21b9779e3da0677d6d3c587778fe3a331e.incomplete'\n",
      "INFO 2025-04-12 09:49:01,179 huggingface_hub.file_download:1652: Downloading 'USE_POLICY.md' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/UcPfAI5B08awK9_TiALuc0iOThI=.ac3c5f21b9779e3da0677d6d3c587778fe3a331e.incomplete'\n",
      "USE_POLICY.md: 100% 6.02k/6.02k [00:00<00:00, 39.0MB/s]\n",
      "Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/USE_POLICY.md\n",
      "INFO 2025-04-12 09:49:01,233 huggingface_hub.file_download:1684: Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/USE_POLICY.md\n",
      "Downloading 'config.json' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.a5a40fa6da567ab026a5a2bf37125a90182be07d.incomplete'\n",
      "INFO 2025-04-12 09:49:01,276 huggingface_hub.file_download:1652: Downloading 'config.json' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.a5a40fa6da567ab026a5a2bf37125a90182be07d.incomplete'\n",
      "config.json: 100% 878/878 [00:00<00:00, 6.92MB/s]\n",
      "Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/config.json\n",
      "INFO 2025-04-12 09:49:01,317 huggingface_hub.file_download:1684: Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/config.json\n",
      "Downloading 'generation_config.json' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/3EVKVggOldJcKSsGjSdoUCN1AyQ=.75ae08310d6d23df373ee2644b497192b3cce6d8.incomplete'\n",
      "INFO 2025-04-12 09:49:01,362 huggingface_hub.file_download:1652: Downloading 'generation_config.json' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/3EVKVggOldJcKSsGjSdoUCN1AyQ=.75ae08310d6d23df373ee2644b497192b3cce6d8.incomplete'\n",
      "generation_config.json: 100% 189/189 [00:00<00:00, 1.73MB/s]\n",
      "Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/generation_config.json\n",
      "INFO 2025-04-12 09:49:01,406 huggingface_hub.file_download:1684: Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/generation_config.json\n",
      "Downloading 'model-00001-of-00002.safetensors' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/aoe4E07IMh7reFyUkVoVk040mQk=.13cbd6d16e927a0c5bad54102514e6e18b4a47b3a6eb911e39d678d328d19f55.incomplete'\n",
      "INFO 2025-04-12 09:49:01,456 huggingface_hub.file_download:1652: Downloading 'model-00001-of-00002.safetensors' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/aoe4E07IMh7reFyUkVoVk040mQk=.13cbd6d16e927a0c5bad54102514e6e18b4a47b3a6eb911e39d678d328d19f55.incomplete'\n",
      "model-00001-of-00002.safetensors: 100% 4.97G/4.97G [00:10<00:00, 454MB/s]\n",
      "Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/model-00001-of-00002.safetensors\n",
      "INFO 2025-04-12 09:49:12,477 huggingface_hub.file_download:1684: Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/model-00001-of-00002.safetensors\n",
      "Downloading 'model-00002-of-00002.safetensors' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/Dr_lZJDwE1cnGAQMwA77jJEQIk8=.7b770216613ac5c34d7c54bdff1fa616bc4e338a9d0b20af6303e48c295ee23c.incomplete'\n",
      "INFO 2025-04-12 09:49:12,524 huggingface_hub.file_download:1652: Downloading 'model-00002-of-00002.safetensors' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/Dr_lZJDwE1cnGAQMwA77jJEQIk8=.7b770216613ac5c34d7c54bdff1fa616bc4e338a9d0b20af6303e48c295ee23c.incomplete'\n",
      "model-00002-of-00002.safetensors: 100% 1.46G/1.46G [00:04<00:00, 317MB/s]\n",
      "Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/model-00002-of-00002.safetensors\n",
      "INFO 2025-04-12 09:49:17,170 huggingface_hub.file_download:1684: Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/model-00002-of-00002.safetensors\n",
      "Downloading 'model.safetensors.index.json' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/yVzAsSxRSINSz-tQbpx-TLpfkLU=.d3a1f0f5f401eeadca0c7a6786bd9e877fd42e58.incomplete'\n",
      "INFO 2025-04-12 09:49:17,211 huggingface_hub.file_download:1652: Downloading 'model.safetensors.index.json' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/yVzAsSxRSINSz-tQbpx-TLpfkLU=.d3a1f0f5f401eeadca0c7a6786bd9e877fd42e58.incomplete'\n",
      "model.safetensors.index.json: 100% 20.9k/20.9k [00:00<00:00, 11.2MB/s]\n",
      "Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/model.safetensors.index.json\n",
      "INFO 2025-04-12 09:49:17,307 huggingface_hub.file_download:1684: Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/model.safetensors.index.json\n",
      "Downloading 'original/consolidated.00.pth' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/_dLw4ih-O1I9AkO57vYC89Z48Os=.dd817d4653a88601bac65e39ae7446ead3988264afafbce48559d5b5359044d6.incomplete'\n",
      "INFO 2025-04-12 09:49:19,369 huggingface_hub.file_download:1652: Downloading 'original/consolidated.00.pth' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/_dLw4ih-O1I9AkO57vYC89Z48Os=.dd817d4653a88601bac65e39ae7446ead3988264afafbce48559d5b5359044d6.incomplete'\n",
      "consolidated.00.pth: 100% 6.43G/6.43G [00:17<00:00, 371MB/s]\n",
      "Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/original/consolidated.00.pth\n",
      "INFO 2025-04-12 09:49:36,744 huggingface_hub.file_download:1684: Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/original/consolidated.00.pth\n",
      "Downloading 'original/orig_params.json' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/1ZYEDeRW88YmfX1zfAZ5sKtcQfw=.3bf279893fb57e49e10bf879dd0adbcff5dab286.incomplete'\n",
      "INFO 2025-04-12 09:49:36,814 huggingface_hub.file_download:1652: Downloading 'original/orig_params.json' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/1ZYEDeRW88YmfX1zfAZ5sKtcQfw=.3bf279893fb57e49e10bf879dd0adbcff5dab286.incomplete'\n",
      "orig_params.json: 100% 220/220 [00:00<00:00, 1.77MB/s]\n",
      "Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/original/orig_params.json\n",
      "INFO 2025-04-12 09:49:36,868 huggingface_hub.file_download:1684: Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/original/orig_params.json\n",
      "Downloading 'original/params.json' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/jqHB00sRqBVJXCrFOHz5gDS2Bg8=.3bf279893fb57e49e10bf879dd0adbcff5dab286.incomplete'\n",
      "INFO 2025-04-12 09:49:36,917 huggingface_hub.file_download:1652: Downloading 'original/params.json' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/jqHB00sRqBVJXCrFOHz5gDS2Bg8=.3bf279893fb57e49e10bf879dd0adbcff5dab286.incomplete'\n",
      "params.json: 100% 220/220 [00:00<00:00, 2.08MB/s]\n",
      "Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/original/params.json\n",
      "INFO 2025-04-12 09:49:36,974 huggingface_hub.file_download:1684: Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/original/params.json\n",
      "Downloading 'original/tokenizer.model' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/7iVfz3cUOMr-hyjiqqRDHEwVBAM=.82e9d31979e92ab929cd544440f129d9ecd797b69e327f80f17e1c50d5551b55.incomplete'\n",
      "INFO 2025-04-12 09:49:37,185 huggingface_hub.file_download:1652: Downloading 'original/tokenizer.model' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/original/7iVfz3cUOMr-hyjiqqRDHEwVBAM=.82e9d31979e92ab929cd544440f129d9ecd797b69e327f80f17e1c50d5551b55.incomplete'\n",
      "tokenizer.model: 100% 2.18M/2.18M [00:00<00:00, 28.0MB/s]\n",
      "Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/original/tokenizer.model\n",
      "INFO 2025-04-12 09:49:37,346 huggingface_hub.file_download:1684: Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/original/tokenizer.model\n",
      "Downloading 'special_tokens_map.json' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/ahkChHUJFxEmOdq5GDFEmerRzCY=.02ee80b6196926a5ad790a004d9efd6ab1ba6542.incomplete'\n",
      "INFO 2025-04-12 09:49:37,394 huggingface_hub.file_download:1652: Downloading 'special_tokens_map.json' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/ahkChHUJFxEmOdq5GDFEmerRzCY=.02ee80b6196926a5ad790a004d9efd6ab1ba6542.incomplete'\n",
      "special_tokens_map.json: 100% 296/296 [00:00<00:00, 2.82MB/s]\n",
      "Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/special_tokens_map.json\n",
      "INFO 2025-04-12 09:49:37,442 huggingface_hub.file_download:1684: Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/special_tokens_map.json\n",
      "Downloading 'tokenizer.json' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.5cc5f00a5b203e90a27a3bd60d1ec393b07971e8.incomplete'\n",
      "INFO 2025-04-12 09:49:37,489 huggingface_hub.file_download:1652: Downloading 'tokenizer.json' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.5cc5f00a5b203e90a27a3bd60d1ec393b07971e8.incomplete'\n",
      "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 18.0MB/s]\n",
      "Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/tokenizer.json\n",
      "INFO 2025-04-12 09:49:38,065 huggingface_hub.file_download:1684: Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/tokenizer.json\n",
      "Downloading 'tokenizer_config.json' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.4ff488a165e900e5129cda7c20ab32d568d2a475.incomplete'\n",
      "INFO 2025-04-12 09:49:38,110 huggingface_hub.file_download:1652: Downloading 'tokenizer_config.json' to 'models/meta-llama/Llama-3.2-3B-Instruct/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.4ff488a165e900e5129cda7c20ab32d568d2a475.incomplete'\n",
      "tokenizer_config.json: 100% 54.5k/54.5k [00:00<00:00, 14.0MB/s]\n",
      "Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/tokenizer_config.json\n",
      "INFO 2025-04-12 09:49:38,270 huggingface_hub.file_download:1684: Download complete. Moving file to models/meta-llama/Llama-3.2-3B-Instruct/tokenizer_config.json\n",
      "INFO 2025-04-12 09:49:38,271 instructlab.model.download:288: \n",
      "ᕦ(òᴗóˇ)ᕤ meta-llama/Llama-3.2-3B-Instruct model download completed successfully! ᕦ(òᴗóˇ)ᕤ\n",
      "\n",
      "INFO 2025-04-12 09:49:38,271 instructlab.model.download:302: Available models (`ilab model list`):\n",
      "+----------------------------------+---------------------+--------+---------------------------------+\n",
      "| Model Name                       | Last Modified       | Size   | Absolute path                   |\n",
      "+----------------------------------+---------------------+--------+---------------------------------+\n",
      "| meta-llama/Llama-3.2-3B-Instruct | 2025-04-12 09:49:38 | 6.0 GB | /content/ilab/models/meta-llama |\n",
      "+----------------------------------+---------------------+--------+---------------------------------+\n",
      "INFO 2025-04-12 09:49:41,494 instructlab.model.download:77: Downloading model from Hugging Face:\n",
      "    Model: mistralai/Mistral-7B-Instruct-v0.2@main\n",
      "    Destination: models\n",
      "Downloading '.gitattributes' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
      "INFO 2025-04-12 09:49:41,774 huggingface_hub.file_download:1652: Downloading '.gitattributes' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
      ".gitattributes: 100% 1.52k/1.52k [00:00<00:00, 11.7MB/s]\n",
      "Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/.gitattributes\n",
      "INFO 2025-04-12 09:49:41,842 huggingface_hub.file_download:1684: Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/.gitattributes\n",
      "Downloading 'README.md' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.ce76664f935ee60910518e93269eb44c7cd56605.incomplete'\n",
      "INFO 2025-04-12 09:49:41,909 huggingface_hub.file_download:1652: Downloading 'README.md' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.ce76664f935ee60910518e93269eb44c7cd56605.incomplete'\n",
      "README.md: 100% 5.52k/5.52k [00:00<00:00, 30.8MB/s]\n",
      "Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/README.md\n",
      "INFO 2025-04-12 09:49:41,972 huggingface_hub.file_download:1684: Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/README.md\n",
      "Downloading 'config.json' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.c0519dc5f5cc99c2238a453da18994599c898b66.incomplete'\n",
      "INFO 2025-04-12 09:49:42,113 huggingface_hub.file_download:1652: Downloading 'config.json' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.c0519dc5f5cc99c2238a453da18994599c898b66.incomplete'\n",
      "config.json: 100% 596/596 [00:00<00:00, 4.32MB/s]\n",
      "Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/config.json\n",
      "INFO 2025-04-12 09:49:42,176 huggingface_hub.file_download:1684: Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/config.json\n",
      "Downloading 'generation_config.json' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/3EVKVggOldJcKSsGjSdoUCN1AyQ=.cb0c9b6c64cf786052efdd1a4ae597337b2f2708.incomplete'\n",
      "INFO 2025-04-12 09:49:42,241 huggingface_hub.file_download:1652: Downloading 'generation_config.json' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/3EVKVggOldJcKSsGjSdoUCN1AyQ=.cb0c9b6c64cf786052efdd1a4ae597337b2f2708.incomplete'\n",
      "generation_config.json: 100% 111/111 [00:00<00:00, 735kB/s]\n",
      "Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/generation_config.json\n",
      "INFO 2025-04-12 09:49:42,306 huggingface_hub.file_download:1684: Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/generation_config.json\n",
      "Downloading 'model-00001-of-00003.safetensors' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/Y6g195DXbsCiN_Ka1NwH6jyLkvc=.63654d601820b88b1fa8b4a98df5714f700fbc5b3df2cc4ecbabdced35096d31.incomplete'\n",
      "INFO 2025-04-12 09:49:42,374 huggingface_hub.file_download:1652: Downloading 'model-00001-of-00003.safetensors' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/Y6g195DXbsCiN_Ka1NwH6jyLkvc=.63654d601820b88b1fa8b4a98df5714f700fbc5b3df2cc4ecbabdced35096d31.incomplete'\n",
      "model-00001-of-00003.safetensors: 100% 4.94G/4.94G [00:12<00:00, 402MB/s]\n",
      "Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/model-00001-of-00003.safetensors\n",
      "INFO 2025-04-12 09:49:54,704 huggingface_hub.file_download:1684: Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/model-00001-of-00003.safetensors\n",
      "Downloading 'model-00002-of-00003.safetensors' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/D5-QTaCbl9VUFv9Wr0C28eFewRc=.a42716540ecb2385d371f2109835921ff535406cac8fe8ff28f2f0b5fc7895bd.incomplete'\n",
      "INFO 2025-04-12 09:49:54,769 huggingface_hub.file_download:1652: Downloading 'model-00002-of-00003.safetensors' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/D5-QTaCbl9VUFv9Wr0C28eFewRc=.a42716540ecb2385d371f2109835921ff535406cac8fe8ff28f2f0b5fc7895bd.incomplete'\n",
      "model-00002-of-00003.safetensors: 100% 5.00G/5.00G [00:14<00:00, 339MB/s]\n",
      "Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/model-00002-of-00003.safetensors\n",
      "INFO 2025-04-12 09:50:09,545 huggingface_hub.file_download:1684: Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/model-00002-of-00003.safetensors\n",
      "Downloading 'model-00003-of-00003.safetensors' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/NmVQgThqhsRkG2FlXLh-M51MilA=.5f86e15cb3ed9078e30ae6e72445e109d0e337d9cde59b9aeea4ce8e44e54a5d.incomplete'\n",
      "INFO 2025-04-12 09:50:09,612 huggingface_hub.file_download:1652: Downloading 'model-00003-of-00003.safetensors' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/NmVQgThqhsRkG2FlXLh-M51MilA=.5f86e15cb3ed9078e30ae6e72445e109d0e337d9cde59b9aeea4ce8e44e54a5d.incomplete'\n",
      "model-00003-of-00003.safetensors: 100% 4.54G/4.54G [00:19<00:00, 229MB/s]\n",
      "Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/model-00003-of-00003.safetensors\n",
      "INFO 2025-04-12 09:50:29,550 huggingface_hub.file_download:1684: Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/model-00003-of-00003.safetensors\n",
      "Downloading 'model.safetensors.index.json' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/yVzAsSxRSINSz-tQbpx-TLpfkLU=.361fa9d25a7f791e18ab531b3468ff8f2010642e.incomplete'\n",
      "INFO 2025-04-12 09:50:29,624 huggingface_hub.file_download:1652: Downloading 'model.safetensors.index.json' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/yVzAsSxRSINSz-tQbpx-TLpfkLU=.361fa9d25a7f791e18ab531b3468ff8f2010642e.incomplete'\n",
      "model.safetensors.index.json: 100% 25.1k/25.1k [00:00<00:00, 20.7MB/s]\n",
      "Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/model.safetensors.index.json\n",
      "INFO 2025-04-12 09:50:29,692 huggingface_hub.file_download:1684: Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/model.safetensors.index.json\n",
      "Downloading 'pytorch_model-00001-of-00003.bin' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/tYEjMy-9HxF6eZaoFpYvo3axcg4=.d8836f675fe1c4c43f3ff4e93f4cc0e97ef7a13e8c240fb39ad02d37ff303ef5.incomplete'\n",
      "INFO 2025-04-12 09:50:29,762 huggingface_hub.file_download:1652: Downloading 'pytorch_model-00001-of-00003.bin' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/tYEjMy-9HxF6eZaoFpYvo3axcg4=.d8836f675fe1c4c43f3ff4e93f4cc0e97ef7a13e8c240fb39ad02d37ff303ef5.incomplete'\n",
      "pytorch_model-00001-of-00003.bin: 100% 4.94G/4.94G [00:21<00:00, 232MB/s]\n",
      "Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/pytorch_model-00001-of-00003.bin\n",
      "INFO 2025-04-12 09:50:51,148 huggingface_hub.file_download:1684: Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/pytorch_model-00001-of-00003.bin\n",
      "Downloading 'pytorch_model-00002-of-00003.bin' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/vCI62Q5XTwTZAhZquX3Y7Xigu0Y=.58a7ddffb463397de5dbe1f1e2ec1ccf6aae2b549565f83f3ded124e0b4c5069.incomplete'\n",
      "INFO 2025-04-12 09:50:51,218 huggingface_hub.file_download:1652: Downloading 'pytorch_model-00002-of-00003.bin' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/vCI62Q5XTwTZAhZquX3Y7Xigu0Y=.58a7ddffb463397de5dbe1f1e2ec1ccf6aae2b549565f83f3ded124e0b4c5069.incomplete'\n",
      "pytorch_model-00002-of-00003.bin: 100% 5.00G/5.00G [00:15<00:00, 318MB/s]\n",
      "Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/pytorch_model-00002-of-00003.bin\n",
      "INFO 2025-04-12 09:51:07,013 huggingface_hub.file_download:1684: Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/pytorch_model-00002-of-00003.bin\n",
      "Downloading 'pytorch_model-00003-of-00003.bin' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/pOHR8BUAxtKStP9HRohOVwMPrHM=.75824d68dcf82d02b731b2bdfd3a9711acb7c58b8d566f4c0d3e9efac52f9a21.incomplete'\n",
      "INFO 2025-04-12 09:51:07,076 huggingface_hub.file_download:1652: Downloading 'pytorch_model-00003-of-00003.bin' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/pOHR8BUAxtKStP9HRohOVwMPrHM=.75824d68dcf82d02b731b2bdfd3a9711acb7c58b8d566f4c0d3e9efac52f9a21.incomplete'\n",
      "pytorch_model-00003-of-00003.bin: 100% 5.06G/5.06G [00:17<00:00, 295MB/s]\n",
      "Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/pytorch_model-00003-of-00003.bin\n",
      "INFO 2025-04-12 09:51:24,329 huggingface_hub.file_download:1684: Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/pytorch_model-00003-of-00003.bin\n",
      "Downloading 'pytorch_model.bin.index.json' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/8TePU6wZ6PO52hlgkfCnTYnlMSI=.76fccc201c63903bcc555d59944b099e5cc7d336.incomplete'\n",
      "INFO 2025-04-12 09:51:24,398 huggingface_hub.file_download:1652: Downloading 'pytorch_model.bin.index.json' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/8TePU6wZ6PO52hlgkfCnTYnlMSI=.76fccc201c63903bcc555d59944b099e5cc7d336.incomplete'\n",
      "pytorch_model.bin.index.json: 100% 23.9k/23.9k [00:00<00:00, 12.4MB/s]\n",
      "Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/pytorch_model.bin.index.json\n",
      "INFO 2025-04-12 09:51:24,468 huggingface_hub.file_download:1684: Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/pytorch_model.bin.index.json\n",
      "Downloading 'special_tokens_map.json' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/ahkChHUJFxEmOdq5GDFEmerRzCY=.451134b2ddc2e78555d1e857518c54b4bdc2e87d.incomplete'\n",
      "INFO 2025-04-12 09:51:24,534 huggingface_hub.file_download:1652: Downloading 'special_tokens_map.json' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/ahkChHUJFxEmOdq5GDFEmerRzCY=.451134b2ddc2e78555d1e857518c54b4bdc2e87d.incomplete'\n",
      "special_tokens_map.json: 100% 414/414 [00:00<00:00, 3.63MB/s]\n",
      "Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/special_tokens_map.json\n",
      "INFO 2025-04-12 09:51:24,671 huggingface_hub.file_download:1684: Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/special_tokens_map.json\n",
      "Downloading 'tokenizer.json' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.1e9f59c515a01088e1f7f72b68ab99f92d15fb96.incomplete'\n",
      "INFO 2025-04-12 09:51:24,737 huggingface_hub.file_download:1652: Downloading 'tokenizer.json' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.1e9f59c515a01088e1f7f72b68ab99f92d15fb96.incomplete'\n",
      "tokenizer.json: 100% 1.80M/1.80M [00:00<00:00, 22.2MB/s]\n",
      "Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/tokenizer.json\n",
      "INFO 2025-04-12 09:51:24,886 huggingface_hub.file_download:1684: Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/tokenizer.json\n",
      "Downloading 'tokenizer.model' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/7iVfz3cUOMr-hyjiqqRDHEwVBAM=.dadfd56d766715c61d2ef780a525ab43b8e6da4de6865bda3d95fdef5e134055.incomplete'\n",
      "INFO 2025-04-12 09:51:24,955 huggingface_hub.file_download:1652: Downloading 'tokenizer.model' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/7iVfz3cUOMr-hyjiqqRDHEwVBAM=.dadfd56d766715c61d2ef780a525ab43b8e6da4de6865bda3d95fdef5e134055.incomplete'\n",
      "tokenizer.model: 100% 493k/493k [00:00<00:00, 9.17MB/s]\n",
      "Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/tokenizer.model\n",
      "INFO 2025-04-12 09:51:25,075 huggingface_hub.file_download:1684: Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/tokenizer.model\n",
      "Downloading 'tokenizer_config.json' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.f63c0374bad221568938f908e397a38d9f0dfac0.incomplete'\n",
      "INFO 2025-04-12 09:51:25,143 huggingface_hub.file_download:1652: Downloading 'tokenizer_config.json' to 'models/mistralai/Mistral-7B-Instruct-v0.2/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.f63c0374bad221568938f908e397a38d9f0dfac0.incomplete'\n",
      "tokenizer_config.json: 100% 2.10k/2.10k [00:00<00:00, 18.0MB/s]\n",
      "Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/tokenizer_config.json\n",
      "INFO 2025-04-12 09:51:25,212 huggingface_hub.file_download:1684: Download complete. Moving file to models/mistralai/Mistral-7B-Instruct-v0.2/tokenizer_config.json\n",
      "INFO 2025-04-12 09:51:25,213 instructlab.model.download:288: \n",
      "ᕦ(òᴗóˇ)ᕤ mistralai/Mistral-7B-Instruct-v0.2 model download completed successfully! ᕦ(òᴗóˇ)ᕤ\n",
      "\n",
      "INFO 2025-04-12 09:51:25,213 instructlab.model.download:302: Available models (`ilab model list`):\n",
      "+------------------------------------+---------------------+---------+---------------------------------+\n",
      "| Model Name                         | Last Modified       | Size    | Absolute path                   |\n",
      "+------------------------------------+---------------------+---------+---------------------------------+\n",
      "| mistralai/Mistral-7B-Instruct-v0.2 | 2025-04-12 09:51:25 | 27.5 GB | /content/ilab/models/mistralai  |\n",
      "| meta-llama/Llama-3.2-3B-Instruct   | 2025-04-12 09:49:38 | 6.0 GB  | /content/ilab/models/meta-llama |\n",
      "+------------------------------------+---------------------+---------+---------------------------------+\n"
     ]
    }
   ],
   "source": [
    "models_dir=\"models\"\n",
    "\n",
    "!ilab model download --repository meta-llama/Llama-3.2-3B-Instruct --hf-token {hf_token} --model-dir {models_dir}\n",
    "!ilab model download --repository mistralai/Mistral-7B-Instruct-v0.2 --hf-token {hf_token} --model-dir {models_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDUmbbHLvxgg",
    "tags": []
   },
   "source": [
    "<a id=\"IL2_0\"></a>\n",
    "# Section 2. Generating Synthetic Data\n",
    "\n",
    "\n",
    "This section demonstrates training with InstructLab. This section is part of a sequential notebook. Before running this section of the notebook, please ensure that you have run the Configuring InstructLab section of this notebook.\n",
    "\n",
    "In this section, we will demonstrate:\n",
    "- Creating a question and answer data file\n",
    "- Generating synthetic data for training\n",
    "- Training the LLM with the generated data\n",
    "\n",
    "The steps in this section are as follows:\n",
    "* Step 2.1 Specify the Data for this Run\n",
    "* Step 2.2 Create the Taxonomy Data Repository\n",
    "* Step 2.3 Generate Synthetic Data\n",
    "* Step 2.4 Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnOLAXRxvxgh",
    "tags": []
   },
   "source": [
    "<a id=\"IL2_data\"></a>\n",
    "## Step 2.1 Specify the Data for this Run\n",
    "\n",
    "We've provided question-and-answer files for these datasets: \"2024 Oscar Awards Ceremony\", \"Quantum Roadmap and Patterns\" and \"Artificial Intelligence Agents\". Feel free to choose one of these datasets, or select your own custom dataset in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltRTPBTVvxgh",
    "tags": []
   },
   "source": [
    "### Optionally, Create your own data set for InstructLab\n",
    "\n",
    "You can optionally provide your own InstructLab QNA file for processing in this step.\n",
    "\n",
    "Follow these steps to add your own dataset:\n",
    "1. Create your own qna.yaml file following the directions on the InstructLab taxonomy [readme](https://github.com/instructlab/taxonomy).\n",
    "1. Create a questions.txt file with related sample questions to use on inferencing.\n",
    "1. Add your qna.yaml and sample questions.txt files to the /content/ilab/data/your_content_1 folder or the /content/ilab/data/your_content_2 folder by dragging and dropping them in the desired folder.\n",
    "1. Double click on the /content/ilab/config.json file to edit and specify the qna_location where your data resides within the Dewey Decimal classification system. Close and save the config.json file.\n",
    "1. You can now specify to run with your own data by selecting **Your Content 1** or **Your Content 2** in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102,
     "referenced_widgets": [
      "762e524f95c24b5e9c2d27dc7dc03397",
      "48b32b0ad9dc4596b01fa811ceef5dd7",
      "43e810ff41b34cc4b0fe54ead08af316"
     ]
    },
    "executionInfo": {
     "elapsed": 73,
     "status": "ok",
     "timestamp": 1744451499041,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "DbnDz_4j5GNi",
    "outputId": "bbacc4b9-6882-4b79-943f-42ec8ecced72"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select the QNA dataset to add:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762e524f95c24b5e9c2d27dc7dc03397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Dataset:', options=('cybersecurity',), style=ToggleButtonsStyle(button_width='auto'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After choosing your dataset, please select and run the following cell\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSelect the QNA dataset to add:\")\n",
    "display(data_set)\n",
    "print(\"After choosing your dataset, please select and run the following cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 91,
     "status": "ok",
     "timestamp": 1744451501309,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "olkDUaKfvxgh",
    "outputId": "aac52cd2-128a-4d08-fd60-28eff58263a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2.2 Choose the Dataset for this Run\n",
      "Using cybersecurity data\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 2.2 Choose the Dataset for this Run\")\n",
    "if data_set.value=='cybersecurity':\n",
    "    use_case=\"cybersecurity\"\n",
    "else:\n",
    "    use_case=\"undefined\"\n",
    "\n",
    "if use_case==\"undefined\":\n",
    "    print(\"ERROR: Undefined data set: \" + data_set.value + \" data\")\n",
    "else:\n",
    "    qna_file=\"data/\" + use_case + \"/qna.yaml\"\n",
    "    qna_location=jsonData[\"use_cases\"][use_case][\"qna_location\"]\n",
    "\n",
    "    print(\"Using \" + data_set.value + \" data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQBSnoObvxgh"
   },
   "source": [
    "<a id=\"IL2_taxonomy\"></a>\n",
    "## Step 2.2 Create the Taxonomy Data Repository\n",
    "Delete the prior repository, clone the empty taxonomy repository and place the QNA file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2552,
     "status": "ok",
     "timestamp": 1744451507846,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "Z2s_UhRCvxgh",
    "outputId": "b6284386-b7eb-44db-fab7-30be8205a3a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete the prior repository and clone the empty taxonomy repository\n",
      "Cloning into 'taxonomy'...\n",
      "remote: Enumerating objects: 135, done.\u001b[K\n",
      "remote: Counting objects: 100% (135/135), done.\u001b[K\n",
      "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
      "remote: Total 135 (delta 16), reused 131 (delta 14), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (135/135), 1.75 MiB | 11.17 MiB/s, done.\n",
      "Resolving deltas: 100% (16/16), done.\n",
      "Show the QNA file\n",
      "version: 3\n",
      "domain: cybersecurity\n",
      "created_by: BHnila\n",
      "seed_examples:\n",
      "  - context: |\n",
      "      Brute-force attacks are a prevalent phenomenon that is getting harder\n",
      "      to successfully detect on a network level due to increasing volume and en-\n",
      "      cryption of network traffic and growing ubiquity of high-speed networks.\n",
      "      Although the research in this field advanced considerably, there still remain\n",
      "      classes of attacks that are undetectable.\n",
      "    questions_and_answers:\n",
      "      - question: |\n",
      "          Why is detecting SSH, Telnet, and RDP brute-force attacks more difficult with encrypted network traffic?\n",
      "        answer: |\n",
      "          Encrypted network traffic conceals the contents of login attempts, such as usernames and passwords, making\n",
      "          it impossible to inspect the payload directly. As a result, detection systems must rely solely on metadata\n",
      "          like connection frequency, failed authentications, and session duration to infer brute-force activity.\n",
      "      - question: |\n",
      "          How do high-speed networks affect the ability to detect brute-force attacks in SSH, Telnet, or RDP logs?\n",
      "        answer: |\n",
      "          High-speed networks allow attackers to perform a large volume of login attempts rapidly and possibly from\n",
      "          multiple distributed IPs. This can overwhelm traditional detection systems that rely on static thresholds,\n",
      "          making it easier for brute-force activity to blend in with normal traffic.\n",
      "      - question: |\n",
      "          Despite advancements in research, why do certain brute-force attack patterns remain undetectable?\n",
      "        answer: |\n",
      "          Some brute-force attack patterns remain undetectable because they are designed to mimic legitimate\n",
      "          user behavior. For example, attackers may space out login attempts over time or rotate through large\n",
      "          pools of IP addresses, avoiding common detection indicators like excessive failed logins in a short period.\n",
      "  - context: |\n",
      "      In recent years, network security research started focusing on flow-based attack\n",
      "      detection in addition to the well-established payload-based detection approach.\n",
      "      Instead of only looking for malicious activity in the actual packet data, network\n",
      "      flows are also considered for analysis. This is not surprising since the amount\n",
      "      of data one has to fight with is drastically reduced and the attacks visible in flow\n",
      "      data tend to complement the attacks that we strive to find in network payload.\n",
      "    questions_and_answers:\n",
      "      - question: |\n",
      "          Why has flow-based detection become an important complement to payload-based methods in identifying brute-force attacks?\n",
      "        answer: |\n",
      "          Flow-based detection is valuable because it significantly reduces data volume and can reveal behavioral\n",
      "Place QNA file in taxononmy as: /taxonomy/knowledge/information/computer_science/cybersecurity/qna.yaml\n",
      "Verify the taxonomy\n",
      "knowledge/information/computer_science/cybersecurity/qna.yaml\n",
      "\u001b[32mTaxonomy in taxonomy is valid :)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Delete the prior repository and clone the empty taxonomy repository\n",
    "print(\"Delete the prior repository and clone the empty taxonomy repository\")\n",
    "shell_command1 = f\"rm -rf taxonomy\"\n",
    "taxonomy_repo=jsonData[\"taxonomy_repo\"]\n",
    "shell_command2 = f\"git clone https://github.com/BHnila/taxonomy.git\"\n",
    "!{shell_command1}\n",
    "!{shell_command2}\n",
    "\n",
    "print(\"Verify the taxonomy\")\n",
    "!ilab taxonomy diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFKW-EIVvxgi",
    "tags": []
   },
   "source": [
    "<a id=\"IL2_generate\"></a>\n",
    "## Step 2.3. Set data generation parameters\n",
    "\n",
    "#### Select pipeline\n",
    "\n",
    "InstructLab has three primary pipelines that can be used: simple, full and acellerated:\n",
    "- The **simple pipeline** runs fast and can be used for initial model and data testing.\n",
    "- The **full pipeline** runs all of the InstrctLab steps and takes more time but produces a better tuned model.\n",
    "\n",
    "**Note:** If you are running with a new or modifed dataset, you may want to use the **Simple pipeline** for the first run to verify the configuration\n",
    "\n",
    "#### Sepect number of samples to generate\n",
    "\n",
    "Data generation takes 19 minutes for generating 15 synthetic data samples. You may wish to generate a small number on your first run to verify the QNA dataset format.\n",
    "\n",
    "To produce **sufficient synthetic data** to focus training on the new material, **about 30 synthetic questions and answer pairs need to be generated** for each question and answer pair provided. This will require a proportionally longer time to generate, but will provide better training.\n",
    "\n",
    "Before following these instructions, ensure the existing model you are adding skills or knowledge to is still running. Alternatively, ilab data generate can start a server for you if you provide a fully qualified model path via --model.\n",
    "\n",
    "To generate a synthetic dataset based on your newly added knowledge or skill set in taxonomy repository, run the following command:\n",
    "\n",
    "    ilab data generate\n",
    "\n",
    "#### **Simple Pipeline**\n",
    "\n",
    "The Simple Pipeline works solely with Merlinite 7b Lab as the teacher model. The Simple Pipeline is called without GPU acceleration as follows:\n",
    "\n",
    "    ilab data generate --pipeline simple\n",
    "\n",
    "#### **Full Pipeline**\n",
    "\n",
    "The Full Pipeline runs the full processing with a GPU. Currently, the Full Pipeline only supports the Mixtral and Mistral Instruct Family models as the teacher model.  This is due to only supporting specific model prompt templates.\n",
    "\n",
    "Using a non-default model such as Mixtral-8x7B-Instruct-v0.1) to generate data with the Full Pipeline:\n",
    "\n",
    "    ilab data generate --model ~/.cache/instructlab/models/mistralai/mixtral-8x7b-instruct-v0.1 --pipeline full --gpus 4\n",
    "\n",
    "**Note** Synthetic Data Generation can take from 2 minutes to 1+ hours to complete, depending on your computing resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133,
     "referenced_widgets": [
      "7d5516d8ecd044e9a476ace2247890fd",
      "942cca6f88024528b7f68e9f4a9639c0",
      "bf59ab4e421c4630b253ee2038135760",
      "b88988ceee1a4bb7bb51570ca29c7710",
      "29053a1b03234e2ca82e72aedd369305",
      "7daefb64b8be42ef9d41d6f4877d3201"
     ]
    },
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1744093135844,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "8rNYESyEvxgi",
    "outputId": "6e189132-014e-4730-8996-760b4bfab39a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Pipeline to use\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5516d8ecd044e9a476ace2247890fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Processing:', index=1, options=('Simple', 'Full with GPU'), style=ToggleButtonsStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88988ceee1a4bb7bb51570ca29c7710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='# of QNAs:', index=2, options=('Default (>450)', '>15', '>50', '>200', '>500', '>10…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After making your selections for data generation, please select and run the following cell\n"
     ]
    }
   ],
   "source": [
    "print(\"Select Pipeline to use\")\n",
    "display(sdg_pipe)\n",
    "display(instr)\n",
    "print(\"After making your selections for data generation, please select and run the following cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-so0dOw4vxgi",
    "tags": []
   },
   "source": [
    "### 2.4 Run data generation\n",
    "Data generation with a GPU can take 2 minutes or more to generate 15 synthetic data samples. It takes proportionately longer to generate more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1744011210809,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "mqhyw4eTqJmZ",
    "outputId": "77857606-45b5-447c-f7fc-fe72ad97e2f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##to copy config.yaml to local directory\n",
    "!cp ./config.yaml /root/.config/instructlab/config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2261944,
     "status": "ok",
     "timestamp": 1743972911858,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "wn6gODEcvxgi",
    "outputId": "a8662130-ccb6-45b1-ef40-4ed4c8bb6fb6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove old datasets\n",
      "Generating data\n",
      "Running: !ilab data generate --taxonomy-path taxonomy --pipeline full --max-num-tokens 512\n",
      "INFO 2025-04-06 20:17:31,019 instructlab.process.process:297: Started subprocess with PID 14836. Logs are being written to /root/.local/share/instructlab/logs/generation/generation-2b08293e-1324-11f0-ba2d-0242ac1c000c.log.\n",
      "INFO 2025-04-06 20:17:33,206 instructlab.model.backends.vllm:115: Trying to connect to model server at http://127.0.0.1:8000/v1\n",
      "INFO 2025-04-06 20:17:34,556 instructlab.model.backends.vllm:332: vLLM starting up on pid 14871 at http://127.0.0.1:34523/v1\n",
      "INFO 2025-04-06 20:17:34,556 instructlab.model.backends.vllm:123: Starting a temporary vLLM server at http://127.0.0.1:34523/v1\n",
      "INFO 2025-04-06 20:17:34,556 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 1/120\n",
      "INFO 2025-04-06 20:17:37,951 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 2/120\n",
      "INFO 2025-04-06 20:17:41,194 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 3/120\n",
      "INFO 2025-04-06 20:17:44,419 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 4/120\n",
      "INFO 2025-04-06 20:17:47,717 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 5/120\n",
      "INFO 2025-04-06 20:17:50,976 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 6/120\n",
      "INFO 2025-04-06 20:17:54,463 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 7/120\n",
      "INFO 2025-04-06 20:17:57,771 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 8/120\n",
      "INFO 2025-04-06 20:18:01,125 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 9/120\n",
      "INFO 2025-04-06 20:18:04,280 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 10/120\n",
      "INFO 2025-04-06 20:18:07,525 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 11/120\n",
      "INFO 2025-04-06 20:18:10,854 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 12/120\n",
      "INFO 2025-04-06 20:18:14,226 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 13/120\n",
      "INFO 2025-04-06 20:18:17,553 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 14/120\n",
      "INFO 2025-04-06 20:18:20,774 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 15/120\n",
      "INFO 2025-04-06 20:18:24,076 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 16/120\n",
      "INFO 2025-04-06 20:18:27,364 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 17/120\n",
      "INFO 2025-04-06 20:18:30,690 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 18/120\n",
      "INFO 2025-04-06 20:18:34,039 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 19/120\n",
      "INFO 2025-04-06 20:18:37,371 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 20/120\n",
      "INFO 2025-04-06 20:18:40,730 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 21/120\n",
      "INFO 2025-04-06 20:18:43,997 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 22/120\n",
      "INFO 2025-04-06 20:18:47,213 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 23/120\n",
      "INFO 2025-04-06 20:18:50,560 instructlab.model.backends.vllm:138: Waiting for the vLLM server to start at http://127.0.0.1:34523/v1, this might take a moment... Attempt: 24/120\n",
      "INFO 2025-04-06 20:18:51,878 instructlab.model.backends.vllm:145: vLLM engine successfully started at http://127.0.0.1:34523/v1\n",
      "INFO 2025-04-06 20:18:52,610 datasets:59: PyTorch version 2.5.1 available.\n",
      "INFO 2025-04-06 20:18:52,611 datasets:71: Polars version 1.21.0 available.\n",
      "INFO 2025-04-06 20:18:52,612 datasets:106: TensorFlow version 2.18.0 available.\n",
      "INFO 2025-04-06 20:18:52,613 datasets:119: JAX version 0.5.2 available.\n",
      "INFO 2025-04-06 20:18:53,459 instructlab:206: Generating synthetic data using 'full' pipeline, 'models/mistralai/Mistral-7B-Instruct-v0.2' model, 'taxonomy' taxonomy, against http://127.0.0.1:34523/v1 server\n",
      "INFO 2025-04-06 20:18:53,460 root:352: Converting taxonomy to samples\n",
      "INFO 2025-04-06 20:18:54,132 instructlab.sdg.utils.taxonomy:153: Processing files...\n",
      "INFO 2025-04-06 20:18:54,132 instructlab.sdg.utils.taxonomy:159: Pattern '*.pdf' matched 2 files.\n",
      "INFO 2025-04-06 20:18:54,132 instructlab.sdg.utils.taxonomy:163: Processing file: /root/.local/share/instructlab/datasets/2025-04-06_201731/preprocessed_2025-04-06T20_18_53/documents/knowledge_information_computer_science_cybersecurity__70ygy0b/Flow-based detection of RDP brute-force attacks.pdf\n",
      "INFO 2025-04-06 20:18:54,132 instructlab.sdg.utils.taxonomy:175: Collected filepath: /root/.local/share/instructlab/datasets/2025-04-06_201731/preprocessed_2025-04-06T20_18_53/documents/knowledge_information_computer_science_cybersecurity__70ygy0b/Flow-based detection of RDP brute-force attacks.pdf\n",
      "INFO 2025-04-06 20:18:54,132 instructlab.sdg.utils.taxonomy:163: Processing file: /root/.local/share/instructlab/datasets/2025-04-06_201731/preprocessed_2025-04-06T20_18_53/documents/knowledge_information_computer_science_cybersecurity__70ygy0b/Flow-based Brute-force Attack Detection.pdf\n",
      "INFO 2025-04-06 20:18:54,132 instructlab.sdg.utils.taxonomy:175: Collected filepath: /root/.local/share/instructlab/datasets/2025-04-06_201731/preprocessed_2025-04-06T20_18_53/documents/knowledge_information_computer_science_cybersecurity__70ygy0b/Flow-based Brute-force Attack Detection.pdf\n",
      "2025-04-06 20:18:56.973655: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-06 20:18:56.991095: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743970737.012632   14836 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743970737.019086   14836 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-06 20:18:57.041927: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO 2025-04-06 20:19:00,666 instructlab.sdg.utils.chunkers:145: Docling models not found on disk, downloading models...\n",
      "INFO 2025-04-06 20:19:00,667 docling.utils.model_downloader:40: Downloading layout model...\n",
      "INFO 2025-04-06 20:19:04,573 docling.utils.model_downloader:48: Downloading tableformer model...\n",
      "INFO 2025-04-06 20:19:06,201 docling.utils.model_downloader:56: Downloading picture classifier model...\n",
      "INFO 2025-04-06 20:19:07,225 docling.utils.model_downloader:64: Downloading code formula model...\n",
      "INFO 2025-04-06 20:19:10,979 docling.utils.model_downloader:90: Downloading easyocr models...\n",
      "WARNING 2025-04-06 20:19:14,043 instructlab.sdg.utils.chunkers:64: Tesseract not found, falling back to EasyOCR.\n",
      "INFO 2025-04-06 20:19:14,044 docling.utils.accelerator_utils:67: Accelerator device: 'cpu'\n",
      "WARNING 2025-04-06 20:19:14,049 easyocr.easyocr:251: Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n",
      "INFO 2025-04-06 20:19:15,568 easyocr.easyocr:255: Download complete\n",
      "WARNING 2025-04-06 20:19:15,568 easyocr.easyocr:176: Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n",
      "INFO 2025-04-06 20:19:16,221 easyocr.easyocr:180: Download complete.\n",
      "INFO 2025-04-06 20:19:18,899 instructlab.sdg.utils.chunkers:301: Successfully loaded tokenizer from: models/mistralai/Mistral-7B-Instruct-v0.2\n",
      "INFO 2025-04-06 20:19:19,010 docling.document_converter:269: Going to convert document batch...\n",
      "INFO 2025-04-06 20:19:19,010 docling.document_converter:304: Initializing pipeline for StandardPdfPipeline with options hash 86b94240c51f821b0082e1bedae91721\n",
      "INFO 2025-04-06 20:19:19,032 docling.models.factories.base_factory:112: Loading plugin 'docling_defaults'\n",
      "INFO 2025-04-06 20:19:19,032 docling.models.factories:16: Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "INFO 2025-04-06 20:19:19,032 docling.utils.accelerator_utils:67: Accelerator device: 'cuda:0'\n",
      "INFO 2025-04-06 20:19:22,042 docling.utils.accelerator_utils:67: Accelerator device: 'cuda:0'\n",
      "INFO 2025-04-06 20:19:22,614 docling.utils.accelerator_utils:67: Accelerator device: 'cuda:0'\n",
      "INFO 2025-04-06 20:19:23,270 docling.models.factories.base_factory:112: Loading plugin 'docling_defaults'\n",
      "INFO 2025-04-06 20:19:23,270 docling.models.factories:26: Registered picture descriptions: ['vlm', 'api']\n",
      "INFO 2025-04-06 20:19:23,270 docling.pipeline.base_pipeline:38: Processing document Flow-based detection of RDP brute-force attacks.pdf\n",
      "INFO 2025-04-06 20:19:36,913 docling.document_converter:284: Finished converting document Flow-based detection of RDP brute-force attacks.pdf in 18.01 sec.\n",
      "INFO 2025-04-06 20:19:36,931 docling.pipeline.base_pipeline:38: Processing document Flow-based Brute-force Attack Detection.pdf\n",
      "INFO 2025-04-06 20:19:52,190 docling.document_converter:284: Finished converting document Flow-based Brute-force Attack Detection.pdf in 15.28 sec.\n",
      "INFO 2025-04-06 20:19:52,202 instructlab.sdg.utils.chunkers:564: Processed 2 docs, of which 0 failed\n",
      "INFO 2025-04-06 20:19:52,205 instructlab.sdg.utils.chunkers:214: Processing parsed docling json file: /root/.local/share/instructlab/datasets/2025-04-06_201731/preprocessed_2025-04-06T20_18_53/documents/docling-artifacts/Flow-based detection of RDP brute-force attacks.json\n",
      "INFO 2025-04-06 20:19:52,304 instructlab.sdg.utils.chunkers:214: Processing parsed docling json file: /root/.local/share/instructlab/datasets/2025-04-06_201731/preprocessed_2025-04-06T20_18_53/documents/docling-artifacts/Flow-based Brute-force Attack Detection.json\n",
      "INFO 2025-04-06 20:19:52,770 instructlab.sdg.generate_data:401: Taxonomy converted to samples and written to /root/.local/share/instructlab/datasets/2025-04-06_201731/preprocessed_2025-04-06T20_18_53\n",
      "INFO 2025-04-06 20:19:52,791 instructlab.sdg.generate_data:437: Synthesizing new instructions. If you aren't satisfied with the generated instructions, interrupt training (Ctrl-C) and try adjusting your YAML files. Adding more examples may help.\n",
      "INFO 2025-04-06 20:19:52,980 instructlab.sdg.checkpointing:59: No existing checkpoints found in /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity, generating from scratch\n",
      "INFO 2025-04-06 20:19:52,980 instructlab.sdg.pipeline:164: Running pipeline with multi-threaded batching. Using 10 workers for batches of size 32\n",
      "INFO 2025-04-06 20:19:53,020 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:19:53,020 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:19:53,021 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:19:53,023 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:19:53,027 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:19:53,028 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:19:53,030 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:19:53,032 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:19:53,037 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:19:53,038 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:19:53,174 instructlab.sdg.blocks.llmblock:55: LLM server supports batched inputs: True\n",
      "INFO 2025-04-06 20:19:53,174 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "INFO 2025-04-06 20:19:53,207 instructlab.sdg.blocks.llmblock:55: LLM server supports batched inputs: True\n",
      "INFO 2025-04-06 20:19:53,207 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "INFO 2025-04-06 20:19:53,209 instructlab.sdg.blocks.llmblock:55: LLM server supports batched inputs: True\n",
      "INFO 2025-04-06 20:19:53,209 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "INFO 2025-04-06 20:19:53,211 instructlab.sdg.blocks.llmblock:55: LLM server supports batched inputs: True\n",
      "INFO 2025-04-06 20:19:53,211 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "INFO 2025-04-06 20:19:53,240 instructlab.sdg.blocks.llmblock:55: LLM server supports batched inputs: True\n",
      "INFO 2025-04-06 20:19:53,240 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "INFO 2025-04-06 20:19:53,253 instructlab.sdg.blocks.llmblock:55: LLM server supports batched inputs: True\n",
      "INFO 2025-04-06 20:19:53,254 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "INFO 2025-04-06 20:19:53,254 instructlab.sdg.blocks.llmblock:55: LLM server supports batched inputs: True\n",
      "INFO 2025-04-06 20:19:53,256 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "INFO 2025-04-06 20:19:53,257 instructlab.sdg.blocks.llmblock:55: LLM server supports batched inputs: True\n",
      "INFO 2025-04-06 20:19:53,260 instructlab.sdg.blocks.llmblock:55: LLM server supports batched inputs: True\n",
      "INFO 2025-04-06 20:19:53,266 instructlab.sdg.blocks.llmblock:55: LLM server supports batched inputs: True\n",
      "INFO 2025-04-06 20:19:53,282 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "INFO 2025-04-06 20:19:53,283 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "INFO 2025-04-06 20:19:53,283 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "INFO 2025-04-06 20:20:20,119 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:20:20,156 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:20:20,169 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:20:22,112 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:20:22,135 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:20:22,148 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:20:23,669 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:20:23,675 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:20:23,710 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:20:23,734 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:20:23,749 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:20:23,782 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:20:30,367 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:20:30,388 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:20:30,401 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:20:40,952 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:20:40,974 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:20:40,988 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:20:44,515 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:20:44,519 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:20:44,564 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:20:44,567 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:20:44,589 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:20:44,597 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:20:47,630 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:20:47,652 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:20:47,664 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:20:50,686 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:20:50,707 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:20:50,720 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:22:37,806 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:22:45,010 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:22:50,803 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:23:06,192 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:23:06,201 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:23:16,594 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:23:34,609 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:23:34,617 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:23:49,716 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:24:32,737 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:26:27,118 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 85.89 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 111.96 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 84.34 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 109.38 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 89.83 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 111.18 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 92.96 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 117.30 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 94.37 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 118.47 examples/s]\n",
      "num_proc must be <= 7. Reducing num_proc to 7 for dataset of size 7.\n",
      "WARNING 2025-04-06 20:26:33,771 datasets.arrow_dataset:3098: num_proc must be <= 7. Reducing num_proc to 7 for dataset of size 7.\n",
      "Map (num_proc=7): 100%|##########| 7/7 [00:00<00:00, 22.69 examples/s]\n",
      "num_proc must be <= 7. Reducing num_proc to 7 for dataset of size 7.\n",
      "WARNING 2025-04-06 20:26:34,387 datasets.arrow_dataset:3098: num_proc must be <= 7. Reducing num_proc to 7 for dataset of size 7.\n",
      "Filter (num_proc=7): 100%|##########| 7/7 [00:00<00:00, 27.40 examples/s]\n",
      "INFO 2025-04-06 20:26:34,960 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "INFO 2025-04-06 20:27:00,991 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 91.15 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 121.11 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 91.37 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 117.38 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 87.95 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 117.81 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 88.09 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 108.54 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 85.29 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 115.13 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 19/19 [00:00<00:00, 49.26 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 19/19 [00:00<00:00, 69.12 examples/s]\n",
      "INFO 2025-04-06 20:27:08,956 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "INFO 2025-04-06 20:27:34,754 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 88.15 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 115.82 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 83.71 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 121.36 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 88.07 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 115.47 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 92.84 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 114.61 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 73.59 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 111.77 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 85.37 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 120.14 examples/s]\n",
      "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "WARNING 2025-04-06 20:27:42,782 datasets.arrow_dataset:3098: num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "Map (num_proc=3): 100%|##########| 3/3 [00:00<00:00, 11.69 examples/s]\n",
      "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "WARNING 2025-04-06 20:27:43,192 datasets.arrow_dataset:3098: num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "Filter (num_proc=3): 100%|##########| 3/3 [00:00<00:00, 13.26 examples/s]\n",
      "INFO 2025-04-06 20:27:43,589 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "INFO 2025-04-06 20:28:12,647 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8):  25%|##5       | 8/32 [00:00<00:00, 70.79 examples/s]INFO 2025-04-06 20:28:13,134 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 52.04 examples/s] \n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 69.02 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 48.51 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  25%|##5       | 8/32 [00:00<00:00, 70.51 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 70.59 examples/s] \n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 65.56 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[AINFO 2025-04-06 20:28:15,348 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 46.98 examples/s]\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 23.14 examples/s]\u001b[A\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 48.02 examples/s]\u001b[A\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 56.41 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 31.50 examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:03,  9.24 examples/s]\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 39.74 examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 19.53 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 32.15 examples/s]\n",
      "\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 34.87 examples/s]\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 97.78 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 54.83 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 30.95 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 65.95 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 44.72 examples/s]\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 56.79 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 40.51 examples/s]\n",
      "\n",
      "Map (num_proc=8):  25%|##5       | 8/32 [00:00<00:00, 26.22 examples/s]\u001b[A\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 33.50 examples/s]\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 30.95 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 46.47 examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 19.77 examples/s]\u001b[A\n",
      "Map (num_proc=8):  25%|##5       | 8/32 [00:00<00:00, 24.06 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 28.36 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 33.16 examples/s]\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 80.57 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 53.42 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 32.51 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 64.43 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 52.08 examples/s]\n",
      "Filter (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 26.28 examples/s]\n",
      "Filter (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 39.81 examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 29.89 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 34.87 examples/s]\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 38.02 examples/s]\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 38.64 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 34.77 examples/s]\n",
      "\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 33.98 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 30.77 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 70.58 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 83.39 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 54.66 examples/s]\n",
      "INFO 2025-04-06 20:28:25,211 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 22.37 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 52.17 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 24.58 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[AINFO 2025-04-06 20:28:26,943 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 17.56 examples/s]\n",
      "Map (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 23.65 examples/s]\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 32.66 examples/s]\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 30.92 examples/s]\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 30.77 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 25.37 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 25.22 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:02,  9.56 examples/s]\n",
      "Map (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 16.16 examples/s]\n",
      "Filter (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 22.66 examples/s]\u001b[A\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 27.01 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 51.45 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\u001b[AINFO 2025-04-06 20:28:28,985 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 33.00 examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 26.07 examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 25.26 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:01<00:00, 24.73 examples/s]\u001b[A\u001b[A\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 37.36 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 22.76 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 51.59 examples/s]\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 33.31 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 36.96 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 30.87 examples/s]\n",
      "Map (num_proc=8):  25%|##5       | 8/32 [00:00<00:02, 10.03 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:01<00:00, 23.93 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:01<00:00, 31.08 examples/s]\n",
      "Map (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 20.82 examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:02, 11.35 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 23.39 examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 19.03 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 19.25 examples/s]\n",
      "\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 33.84 examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 41.99 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 32.13 examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 25.52 examples/s]\n",
      "\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:01<00:00, 17.13 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 17.79 examples/s]\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 38.42 examples/s]\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 16.32 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 28.86 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 34.35 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 26.11 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 28.47 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 27.80 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 39.40 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 32.07 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 22.25 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 24.50 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 12.05 examples/s]\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 25.40 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 42.71 examples/s]\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 24.25 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:01<00:00, 30.99 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 16.07 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 21.82 examples/s]\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:03,  8.55 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 27.40 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 36.51 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 32.20 examples/s]\u001b[A\u001b[A\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 26.80 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 21.44 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 20.44 examples/s]\u001b[A\n",
      "\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 20.38 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "Filter (num_proc=8):  25%|##5       | 8/32 [00:00<00:00, 27.33 examples/s]\u001b[A\n",
      "\n",
      "\n",
      "Filter (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 18.75 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "Filter (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 22.45 examples/s]\u001b[A\n",
      "\n",
      "\n",
      "Filter (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 24.55 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 23.30 examples/s]\u001b[A\n",
      "\n",
      "\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 24.82 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "Filter (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 23.63 examples/s]\u001b[A\n",
      "\n",
      "\n",
      "Filter (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 22.51 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:01<00:00, 26.91 examples/s]\u001b[A\n",
      "\n",
      "\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:01<00:00, 23.76 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:01<00:00, 27.46 examples/s]\u001b[A\n",
      "\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 22.41 examples/s]\n",
      "\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 20.31 examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:02, 10.76 examples/s]\n",
      "\n",
      "Map (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 13.74 examples/s]\n",
      "\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:01, 18.41 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 21.92 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 27.67 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:01<00:00, 29.98 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/18 [00:00<?, ? examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 22.11 examples/s]\n",
      "\n",
      "Map (num_proc=8):  17%|#6        | 3/18 [00:00<00:00, 16.80 examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:02, 10.52 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  44%|####4     | 8/18 [00:00<00:00, 18.25 examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 29.29 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  56%|#####5    | 10/18 [00:00<00:00, 17.11 examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 30.38 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  67%|######6   | 12/18 [00:00<00:00, 11.77 examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:01<00:00, 30.92 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  78%|#######7  | 14/18 [00:01<00:00, 11.66 examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 30.05 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\u001b[A\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 22.37 examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:02,  9.86 examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 18/18 [00:01<00:00, 10.47 examples/s]\n",
      "\n",
      "\n",
      "\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 29.45 examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:01, 18.76 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 22.24 examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 26.95 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:01<00:00, 25.44 examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 29.95 examples/s]\n",
      "\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:02, 12.17 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 20.89 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 19.85 examples/s]\n",
      "\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 37.99 examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:02,  9.50 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 25.74 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 33.93 examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 37.37 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 26.16 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 29.71 examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:03,  8.89 examples/s]INFO 2025-04-06 20:28:43,637 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 43.61 examples/s]\n",
      "Filter (num_proc=8):  17%|#6        | 3/18 [00:00<00:01, 10.61 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 26.39 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  67%|######6   | 12/18 [00:00<00:00, 19.53 examples/s]\u001b[A\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 18/18 [00:01<00:00, 12.67 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 19.93 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:05,  4.75 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:01<00:00, 25.76 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:01<00:00, 26.95 examples/s]\n",
      "\n",
      "\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\u001b[A\u001b[A\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:05,  5.49 examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:01<00:00, 27.80 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:01, 16.33 examples/s]\u001b[A\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:01<00:00, 25.86 examples/s]\u001b[A\n",
      "\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 19.83 examples/s]\n",
      "\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:01<00:00, 30.70 examples/s]\u001b[A\n",
      "\n",
      "\n",
      "Filter (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 29.83 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:03,  8.23 examples/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Filter (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 38.31 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 48.97 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Map (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 13.28 examples/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 20.03 examples/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 36.34 examples/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:02<00:00, 13.45 examples/s]\n",
      "INFO 2025-04-06 20:28:47,287 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 46.63 examples/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 15.07 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 19.99 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 34.17 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 29.75 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:02<00:00, 15.90 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 29.15 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/9 [00:00<?, ? examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 19.83 examples/s]\n",
      "\n",
      "Map (num_proc=8):  22%|##2       | 2/9 [00:00<00:00,  8.49 examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 15.03 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Map (num_proc=8):  14%|#4        | 2/14 [00:00<00:02,  5.65 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 19.10 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  33%|###3      | 3/9 [00:00<00:01,  5.07 examples/s]\u001b[A\n",
      "\n",
      "\n",
      "Map (num_proc=8):  29%|##8       | 4/14 [00:00<00:01,  7.65 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 20.72 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  56%|#####5    | 5/9 [00:00<00:00,  7.51 examples/s]\u001b[A\n",
      "\n",
      "\n",
      "Map (num_proc=8):  43%|####2     | 6/14 [00:00<00:00,  9.10 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 37.55 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  67%|######6   | 6/9 [00:00<00:00,  8.01 examples/s]\u001b[A\n",
      "\n",
      "\n",
      "Map (num_proc=8):  57%|#####7    | 8/14 [00:00<00:00, 11.46 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 46.41 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  78%|#######7  | 7/9 [00:00<00:00,  7.78 examples/s]\u001b[A\n",
      "\n",
      "\n",
      "Map (num_proc=8):  71%|#######1  | 10/14 [00:01<00:00, 12.26 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "Map (num_proc=8):  89%|########8 | 8/9 [00:01<00:00,  6.87 examples/s]\u001b[A\n",
      "\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 26.08 examples/s]\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 14/14 [00:01<00:00, 10.37 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 9/9 [00:01<00:00,  5.79 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 14/14 [00:01<00:00,  8.37 examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 28.58 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  25%|##5       | 8/32 [00:00<00:00, 24.77 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 22.58 examples/s]\n",
      "\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 39.92 examples/s]\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 49.67 examples/s]\u001b[A\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 32.67 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 27.53 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:03,  8.86 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 14.76 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  38%|###7      | 12/32 [00:00<00:01, 18.86 examples/s]\u001b[A\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:01<00:00, 27.16 examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=8):   0%|          | 0/14 [00:00<?, ? examples/s]\u001b[A\u001b[A\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:01<00:00, 27.41 examples/s]\u001b[A\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:02, 12.25 examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 17.25 examples/s]\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 18.38 examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:01, 19.58 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  71%|#######1  | 10/14 [00:00<00:00, 17.51 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 25.04 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  93%|#########2| 13/14 [00:00<00:00, 18.23 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:01<00:00, 31.96 examples/s]\n",
      "\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 14/14 [00:01<00:00, 12.57 examples/s]\n",
      "\n",
      "\n",
      "\n",
      "Filter (num_proc=8):  56%|#####5    | 5/9 [00:00<00:00,  7.26 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 21.84 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "\n",
      "\n",
      "Filter (num_proc=8):  78%|#######7  | 7/9 [00:01<00:00,  6.53 examples/s]\u001b[A\u001b[A\u001b[A\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 19.91 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 9/9 [00:01<00:00,  6.39 examples/s]\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 44.51 examples/s]\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 45.83 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 45.08 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 35.76 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/20 [00:00<?, ? examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 29.58 examples/s]\n",
      "\n",
      "Map (num_proc=8):  45%|####5     | 9/20 [00:00<00:00, 20.43 examples/s]\u001b[A\n",
      "Map (num_proc=8):  60%|######    | 12/20 [00:00<00:00, 22.35 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 20/20 [00:01<00:00, 19.88 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 89.90 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 49.23 examples/s]\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 17.86 examples/s]\u001b[A\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 26.29 examples/s]\u001b[A\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 29.05 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 57.59 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 42.21 examples/s]\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/31 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):  52%|#####1    | 16/31 [00:00<00:00, 39.73 examples/s]\n",
      "\n",
      "Map (num_proc=8):  90%|######### | 28/31 [00:00<00:00, 42.23 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 31/31 [00:01<00:00, 27.80 examples/s]\n",
      "\n",
      "\n",
      "INFO 2025-04-06 20:28:59,528 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "Filter (num_proc=8): 100%|##########| 20/20 [00:01<00:00, 11.62 examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 26.04 examples/s]INFO 2025-04-06 20:28:59,680 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 39.14 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 90.56 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 44.45 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  13%|#2        | 4/31 [00:00<00:01, 25.33 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 31/31 [00:00<00:00, 53.88 examples/s] \n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 25.62 examples/s]\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 35.12 examples/s]INFO 2025-04-06 20:29:01,483 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 27.99 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 152.08 examples/s]\u001b[AINFO 2025-04-06 20:29:01,700 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 34.83 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 80.18 examples/s] \n",
      "Filter (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 59.98 examples/s]\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 53.09 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 44.43 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 55.39 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 59.58 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 42.63 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 121.38 examples/s]\n",
      "Filter (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 56.16 examples/s]\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 60.42 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 47.83 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 54.29 examples/s] \n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 82.28 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 48.03 examples/s]\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 77.14 examples/s]\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 56.14 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/9 [00:00<?, ? examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 45.72 examples/s]\n",
      "INFO 2025-04-06 20:29:07,313 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 9/9 [00:00<00:00, 13.50 examples/s]\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 30.92 examples/s]\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 39.46 examples/s]\n",
      "Map (num_proc=8):  15%|#5        | 2/13 [00:00<00:00, 16.81 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 37.36 examples/s]\n",
      "\n",
      "Filter (num_proc=8):   0%|          | 0/9 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8): 100%|##########| 13/13 [00:00<00:00, 13.60 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 9/9 [00:00<00:00, 17.12 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 9/9 [00:00<00:00, 13.22 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 27.78 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 48.55 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 54.18 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 47.36 examples/s]\n",
      "Filter (num_proc=8):  15%|#5        | 2/13 [00:00<00:00, 15.69 examples/s]INFO 2025-04-06 20:29:10,173 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "Filter (num_proc=8): 100%|##########| 13/13 [00:00<00:00, 28.52 examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 36.97 examples/s]INFO 2025-04-06 20:29:10,736 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 81.93 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 116.84 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 92.68 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 117.74 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 91.71 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 106.52 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 89.32 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 119.15 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 87.22 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 107.44 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 17/17 [00:00<00:00, 51.90 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 17/17 [00:00<00:00, 60.63 examples/s]\n",
      "INFO 2025-04-06 20:29:18,185 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "INFO 2025-04-06 20:29:41,427 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]INFO 2025-04-06 20:29:41,873 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 84.35 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 46.49 examples/s]\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 36.25 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 54.64 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 56.77 examples/s] \n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 59.81 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 48.11 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 51.86 examples/s]\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 118.55 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 49.49 examples/s] \n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 35.56 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 61.16 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 61.13 examples/s] \n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 46.61 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 47.64 examples/s]\n",
      "Map (num_proc=8):  13%|#2        | 4/31 [00:00<00:00, 38.78 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 41.21 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 31/31 [00:00<00:00, 82.87 examples/s] \n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 51.12 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/31 [00:00<?, ? examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 46.28 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 31/31 [00:00<00:00, 96.20 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]INFO 2025-04-06 20:29:48,686 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 84.50 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 111.72 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 87.74 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 112.19 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 94.30 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 105.83 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 97.54 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 114.63 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 20/20 [00:00<00:00, 56.83 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 20/20 [00:00<00:00, 69.65 examples/s]\n",
      "INFO 2025-04-06 20:29:55,016 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "INFO 2025-04-06 20:30:18,806 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 88.89 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 108.40 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 87.46 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 109.76 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 90.69 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 110.33 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 20/20 [00:00<00:00, 55.01 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 20/20 [00:00<00:00, 71.79 examples/s]\n",
      "INFO 2025-04-06 20:30:24,253 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "INFO 2025-04-06 20:30:52,063 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 85.43 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 117.65 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 79.66 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 113.23 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 89.06 examples/s] \n",
      "INFO 2025-04-06 20:30:55,467 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:03,  8.44 examples/s]\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 21.17 examples/s]INFO 2025-04-06 20:30:56,564 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 71.51 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 28.43 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 39.19 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 27.54 examples/s]\n",
      "\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 24.50 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 62.52 examples/s] \n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 27.10 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 61.91 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 42.82 examples/s]\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 14.83 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 22.00 examples/s]\u001b[A\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 36.04 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 40.17 examples/s]\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 36.74 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 33.49 examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 48.94 examples/s]\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 39.22 examples/s]\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 38.81 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 33.43 examples/s]\n",
      "\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 39.95 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 68.88 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 46.15 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 94.74 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 51.20 examples/s]\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 35.07 examples/s]\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 28.90 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 38.26 examples/s]\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 36.53 examples/s]\u001b[A\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 35.50 examples/s]\u001b[A\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 34.82 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 25.72 examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8):  13%|#2        | 4/31 [00:00<00:00, 34.17 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 54.19 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 36.41 examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 31/31 [00:00<00:00, 43.77 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 73.37 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 46.53 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 28.80 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 94.05 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 47.73 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 31/31 [00:00<00:00, 57.14 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]INFO 2025-04-06 20:31:06,846 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 36.08 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 31.69 examples/s]\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 27.02 examples/s]\n",
      "\n",
      "Map (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 17.40 examples/s]\u001b[A\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 32.68 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:02, 13.10 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 42.00 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 51.43 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 31.21 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 29.10 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 33.28 examples/s]\n",
      "INFO 2025-04-06 20:31:08,971 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 69.27 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 85.68 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 49.20 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:03,  8.71 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 38.95 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 50.91 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 33.32 examples/s]\n",
      "\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 32.06 examples/s]\u001b[A\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 29.39 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 51.43 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 40.51 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 28.37 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  25%|##5       | 8/32 [00:00<00:00, 27.53 examples/s]\u001b[A\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 39.55 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 62.75 examples/s]\n",
      "num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]WARNING 2025-04-06 20:31:13,554 datasets.arrow_dataset:3098: num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 42.07 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 15.69 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 30.31 examples/s]INFO 2025-04-06 20:31:14,008 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 37.33 examples/s]\n",
      "Map (num_proc=6):  67%|######6   | 4/6 [00:00<00:00, 10.41 examples/s]\n",
      "Map (num_proc=6): 100%|##########| 6/6 [00:00<00:00,  6.84 examples/s]\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):  27%|##7       | 6/22 [00:00<00:01, 13.18 examples/s]\u001b[Anum_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "WARNING 2025-04-06 20:31:15,399 datasets.arrow_dataset:3098: num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 14.50 examples/s]\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 39.35 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 22/22 [00:01<00:00, 17.58 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 37.11 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/24 [00:00<?, ? examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=6):  17%|#6        | 1/6 [00:00<00:01,  2.64 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 25.54 examples/s]INFO 2025-04-06 20:31:16,503 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 3/24 [00:00<00:01, 14.76 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 24/24 [00:00<00:00, 35.66 examples/s]\n",
      "Filter (num_proc=6): 100%|##########| 6/6 [00:01<00:00,  5.26 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/22 [00:00<?, ? examples/s]\n",
      "Filter (num_proc=8):  41%|####      | 9/22 [00:00<00:00, 16.42 examples/s]INFO 2025-04-06 20:31:17,947 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Filter (num_proc=8):  82%|########1 | 18/22 [00:00<00:00, 30.04 examples/s]\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\u001b[A\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:03,  8.17 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 14.88 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 14.94 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 25.84 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 37.79 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Filter (num_proc=8):   0%|          | 0/24 [00:00<?, ? examples/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 39.12 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 22/22 [00:02<00:00, 10.81 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 26.12 examples/s]\n",
      "\n",
      "\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]INFO 2025-04-06 20:31:19,468 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 17.44 examples/s]\n",
      "\n",
      "\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 24/24 [00:00<00:00, 35.05 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 43.44 examples/s]\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:02,  9.86 examples/s]\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 35.87 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 41.19 examples/s]\n",
      "\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 62.98 examples/s]\u001b[AINFO 2025-04-06 20:31:21,107 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "\n",
      "INFO 2025-04-06 20:31:21,209 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 38.39 examples/s]\u001b[AINFO 2025-04-06 20:31:21,530 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 34.51 examples/s]\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 66.37 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 46.88 examples/s]\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 25.26 examples/s]\u001b[A\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 65.95 examples/s]\u001b[A\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 29.24 examples/s]\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 44.11 examples/s]\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 60.76 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 42.51 examples/s]\n",
      "\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 51.19 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 34.99 examples/s]\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 53.16 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 39.86 examples/s]\n",
      "\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 48.27 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 31.85 examples/s]\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 47.85 examples/s]\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 29.06 examples/s]\u001b[A\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 28.77 examples/s]\u001b[A\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 37.89 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 30.82 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 59.95 examples/s] \n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 60.48 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 35.57 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:02, 12.67 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 44.41 examples/s]\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 54.31 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 45.72 examples/s]\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 27.18 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 36.42 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 51.78 examples/s] \n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 48.28 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 36.78 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:02, 12.06 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 20.30 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 32.92 examples/s]\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 22.61 examples/s]\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 36.11 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 53.93 examples/s]\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 82.06 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 30.55 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 52.91 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 21/24 [00:00<00:00, 29.71 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[AINFO 2025-04-06 20:31:32,278 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 24/24 [00:01<00:00, 23.99 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 37.03 examples/s]\u001b[A\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 43.58 examples/s]\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 30.06 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 47.05 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 27.56 examples/s]\n",
      "\n",
      "Filter (num_proc=8):   0%|          | 0/24 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 36.83 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  12%|#2        | 3/24 [00:00<00:03,  6.22 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  50%|#####     | 12/24 [00:00<00:00, 25.26 examples/s]\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 24/24 [00:00<00:00, 27.79 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 20.54 examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8):  67%|######6   | 6/9 [00:00<00:00, 15.02 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 9/9 [00:00<00:00, 10.42 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 70.59 examples/s]\n",
      "Filter (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 45.03 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 37.55 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  22%|##2       | 2/9 [00:00<00:01,  5.65 examples/s]\u001b[AINFO 2025-04-06 20:31:37,048 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:31:37,062 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_daf915ba70864d7387afd9e58ee1305f.jsonl\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/11 [00:00<?, ? examples/s]INFO 2025-04-06 20:31:37,274 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "Filter (num_proc=8): 100%|##########| 9/9 [00:00<00:00, 11.79 examples/s]\n",
      "Map (num_proc=8):  91%|######### | 10/11 [00:00<00:00, 17.30 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 33.12 examples/s]\u001b[A\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 77.85 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 11/11 [00:00<00:00, 12.42 examples/s]\n",
      "INFO 2025-04-06 20:31:38,054 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 16.75ba/s]\n",
      "INFO 2025-04-06 20:31:38,100 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 53.94 examples/s] \n",
      "Filter (num_proc=8):  82%|########1 | 9/11 [00:00<00:00, 42.39 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 11/11 [00:00<00:00, 19.02 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 38.08 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 83.24 examples/s]\u001b[AINFO 2025-04-06 20:31:39,246 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 97.45 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 84.92 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 116.38 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 90.22 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 113.73 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 26/26 [00:00<00:00, 63.45 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 26/26 [00:00<00:00, 94.50 examples/s]\n",
      "INFO 2025-04-06 20:31:43,500 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:31:43,500 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_a01afdbf3e0a474895c7c13cbb95a70e.jsonl\n",
      "INFO 2025-04-06 20:31:43,513 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 74.64ba/s]\n",
      "INFO 2025-04-06 20:31:52,360 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:31:52,383 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:31:52,396 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:31:52,468 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 85.80 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 115.25 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 90.56 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 114.07 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 93.27 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 110.30 examples/s]\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 86.04 examples/s]INFO 2025-04-06 20:31:57,044 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 51.56 examples/s]\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 44.88 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 38.47 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 109.37 examples/s]\n",
      "Filter (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 63.32 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 53.89 examples/s]\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 33.89 examples/s]\u001b[A\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 69.30 examples/s]\u001b[A\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 38.63 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 85.39 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 60.90 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 68.87 examples/s]\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 33.81 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 35.33 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 37.01 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 78.52 examples/s]\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 59.58 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]INFO 2025-04-06 20:32:03,363 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "\u001b[AINFO 2025-04-06 20:32:03,479 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 45.47 examples/s]\n",
      "INFO 2025-04-06 20:32:03,552 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 20.78 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 54.75 examples/s]\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 53.86 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 52.06 examples/s]\n",
      "Map (num_proc=8):  13%|#3        | 2/15 [00:00<00:00, 19.78 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 41.80 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 15/15 [00:00<00:00, 39.32 examples/s]\n",
      "Filter (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 44.66 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/15 [00:00<?, ? examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 45.02 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 15/15 [00:00<00:00, 53.44 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]INFO 2025-04-06 20:32:06,299 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 37.06 examples/s]INFO 2025-04-06 20:32:06,442 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 85.91 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 111.71 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 19/19 [00:00<00:00, 55.76 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 19/19 [00:00<00:00, 66.74 examples/s]\n",
      "INFO 2025-04-06 20:32:08,684 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "INFO 2025-04-06 20:32:09,927 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:32:09,950 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:32:09,963 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:32:14,595 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:32:14,619 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:32:14,632 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:32:48,703 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:32:48,726 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:32:48,739 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:33:24,394 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:33:32,229 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:33:40,306 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 87.91 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 112.03 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 86.61 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 109.75 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 86.45 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 117.69 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 87.23 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 113.08 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 86.19 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 116.99 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 23/23 [00:00<00:00, 64.16 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 23/23 [00:00<00:00, 81.34 examples/s]\n",
      "INFO 2025-04-06 20:33:48,475 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:33:48,486 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "INFO 2025-04-06 20:34:10,485 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:34:16,250 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:34:17,155 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 86.78 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 116.32 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 88.27 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 107.74 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 79.80 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 108.56 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 90.45 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 111.35 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 88.82 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 109.62 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 8/8 [00:00<00:00, 23.13 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/8 [00:00<?, ? examples/s]INFO 2025-04-06 20:34:25,051 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Filter (num_proc=8): 100%|##########| 8/8 [00:00<00:00, 16.65 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]INFO 2025-04-06 20:34:25,579 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:34:25,580 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_b3cf6299d4d94381b9882130d2db66a9.jsonl\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 33.98 examples/s]INFO 2025-04-06 20:34:25,704 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 51.58 examples/s]\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 23.94ba/s]\n",
      "INFO 2025-04-06 20:34:25,812 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_34616de89df544b6b68aa7a0bcd7bdd7.jsonl\n",
      "\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 88.24ba/s]\n",
      "INFO 2025-04-06 20:34:25,832 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_f8bf625ec9ff4d85bbfc1b97be199ed7.jsonl\n",
      "\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 116.49ba/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 74.99 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 113.87 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 83.65 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 110.99 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 82.25 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]INFO 2025-04-06 20:34:29,240 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 105.33 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 50.29 examples/s] \n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 24.66 examples/s]\u001b[A\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 40.37 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 59.63 examples/s] \n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 72.92 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 61.47 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 44.99 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 98.99 examples/s]\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 33.07 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 53.14 examples/s]\n",
      "Map (num_proc=8):  26%|##6       | 6/23 [00:00<00:00, 53.57 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 39.80 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 23/23 [00:00<00:00, 56.22 examples/s]\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 51.66 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/23 [00:00<?, ? examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 44.94 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 23/23 [00:00<00:00, 77.31 examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 25.52 examples/s]INFO 2025-04-06 20:34:34,556 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 75.55 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 111.89 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 87.20 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 112.95 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 29/29 [00:00<00:00, 84.46 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 29/29 [00:00<00:00, 93.75 examples/s]\n",
      "INFO 2025-04-06 20:34:38,220 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:34:38,230 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "INFO 2025-04-06 20:34:40,554 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:34:40,584 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:34:40,597 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:34:45,838 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 84.70 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 112.34 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 89.64 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 108.06 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 81.07 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 118.43 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 87.49 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 102.24 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 80.09 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 107.09 examples/s]\n",
      "num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n",
      "WARNING 2025-04-06 20:34:52,787 datasets.arrow_dataset:3098: num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n",
      "Map (num_proc=4): 100%|##########| 4/4 [00:00<00:00, 14.65 examples/s]\n",
      "num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n",
      "WARNING 2025-04-06 20:34:53,262 datasets.arrow_dataset:3098: num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n",
      "Filter (num_proc=4): 100%|##########| 4/4 [00:00<00:00, 17.28 examples/s]\n",
      "INFO 2025-04-06 20:34:53,710 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:34:53,722 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "INFO 2025-04-06 20:34:57,815 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:35:07,853 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:35:07,876 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:35:07,889 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:35:22,840 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:35:22,862 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:35:22,875 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:35:41,437 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:35:41,459 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:35:41,472 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:36:37,197 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:37:10,299 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:37:18,551 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:37:32,853 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 81.29 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 115.91 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 81.20 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 113.80 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 87.14 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 114.85 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 92.59 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 103.66 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 11/11 [00:00<00:00, 30.98 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 11/11 [00:00<00:00, 38.82 examples/s]\n",
      "INFO 2025-04-06 20:37:39,806 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:37:39,806 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_03aebc2cd7fa41c599fa0d5c0cdb51d6.jsonl\n",
      "INFO 2025-04-06 20:37:39,821 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 102.95ba/s]\n",
      "INFO 2025-04-06 20:37:39,847 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_f7b7ae4c72a544e2b616dba9007cc77c.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 140.07ba/s]\n",
      "INFO 2025-04-06 20:37:39,860 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_536e137bb165430e8a40704599f60600.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 113.77ba/s]\n",
      "INFO 2025-04-06 20:37:39,875 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_77d42142681b4cbfb685e733b433422c.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 132.73ba/s]\n",
      "INFO 2025-04-06 20:37:39,887 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_71e0f4a30b954568a60319c68379e30f.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 88.79ba/s]\n",
      "INFO 2025-04-06 20:38:02,438 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:38:20,274 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 77.03 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 111.70 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 93.87 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 112.72 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 87.92 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 112.50 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 90.97 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 108.89 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 14/14 [00:00<00:00, 40.43 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 14/14 [00:00<00:00, 43.17 examples/s]\n",
      "INFO 2025-04-06 20:38:27,202 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "INFO 2025-04-06 20:38:41,069 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 86.74 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 108.68 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 84.93 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 110.26 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 89.76 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 111.27 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 90.18 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 113.55 examples/s]\n",
      "INFO 2025-04-06 20:38:46,610 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:38:46,657 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:38:46,677 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 91.77 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 114.34 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 31/31 [00:00<00:00, 81.90 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 31/31 [00:00<00:00, 113.43 examples/s]\n",
      "INFO 2025-04-06 20:38:49,448 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "INFO 2025-04-06 20:38:58,755 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 88.83 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 115.65 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 89.44 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 118.12 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 84.45 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 98.61 examples/s] \n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 87.86 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 117.04 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 87.48 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 109.01 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 88.71 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 116.40 examples/s]\n",
      "num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n",
      "WARNING 2025-04-06 20:39:07,063 datasets.arrow_dataset:3098: num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n",
      "Map (num_proc=4): 100%|##########| 4/4 [00:00<00:00, 14.63 examples/s]\n",
      "num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n",
      "WARNING 2025-04-06 20:39:07,535 datasets.arrow_dataset:3098: num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n",
      "Filter (num_proc=4): 100%|##########| 4/4 [00:00<00:00, 17.50 examples/s]\n",
      "INFO 2025-04-06 20:39:07,980 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "INFO 2025-04-06 20:39:08,112 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 86.64 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 111.50 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 91.17 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 93.02 examples/s] \n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 85.58 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 114.74 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 79.05 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 110.88 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 90.17 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 105.51 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 89.41 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 106.63 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 86.34 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 106.38 examples/s]\n",
      "num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "WARNING 2025-04-06 20:39:17,942 datasets.arrow_dataset:3098: num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "Map (num_proc=6): 100%|##########| 6/6 [00:00<00:00, 18.62 examples/s]\n",
      "num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "WARNING 2025-04-06 20:39:18,554 datasets.arrow_dataset:3098: num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "Filter (num_proc=6): 100%|##########| 6/6 [00:00<00:00, 22.95 examples/s]\n",
      "INFO 2025-04-06 20:39:19,105 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "INFO 2025-04-06 20:39:38,971 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 91.71 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 106.88 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 83.56 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 113.44 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 86.68 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 109.01 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 88.04 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 115.34 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 85.35 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 107.33 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 90.11 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 104.35 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 92.57 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 111.96 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 16/16 [00:00<00:00, 44.57 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 16/16 [00:00<00:00, 55.43 examples/s]\n",
      "INFO 2025-04-06 20:39:50,139 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "INFO 2025-04-06 20:40:42,891 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 87.31 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 110.45 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 86.76 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 106.18 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 89.71 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 114.18 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 86.64 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 115.09 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 31/31 [00:00<00:00, 81.46 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 31/31 [00:00<00:00, 109.87 examples/s]\n",
      "INFO 2025-04-06 20:40:49,884 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "INFO 2025-04-06 20:40:50,117 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:40:55,918 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 81.36 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 110.09 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 91.27 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 108.35 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 85.66 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 109.92 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 85.17 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 109.52 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 9/9 [00:00<00:00, 21.38 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 9/9 [00:00<00:00, 31.87 examples/s]\n",
      "INFO 2025-04-06 20:41:03,032 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "INFO 2025-04-06 20:41:05,303 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 79.90 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 113.12 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 93.14 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 113.04 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 84.81 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 112.11 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]INFO 2025-04-06 20:41:09,873 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 90.55 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 43.46 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 48.50 examples/s]\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 61.03 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 45.02 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 82.96 examples/s]\n",
      "Map (num_proc=8):  70%|#######   | 7/10 [00:00<00:00, 23.80 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 10/10 [00:00<00:00, 16.94 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 10/10 [00:00<00:00, 13.04 examples/s]\n",
      "\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 83.43 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 52.95 examples/s] \n",
      "Filter (num_proc=8):  80%|########  | 8/10 [00:00<00:00, 15.76 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 10/10 [00:00<00:00, 13.12 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 108.28 examples/s]\n",
      "INFO 2025-04-06 20:41:14,303 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 87.75 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 107.92 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 89.00 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 115.16 examples/s]\n",
      "num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n",
      "WARNING 2025-04-06 20:41:17,188 datasets.arrow_dataset:3098: num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n",
      "Map (num_proc=5): 100%|##########| 5/5 [00:00<00:00, 17.27 examples/s]\n",
      "num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n",
      "WARNING 2025-04-06 20:41:17,727 datasets.arrow_dataset:3098: num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n",
      "Filter (num_proc=5): 100%|##########| 5/5 [00:00<00:00, 19.69 examples/s]\n",
      "INFO 2025-04-06 20:41:18,225 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "INFO 2025-04-06 20:41:53,303 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 86.08 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 115.51 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 86.32 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 101.19 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 85.11 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 116.01 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 79.36 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 107.47 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 84.79 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 100.50 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 16/16 [00:00<00:00, 40.09 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 16/16 [00:00<00:00, 51.22 examples/s]\n",
      "INFO 2025-04-06 20:42:02,262 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "INFO 2025-04-06 20:42:16,925 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 81.05 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 112.69 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 82.04 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 113.30 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 80.62 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 109.45 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 81.34 examples/s]\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 75.13 examples/s]INFO 2025-04-06 20:42:22,783 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 64.13 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]num_proc must be <= 7. Reducing num_proc to 7 for dataset of size 7.\n",
      "WARNING 2025-04-06 20:42:23,224 datasets.arrow_dataset:3098: num_proc must be <= 7. Reducing num_proc to 7 for dataset of size 7.\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 42.04 examples/s]\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 41.45 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 49.66 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 37.04 examples/s]\n",
      "Map (num_proc=7): 100%|##########| 7/7 [00:00<00:00, 18.58 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]num_proc must be <= 7. Reducing num_proc to 7 for dataset of size 7.\n",
      "WARNING 2025-04-06 20:42:24,538 datasets.arrow_dataset:3098: num_proc must be <= 7. Reducing num_proc to 7 for dataset of size 7.\n",
      "Filter (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 46.03 examples/s]\n",
      "Filter (num_proc=7):   0%|          | 0/7 [00:00<?, ? examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 45.88 examples/s]\n",
      "Filter (num_proc=7): 100%|##########| 7/7 [00:00<00:00, 24.78 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]INFO 2025-04-06 20:42:25,679 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 80.35 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 113.86 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 83.94 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 89.42 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 90.84 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 108.86 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 83.56 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 113.13 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 85.23 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 113.64 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 22/22 [00:00<00:00, 60.82 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 22/22 [00:00<00:00, 73.79 examples/s]\n",
      "INFO 2025-04-06 20:42:33,911 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "INFO 2025-04-06 20:42:38,549 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 77.56 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 105.84 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 87.10 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 106.04 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]INFO 2025-04-06 20:42:41,870 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 49.30 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 44.85 examples/s]\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 32.82 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 49.69 examples/s]\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 112.35 examples/s]INFO 2025-04-06 20:42:43,356 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 61.75 examples/s] \n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]INFO 2025-04-06 20:42:43,897 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 50.77 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 44.23 examples/s]\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:02, 12.52 examples/s]\u001b[A\n",
      "Map (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 20.40 examples/s]\u001b[A\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 24.53 examples/s]\u001b[A\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:03,  8.95 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 31.13 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 21.69 examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:01<00:00, 29.10 examples/s]\n",
      "\n",
      "Map (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 13.44 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 21.96 examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 24.23 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 41.77 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 14.87 examples/s]\u001b[A\n",
      "Map (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 22.44 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 21.06 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 30.66 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 71.50 examples/s] \n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 45.83 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 40.18 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:03,  7.08 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 12.77 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 26.56 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:03,  8.17 examples/s]\n",
      "Filter (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 22.74 examples/s]\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 23.70 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 58.04 examples/s]\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:02, 10.51 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):   0%|          | 0/12 [00:00<?, ? examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 21.19 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 40.40 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  17%|#6        | 2/12 [00:00<00:00, 12.43 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 41.99 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  50%|#####     | 6/12 [00:00<00:00, 18.73 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 16.86 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 23.27 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 12/12 [00:00<00:00, 12.71 examples/s]\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 37.14 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 41.69 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 31.55 examples/s]\n",
      "\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 28.86 examples/s]\u001b[A\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 31.47 examples/s]\u001b[A\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 34.25 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 15.83 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 23.01 examples/s]\n",
      "\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 37.19 examples/s]\n",
      "\n",
      "\n",
      "Filter (num_proc=8):  33%|###3      | 4/12 [00:00<00:00, 13.26 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Filter (num_proc=8):  50%|#####     | 6/12 [00:00<00:00, 14.35 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 12/12 [00:00<00:00, 13.38 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 64.31 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:02, 12.66 examples/s]\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 24.39 examples/s]\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 42.21 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 41.70 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 27.93 examples/s]\n",
      "INFO 2025-04-06 20:42:55,743 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 21.53 examples/s]\u001b[A\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 52.58 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 30.36 examples/s]\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 34.75 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 45.03 examples/s]\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 28.79 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 32.84 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 27.03 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 53.79 examples/s] \n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 54.53 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 37.48 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 36.25 examples/s]\n",
      "\n",
      "Map (num_proc=8):  53%|#####3    | 16/30 [00:00<00:00, 69.80 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 30/30 [00:00<00:00, 32.02 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 84.15 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 49.96 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 26.71 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 63.19 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 60.56 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 41.14 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 30/30 [00:00<00:00, 64.73 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 30/30 [00:00<00:00, 41.44 examples/s]\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 30.21 examples/s]\u001b[A\n",
      "Map (num_proc=8):  25%|##5       | 8/32 [00:00<00:00, 24.48 examples/s]\u001b[A\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 28.24 examples/s]\u001b[A\n",
      "Map (num_proc=8):  13%|#3        | 4/30 [00:00<00:00, 36.90 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 55.37 examples/s]\u001b[AINFO 2025-04-06 20:43:02,942 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 34.96 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 30/30 [00:00<00:00, 70.37 examples/s]\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 60.10 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/30 [00:00<?, ? examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 40.67 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 30/30 [00:00<00:00, 99.17 examples/s]\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 76.65 examples/s]INFO 2025-04-06 20:43:04,903 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 77.85 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 107.70 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 87.09 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 97.85 examples/s]\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "WARNING 2025-04-06 20:43:07,341 datasets.arrow_dataset:3098: num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "Map: 100%|##########| 1/1 [00:00<00:00, 142.81 examples/s]\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "WARNING 2025-04-06 20:43:07,352 datasets.arrow_dataset:3098: num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "Filter: 100%|##########| 1/1 [00:00<00:00, 317.53 examples/s]\n",
      "INFO 2025-04-06 20:43:07,372 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "INFO 2025-04-06 20:43:14,656 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 89.28 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 102.80 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 84.82 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 107.51 examples/s]\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 52.20 examples/s]INFO 2025-04-06 20:43:18,216 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 46.52 examples/s]\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 34.82 examples/s]\u001b[A\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 76.78 examples/s]\u001b[A\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 38.63 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 76.83 examples/s] \n",
      "Filter (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 47.86 examples/s]\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 52.23 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 45.69 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 27/27 [00:00<00:00, 47.37 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 53.38 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/27 [00:00<?, ? examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 37.79 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 27/27 [00:00<00:00, 87.84 examples/s]\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 22.57 examples/s]INFO 2025-04-06 20:43:22,698 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 81.56 examples/s] \n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 67.59 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 109.15 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 86.95 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 95.47 examples/s]\n",
      "num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "WARNING 2025-04-06 20:43:25,947 datasets.arrow_dataset:3098: num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "Map (num_proc=6): 100%|##########| 6/6 [00:00<00:00, 18.12 examples/s]\n",
      "num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "WARNING 2025-04-06 20:43:26,587 datasets.arrow_dataset:3098: num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "Filter (num_proc=6): 100%|##########| 6/6 [00:00<00:00, 19.86 examples/s]\n",
      "INFO 2025-04-06 20:43:27,230 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:43:27,240 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "INFO 2025-04-06 20:43:31,513 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 76.94 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 113.32 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 81.64 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 104.19 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 80.72 examples/s] \n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]INFO 2025-04-06 20:43:35,740 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 56.17 examples/s] \n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 43.38 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 45.83 examples/s]\n",
      "Map (num_proc=8):  16%|#6        | 4/25 [00:00<00:00, 36.42 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 34.42 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 25/25 [00:00<00:00, 59.67 examples/s] \n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 48.10 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 40.63 examples/s]\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 25/25 [00:00<00:00, 47.77 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]INFO 2025-04-06 20:43:39,026 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 31.57 examples/s]INFO 2025-04-06 20:43:39,133 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 69.90 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 104.42 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 77.16 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 108.12 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 81.20 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 105.87 examples/s]\n",
      "num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "WARNING 2025-04-06 20:43:43,312 datasets.arrow_dataset:3098: num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "Map (num_proc=6): 100%|##########| 6/6 [00:00<00:00, 18.83 examples/s]\n",
      "num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "WARNING 2025-04-06 20:43:43,951 datasets.arrow_dataset:3098: num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "Filter (num_proc=6): 100%|##########| 6/6 [00:00<00:00, 22.82 examples/s]\n",
      "INFO 2025-04-06 20:43:44,536 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "INFO 2025-04-06 20:44:04,710 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 79.31 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 114.20 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 82.05 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 104.51 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 85.13 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 98.96 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 69.59 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 105.25 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 81.07 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 96.18 examples/s] \n",
      "num_proc must be <= 7. Reducing num_proc to 7 for dataset of size 7.\n",
      "WARNING 2025-04-06 20:44:12,373 datasets.arrow_dataset:3098: num_proc must be <= 7. Reducing num_proc to 7 for dataset of size 7.\n",
      "Map (num_proc=7): 100%|##########| 7/7 [00:00<00:00, 19.27 examples/s]\n",
      "num_proc must be <= 7. Reducing num_proc to 7 for dataset of size 7.\n",
      "WARNING 2025-04-06 20:44:13,095 datasets.arrow_dataset:3098: num_proc must be <= 7. Reducing num_proc to 7 for dataset of size 7.\n",
      "Filter (num_proc=7): 100%|##########| 7/7 [00:00<00:00, 25.23 examples/s]\n",
      "INFO 2025-04-06 20:44:13,736 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:44:13,747 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "INFO 2025-04-06 20:44:15,873 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 79.14 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 109.40 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]INFO 2025-04-06 20:44:17,762 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:44:17,856 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:44:17,893 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 71.67 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 109.05 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 84.19 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 103.42 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 84.82 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 102.86 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 10/10 [00:00<00:00, 25.17 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/10 [00:00<?, ? examples/s]INFO 2025-04-06 20:44:23,174 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:44:23,224 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "Filter (num_proc=8):  20%|##        | 2/10 [00:00<00:00, 14.43 examples/s]INFO 2025-04-06 20:44:23,342 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "Filter (num_proc=8): 100%|##########| 10/10 [00:00<00:00, 28.68 examples/s]\n",
      "INFO 2025-04-06 20:44:23,578 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "INFO 2025-04-06 20:44:28,360 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 82.36 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 108.67 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 73.36 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 113.79 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 81.68 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 102.33 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 24/24 [00:00<00:00, 60.63 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 24/24 [00:00<00:00, 75.10 examples/s]\n",
      "INFO 2025-04-06 20:44:34,373 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "INFO 2025-04-06 20:44:34,382 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "INFO 2025-04-06 20:44:45,649 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:44:45,675 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:44:45,688 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:45:09,349 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 80.50 examples/s] \n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 39.78 examples/s]INFO 2025-04-06 20:45:10,762 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 86.63 examples/s]\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 42.10 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 51.00 examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 39.12 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 36.48 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 72.66 examples/s]\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 59.92 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 47.13 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  25%|##5       | 8/32 [00:00<00:00, 71.64 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 63.45 examples/s] \n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 23.55 examples/s]\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 25.18 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 52.32 examples/s]\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 81.90 examples/s]\u001b[AINFO 2025-04-06 20:45:14,651 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 33.76 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 57.84 examples/s] \n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 37.79 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:01<00:00, 22.57 examples/s]\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:02, 11.45 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 30.63 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 23.54 examples/s]\n",
      "\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 67.43 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 36.84 examples/s]\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 52.38 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 38.75 examples/s]\n",
      "INFO 2025-04-06 20:45:17,910 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:02, 10.35 examples/s]\u001b[A\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 21.91 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 28.05 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 48.91 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 25.07 examples/s]\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:02, 13.79 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 21.04 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 27.01 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 46.30 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 25.11 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 48.19 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 36.78 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 32.54 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 25.46 examples/s]\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 57.42 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 46.04 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 22.53 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 83.05 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 41.30 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[Anum_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n",
      "WARNING 2025-04-06 20:45:22,291 datasets.arrow_dataset:3098: num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n",
      "Filter (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 15.33 examples/s]\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 29.26 examples/s]\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 40.83 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 46.67 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 54.44 examples/s]\u001b[A\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 34.20 examples/s]\n",
      "\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 23.50 examples/s]\n",
      "Map (num_proc=5):   0%|          | 0/5 [00:00<?, ? examples/s]\n",
      "\n",
      "Map (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 21.58 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Map (num_proc=5):  20%|##        | 1/5 [00:00<00:00,  4.50 examples/s]\n",
      "\n",
      "Map (num_proc=5):  80%|########  | 4/5 [00:00<00:00, 13.69 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 51.34 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 25.10 examples/s]\n",
      "Map (num_proc=5): 100%|##########| 5/5 [00:00<00:00,  5.34 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:02,  9.39 examples/s]\u001b[A\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 26.21 examples/s]\u001b[Anum_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n",
      "WARNING 2025-04-06 20:45:24,668 datasets.arrow_dataset:3098: num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 16.03 examples/s]\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 49.02 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 25.00 examples/s]\n",
      "\n",
      "\n",
      "Filter (num_proc=5):  20%|##        | 1/5 [00:00<00:01,  2.65 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 25.68 examples/s]\n",
      "Filter (num_proc=5): 100%|##########| 5/5 [00:00<00:00,  5.73 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 47.91 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:02, 11.82 examples/s]\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 64.70 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 50.72 examples/s] \n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[AINFO 2025-04-06 20:45:27,552 instructlab.sdg.pipeline:202: Running block: duplicate_document_col\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 34.21 examples/s]\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 29.71 examples/s]\u001b[AINFO 2025-04-06 20:45:27,629 instructlab.sdg.pipeline:202: Running block: gen_spellcheck\n",
      "\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 61.22 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 33.46 examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:02, 13.67 examples/s]\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 20.91 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 43.75 examples/s]\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 50.71 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 28.00 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 31.42 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 56.04 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 41.01 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 26.30 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  25%|##5       | 8/32 [00:00<00:00, 32.09 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 47.34 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 37.08 examples/s]\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 128.00 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 49.18 examples/s] \n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 29.15 examples/s]\u001b[A\n",
      "Map (num_proc=8):  25%|##5       | 8/32 [00:00<00:01, 22.16 examples/s]\u001b[A\n",
      "Map (num_proc=8):   0%|          | 0/26 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):  15%|#5        | 4/26 [00:00<00:00, 33.34 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 25.44 examples/s]\n",
      "Map (num_proc=8):  65%|######5   | 17/26 [00:00<00:00, 26.94 examples/s]\n",
      "Map (num_proc=8):  88%|########8 | 23/26 [00:00<00:00, 31.94 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 26/26 [00:00<00:00, 28.12 examples/s]\n",
      "\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 87.74 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 39.34 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 54.89 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/26 [00:00<?, ? examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 38.40 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  42%|####2     | 11/26 [00:00<00:00, 40.20 examples/s]\u001b[A\n",
      "Filter (num_proc=8):  65%|######5   | 17/26 [00:00<00:00, 37.62 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 26/26 [00:00<00:00, 30.33 examples/s]\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 108.10 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 49.44 examples/s] \n",
      "\n",
      "Map (num_proc=8):  17%|#6        | 3/18 [00:00<00:00, 24.88 examples/s]\u001b[A\n",
      "Map (num_proc=8):  67%|######6   | 12/18 [00:00<00:00, 48.41 examples/s]\u001b[AINFO 2025-04-06 20:45:35,500 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "Map (num_proc=8): 100%|##########| 18/18 [00:00<00:00, 28.41 examples/s]\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 48.93 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 41.43 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 37.10 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 18/18 [00:00<00:00, 53.86 examples/s]\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 30.23 examples/s]INFO 2025-04-06 20:45:37,434 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_5d4010c455af45ac8aeb7396bff49227.jsonl\n",
      "\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 34.16ba/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 103.11 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 10/10 [00:00<00:00, 26.78 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 10/10 [00:00<00:00, 33.71 examples/s]\n",
      "INFO 2025-04-06 20:45:39,101 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_11bd242ec41144c9868f8893607cd792.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 57.82ba/s]\n",
      "INFO 2025-04-06 20:45:39,126 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_8ff38cce674e44699a5aec155bcb0149.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 97.80ba/s]\n",
      "INFO 2025-04-06 20:45:39,142 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_a1accacb4ff84390b90dcf0a54722912.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 121.77ba/s]\n",
      "INFO 2025-04-06 20:45:39,155 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_fde7ab0a97934e33a1cd3fedbc198af3.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 99.57ba/s]\n",
      "INFO 2025-04-06 20:45:39,170 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_0ce9af170628413e903017e1ed4c9aa2.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 98.67ba/s]\n",
      "INFO 2025-04-06 20:45:48,780 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:45:48,805 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:45:48,818 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:45:56,613 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 83.19 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 93.93 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 71.47 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 110.05 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]INFO 2025-04-06 20:46:00,176 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 71.08 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 107.22 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 79.54 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 104.14 examples/s]\n",
      "num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.\n",
      "WARNING 2025-04-06 20:46:02,804 datasets.arrow_dataset:3098: num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.\n",
      "Map (num_proc=2): 100%|##########| 2/2 [00:00<00:00,  8.06 examples/s]\n",
      "num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.\n",
      "WARNING 2025-04-06 20:46:03,205 datasets.arrow_dataset:3098: num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.\n",
      "Filter (num_proc=2): 100%|##########| 2/2 [00:00<00:00,  9.12 examples/s]\n",
      "INFO 2025-04-06 20:46:03,581 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "INFO 2025-04-06 20:46:10,605 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:46:14,666 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 83.97 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 107.74 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 81.67 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 110.73 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 79.50 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 99.08 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 29/29 [00:00<00:00, 74.07 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 29/29 [00:00<00:00, 94.18 examples/s]\n",
      "INFO 2025-04-06 20:46:20,688 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_911772448d9c4dc2bd5a9ad50f5d2909.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 52.70ba/s]\n",
      "INFO 2025-04-06 20:46:20,718 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_a50d68f9d3e549008e059f248a1b1680.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 102.86ba/s]\n",
      "INFO 2025-04-06 20:46:20,735 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_e1f16ab8d4774822a704ae7308e2574c.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 105.14ba/s]\n",
      "INFO 2025-04-06 20:46:38,966 instructlab.sdg.pipeline:202: Running block: flatten_auxiliary_columns\n",
      "INFO 2025-04-06 20:46:38,990 instructlab.sdg.pipeline:202: Running block: rename_to_document_column\n",
      "INFO 2025-04-06 20:46:39,000 instructlab.sdg.pipeline:202: Running block: gen_knowledge\n",
      "INFO 2025-04-06 20:47:14,882 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:47:28,545 instructlab.sdg.pipeline:202: Running block: eval_faithfulness_qa_pair\n",
      "INFO 2025-04-06 20:48:13,745 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 80.20 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 100.94 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 79.06 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 107.76 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 87.19 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 92.46 examples/s] \n",
      "Map (num_proc=8): 100%|##########| 24/24 [00:00<00:00, 61.29 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 24/24 [00:00<00:00, 80.74 examples/s]\n",
      "INFO 2025-04-06 20:48:19,793 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_41b27cc6e677459d96d681e90f44ea1c.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 79.00ba/s]\n",
      "INFO 2025-04-06 20:48:28,226 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 77.77 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 97.20 examples/s] \n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 76.25 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]INFO 2025-04-06 20:48:31,072 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 63.50 examples/s]\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 30.18 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 46.89 examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 37.15 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 33.43 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 76.89 examples/s] \n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 54.88 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 40.45 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 107.16 examples/s]\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 27.75 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 50.68 examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 31.53 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 33.16 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 77.45 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 62.95 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 41.97 examples/s]\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 59.95 examples/s] \n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 30.42 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 49.09 examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 33.33 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 33.35 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 71.47 examples/s]\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 47.73 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 38.08 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 105.94 examples/s]\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 27.57 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/17 [00:00<?, ? examples/s]\u001b[A\n",
      "Map (num_proc=8):  18%|#7        | 3/17 [00:00<00:00, 27.52 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 35.21 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 17/17 [00:00<00:00, 39.48 examples/s]\n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 65.82 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 55.17 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 42.27 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 17/17 [00:00<00:00, 49.54 examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 35.10 examples/s]INFO 2025-04-06 20:48:42,446 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 74.63 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 104.98 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 14/14 [00:00<00:00, 39.39 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 14/14 [00:00<00:00, 45.81 examples/s]\n",
      "INFO 2025-04-06 20:48:44,948 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "INFO 2025-04-06 20:48:48,277 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 83.03 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 96.14 examples/s] \n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 83.15 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 102.02 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 21/21 [00:00<00:00, 56.08 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 21/21 [00:00<00:00, 73.53 examples/s]\n",
      "INFO 2025-04-06 20:48:52,881 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "INFO 2025-04-06 20:49:02,711 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 84.11 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 93.98 examples/s] \n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 86.66 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 108.63 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 77.69 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 110.16 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 80.57 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 92.02 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 26/26 [00:00<00:00, 70.07 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 26/26 [00:00<00:00, 86.93 examples/s]\n",
      "INFO 2025-04-06 20:49:10,295 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "INFO 2025-04-06 20:49:29,696 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 79.99 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 105.84 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 80.48 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 105.35 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/18 [00:00<?, ? examples/s]INFO 2025-04-06 20:49:33,181 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Map (num_proc=8):  56%|#####5    | 10/18 [00:00<00:00, 15.62 examples/s]\n",
      "Map (num_proc=8):  89%|########8 | 16/18 [00:00<00:00, 25.24 examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 36.52 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 18/18 [00:00<00:00, 19.80 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 87.65 examples/s] \n",
      "Filter (num_proc=8):  33%|###3      | 6/18 [00:00<00:00, 17.93 examples/s]\n",
      "Filter (num_proc=8):  56%|#####5    | 10/18 [00:00<00:00, 17.34 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 18/18 [00:00<00:00, 22.07 examples/s]\n",
      "INFO 2025-04-06 20:49:35,330 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 94.40 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 78.26 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 96.66 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 80.14 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 107.63 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 28/28 [00:00<00:00, 76.05 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 28/28 [00:00<00:00, 88.29 examples/s]\n",
      "INFO 2025-04-06 20:49:40,103 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "INFO 2025-04-06 20:49:50,455 instructlab.sdg.pipeline:202: Running block: filter_faithfulness\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]INFO 2025-04-06 20:49:50,906 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 66.68 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 42.36 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 49.61 examples/s] \n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 104.19 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 47.77 examples/s] \n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 32.63 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 56.72 examples/s] \n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 41.93 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 34.38 examples/s]\n",
      "\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 56.29 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 43.64 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 61.29 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 100.42 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 50.82 examples/s] \n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 29.05 examples/s]\u001b[A\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 51.04 examples/s]\u001b[A\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 71.04 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 37.88 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 54.29 examples/s] \n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 43.66 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 50.96 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 37.41 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 128.00 examples/s]\u001b[AINFO 2025-04-06 20:49:58,151 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 75.93 examples/s] \n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 29.21 examples/s]\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:01<00:00, 24.65 examples/s]\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:02,  9.35 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 23.89 examples/s]\n",
      "\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 18.74 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 38.07 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 53.23 examples/s]\u001b[A\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 58.26 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 28.97 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 32.42 examples/s]\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 50.68 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 36.51 examples/s]\n",
      "\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 23.62 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 32.03 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 54.82 examples/s] \n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 36.21 examples/s]\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 29.81 examples/s]INFO 2025-04-06 20:50:03,410 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 35.72 examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 2/16 [00:00<00:01,  7.83 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 25.35 examples/s]\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):  75%|#######5  | 12/16 [00:00<00:00, 19.55 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 16/16 [00:01<00:00, 13.05 examples/s]\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:01, 17.97 examples/s]\n",
      "Map (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 34.95 examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 24.42 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 37.13 examples/s]\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 29.16 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 23.47 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 29.94 examples/s]\u001b[A\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 24.20 examples/s]\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 30.64 examples/s]\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 39.55 examples/s]\n",
      "\n",
      "\n",
      "Filter (num_proc=8):  62%|######2   | 10/16 [00:00<00:00, 40.26 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 16/16 [00:00<00:00, 22.37 examples/s]\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 80.51 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 46.21 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:02, 11.76 examples/s]\u001b[A\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Filter (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 22.71 examples/s]\u001b[Anum_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "WARNING 2025-04-06 20:50:08,004 datasets.arrow_dataset:3098: num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 16.80 examples/s]\n",
      "\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 29.73 examples/s]\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 25.71 examples/s]INFO 2025-04-06 20:50:08,502 instructlab.sdg.pipeline:202: Running block: eval_relevancy_qa_pair\n",
      "Map (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 53.73 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 35.21 examples/s]\n",
      "Map (num_proc=3): 100%|##########| 3/3 [00:00<00:00,  4.30 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "WARNING 2025-04-06 20:50:09,149 datasets.arrow_dataset:3098: num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 18.84 examples/s]\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 29.32 examples/s]\n",
      "Filter (num_proc=3): 100%|##########| 3/3 [00:00<00:00,  5.98 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:01<00:00, 28.91 examples/s]\n",
      "\n",
      "Filter (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 32.57 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 163.48 examples/s]\u001b[AINFO 2025-04-06 20:50:10,324 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 68.77 examples/s] \n",
      "Filter (num_proc=8):  75%|#######5  | 24/32 [00:00<00:00, 105.83 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 48.50 examples/s] \n",
      "\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:00, 37.42 examples/s]\u001b[A\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 57.83 examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 43.22 examples/s]\n",
      "Map (num_proc=8):  76%|#######6  | 13/17 [00:00<00:00, 27.79 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 17/17 [00:00<00:00, 21.88 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 17/17 [00:00<00:00, 19.48 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 99.35 examples/s]\n",
      "Filter (num_proc=8):  88%|########8 | 15/17 [00:00<00:00, 25.75 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/21 [00:00<?, ? examples/s]\u001b[A\n",
      "Map (num_proc=8):  14%|#4        | 3/21 [00:00<00:00, 28.63 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 17/17 [00:00<00:00, 22.00 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 21/21 [00:00<00:00, 57.34 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 21/21 [00:00<00:00, 69.75 examples/s]\n",
      "INFO 2025-04-06 20:50:14,983 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "INFO 2025-04-06 20:50:21,866 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 88.00 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 105.72 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 78.08 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 89.89 examples/s] \n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 83.49 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 104.49 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 22/22 [00:00<00:00, 60.12 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 22/22 [00:00<00:00, 71.44 examples/s]\n",
      "INFO 2025-04-06 20:50:27,942 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_ac760cc8f36048daa7f470a253b7f61e.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 76.63ba/s]\n",
      "INFO 2025-04-06 20:50:38,069 instructlab.sdg.pipeline:202: Running block: filter_relevancy\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 75.51 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 105.36 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 89.45 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 108.52 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 84.50 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 108.98 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 83.04 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 104.86 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 12/12 [00:00<00:00, 30.09 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 12/12 [00:00<00:00, 37.82 examples/s]\n",
      "INFO 2025-04-06 20:50:45,608 instructlab.sdg.pipeline:202: Running block: eval_verify_question\n",
      "INFO 2025-04-06 20:50:57,787 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 83.13 examples/s] \n",
      "INFO 2025-04-06 20:50:58,566 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:01, 19.52 examples/s]\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 32.26 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 32.53 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 96.22 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):  12%|#2        | 4/32 [00:00<00:01, 23.29 examples/s]\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 45.53 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 71.04 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 59.77 examples/s]\n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 22.98 examples/s]\n",
      "Map (num_proc=8):  62%|######2   | 20/32 [00:00<00:00, 35.64 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 36.04 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 102.46 examples/s]\n",
      "Filter (num_proc=8):  88%|########7 | 28/32 [00:00<00:00, 48.90 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 41.45 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 55.30 examples/s] \n",
      "Map (num_proc=8):  38%|###7      | 12/32 [00:00<00:00, 22.25 examples/s]\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 26.71 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 104.08 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 33.10 examples/s]\n",
      "Map (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 24.75 examples/s]\n",
      "Filter (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 34.84 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 104.51 examples/s]\n",
      "Filter (num_proc=8):  50%|#####     | 16/32 [00:00<00:00, 38.36 examples/s]\n",
      "Map (num_proc=8):   0%|          | 0/32 [00:00<?, ? examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 42.70 examples/s]\n",
      "\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 53.78 examples/s]\n",
      "Map (num_proc=8):  65%|######5   | 17/26 [00:00<00:00, 40.93 examples/s]\n",
      "Map (num_proc=8):  88%|########8 | 23/26 [00:00<00:00, 41.00 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 26/26 [00:00<00:00, 31.61 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 97.88 examples/s]\n",
      "Filter (num_proc=8):  77%|#######6  | 20/26 [00:00<00:00, 33.40 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 26/26 [00:00<00:00, 39.38 examples/s]\n",
      "Map (num_proc=8):  17%|#6        | 2/12 [00:00<00:00, 18.21 examples/s]\u001b[A\n",
      "Filter (num_proc=8): 100%|##########| 26/26 [00:00<00:00, 31.06 examples/s]\n",
      "INFO 2025-04-06 20:51:10,579 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_d43901ed37b145ec820a3211bc7f84fe.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 30.94ba/s]\n",
      "Map (num_proc=8): 100%|##########| 12/12 [00:00<00:00, 29.14 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 12/12 [00:00<00:00, 38.69 examples/s]\n",
      "INFO 2025-04-06 20:51:11,485 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_6ffce0ef62704d3d890db63d8f6fe8bb.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 69.58ba/s]\n",
      "INFO 2025-04-06 20:51:14,067 instructlab.sdg.pipeline:202: Running block: filter_verify_question\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 86.87 examples/s] \n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 102.73 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 82.53 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 104.90 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 83.30 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 112.09 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 82.66 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 32/32 [00:00<00:00, 104.61 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 8/8 [00:00<00:00, 22.39 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 8/8 [00:00<00:00, 27.19 examples/s]\n",
      "INFO 2025-04-06 20:51:21,450 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_20f0d4d556aa4289ac1fb206db5b27e0.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 72.01ba/s]\n",
      "INFO 2025-04-06 20:51:21,472 instructlab.sdg.checkpointing:44: Saving checkpoint to /root/.local/share/instructlab/datasets/checkpoints/knowledge_information_computer_science_cybersecurity/data_checkpoint_b98025e1d12244c58b9851503a479a80.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 115.77ba/s]\n",
      "INFO 2025-04-06 20:51:22,685 instructlab.sdg.generate_data:474: Generated 3257 samples\n",
      "INFO 2025-04-06 20:51:22,778 instructlab.sdg.pipeline:164: Running pipeline with multi-threaded batching. Using 10 workers for batches of size 32\n",
      "INFO 2025-04-06 20:51:22,821 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:51:22,825 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:51:22,828 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:51:22,832 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:51:22,839 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:51:22,843 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:51:22,847 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:51:22,855 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:51:22,885 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:51:22,910 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:52:09,311 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:52:09,325 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:52:31,652 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:52:41,331 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:52:43,079 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:52:44,206 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:53:03,653 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:53:09,310 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:53:11,752 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:53:15,123 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:53:21,618 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:53:24,822 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:53:26,902 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:53:36,378 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "INFO 2025-04-06 20:53:47,677 instructlab.sdg.pipeline:202: Running block: gen_mmlu_knowledge\n",
      "Filter: 100%|##########| 229/229 [00:00<00:00, 25861.49 examples/s]\n",
      "Filter: 100%|##########| 209/209 [00:00<00:00, 15624.45 examples/s]\n",
      "Flattening the indices: 100%|##########| 209/209 [00:00<00:00, 22733.65 examples/s]\n",
      "Map: 100%|##########| 209/209 [00:00<00:00, 6522.39 examples/s]\n",
      "Map: 100%|##########| 209/209 [00:00<00:00, 6031.07 examples/s]\n",
      "Map: 100%|##########| 209/209 [00:00<00:00, 6173.26 examples/s]\n",
      "Filter: 100%|##########| 209/209 [00:00<00:00, 23455.06 examples/s]\n",
      "Filter: 100%|##########| 209/209 [00:00<00:00, 12995.47 examples/s]\n",
      "Filter: 100%|##########| 98/98 [00:00<00:00, 11233.72 examples/s]\n",
      "Flattening the indices: 100%|##########| 91/91 [00:00<00:00, 12319.46 examples/s]\n",
      "Casting to class labels: 100%|##########| 91/91 [00:00<00:00, 5715.34 examples/s]\n",
      "INFO 2025-04-06 20:54:49,155 instructlab.sdg.eval_data:126: Saving MMLU Dataset /root/.local/share/instructlab/datasets/2025-04-06_201731/node_datasets_2025-04-06T20_18_53/mmlubench_knowledge_information_computer_science_cybersecurity.jsonl\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 199.55ba/s]\n",
      "INFO 2025-04-06 20:54:49,161 instructlab.sdg.eval_data:130: Saving MMLU Task yaml /root/.local/share/instructlab/datasets/2025-04-06_201731/node_datasets_2025-04-06T20_18_53/knowledge_information_computer_science_cybersecurity_task.yaml\n",
      "Map: 100%|##########| 3257/3257 [00:00<00:00, 5024.97 examples/s]\n",
      "Map: 100%|##########| 3257/3257 [00:00<00:00, 12227.54 examples/s]\n",
      "Filter: 100%|##########| 3257/3257 [00:00<00:00, 42598.30 examples/s]\n",
      "Map: 100%|##########| 49/49 [00:00<00:00, 5698.62 examples/s]\n",
      "Map: 100%|##########| 49/49 [00:00<00:00, 6933.44 examples/s]\n",
      "Creating json from Arrow format: 100%|##########| 4/4 [00:00<00:00, 19.46ba/s]\n",
      "Map: 100%|##########| 3257/3257 [00:00<00:00, 4916.58 examples/s]\n",
      "Map: 100%|##########| 3257/3257 [00:00<00:00, 4542.49 examples/s]\n",
      "Map: 100%|##########| 3257/3257 [00:00<00:00, 5096.08 examples/s]\n",
      "Map: 100%|##########| 3257/3257 [00:00<00:00, 12277.68 examples/s]\n",
      "Filter: 100%|##########| 3257/3257 [00:00<00:00, 42327.19 examples/s]\n",
      "Map: 100%|##########| 49/49 [00:00<00:00, 5235.40 examples/s]\n",
      "Creating json from Arrow format: 100%|##########| 7/7 [00:00<00:00, 10.57ba/s]\n",
      "INFO 2025-04-06 20:54:54,522 instructlab.sdg.datamixing:138: Loading dataset from /root/.local/share/instructlab/datasets/2025-04-06_201731/node_datasets_2025-04-06T20_18_53/knowledge_information_computer_science_cybersecurity_p10.jsonl ...\n",
      "Generating train split: 6563 examples [00:00, 46047.23 examples/s]\n",
      "INFO 2025-04-06 20:54:54,881 instructlab.sdg.datamixing:140: Dataset columns: ['messages', 'metadata', 'id']\n",
      "INFO 2025-04-06 20:54:54,882 instructlab.sdg.datamixing:141: Dataset loaded with 6563 samples\n",
      "Map (num_proc=8): 100%|##########| 6563/6563 [00:00<00:00, 14627.02 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 6563/6563 [00:00<00:00, 15631.78 examples/s]\n",
      "Creating json from Arrow format: 100%|##########| 7/7 [00:00<00:00, 10.02ba/s]\n",
      "INFO 2025-04-06 20:54:57,321 instructlab.sdg.datamixing:215: Mixed Dataset saved to /root/.local/share/instructlab/datasets/2025-04-06_201731/skills_train_msgs_2025-04-06T20_18_53.jsonl\n",
      "INFO 2025-04-06 20:54:57,325 instructlab.sdg.datamixing:138: Loading dataset from /root/.local/share/instructlab/datasets/2025-04-06_201731/node_datasets_2025-04-06T20_18_53/knowledge_information_computer_science_cybersecurity_p07.jsonl ...\n",
      "Generating train split: 3306 examples [00:00, 67728.05 examples/s]\n",
      "INFO 2025-04-06 20:54:57,487 instructlab.sdg.datamixing:140: Dataset columns: ['messages', 'metadata', 'id']\n",
      "INFO 2025-04-06 20:54:57,487 instructlab.sdg.datamixing:141: Dataset loaded with 3306 samples\n",
      "Map (num_proc=8): 100%|##########| 3306/3306 [00:00<00:00, 10122.01 examples/s]\n",
      "Map (num_proc=8): 100%|##########| 3306/3306 [00:00<00:00, 10974.13 examples/s]\n",
      "Creating json from Arrow format: 100%|##########| 4/4 [00:00<00:00, 19.49ba/s]\n",
      "INFO 2025-04-06 20:54:59,178 instructlab.sdg.datamixing:215: Mixed Dataset saved to /root/.local/share/instructlab/datasets/2025-04-06_201731/knowledge_train_msgs_2025-04-06T20_18_53.jsonl\n",
      "INFO 2025-04-06 20:54:59,179 instructlab.sdg.generate_data:732: Generation took 2165.72s\n",
      "INFO 2025-04-06 20:55:03,002 instructlab.model.backends.vllm:512: Waiting for GPU VRAM reclamation...\n",
      "ᕦ(òᴗóˇ)ᕤ Data generate completed successfully! ᕦ(òᴗóˇ)ᕤ\n",
      "Renaming train_2025-04-06T20_18_53.jsonl to train_gen.jsonl\n",
      "Renaming test_2025-04-06T20_18_53.jsonl to test_gen.jsonl\n",
      "Training and test files successfully created in: data/cybersecurity/ilab_generated/\n"
     ]
    }
   ],
   "source": [
    "gen_directory = \"data/\"+ use_case+\"/ilab_generated/\"\n",
    "if instr.value == \"Default (>450)\":\n",
    "        sdg_factor=\"\"\n",
    "elif instr.value == '>15':\n",
    "    sdg_factor=\"--sdg-scale-factor 1\"\n",
    "elif instr.value == '>50':\n",
    "    sdg_factor=\"--sdg-scale-factor 3\"\n",
    "elif instr.value == '>200':\n",
    "    sdg_factor=\"--sdg-scale-factor 13\"\n",
    "elif instr.value == '>500':\n",
    "    sdg_factor=\"--sdg-scale-factor 33\"\n",
    "else:\n",
    "    sdg_factor=\"--sdg-scale-factor 67\"\n",
    "\n",
    "sdg_pipe.value = 'Full with GPU'\n",
    "\n",
    "if sdg_pipe.value == 'Full with GPU':\n",
    "    pipeline = 'full'\n",
    "    model = ''\n",
    "#   model = '--model models/instructlab/granite-7b-lab'\n",
    "    gpus = '--gpus 1'\n",
    "else:\n",
    "    print(\"ERROR: Undefined pipeline\")\n",
    "\n",
    "il_data_path= '/root/.local/share/instructlab/datasets/'\n",
    "#Remove old data so there is only one test_merlinite and train_merlinite after generation\n",
    "print(\"Remove old datasets\")\n",
    "!rm -rf {il_data_path}*\n",
    "#shell_command = f\"ilab --verbose data generate {model} --num-cpus 10 {gpus} {sdg_factor} --taxonomy-path taxonomy --pipeline {pipeline} --max-num-tokens 512\"\n",
    "shell_command = f\"ilab data generate --taxonomy-path taxonomy --pipeline full --max-num-tokens 512\"\n",
    "\n",
    "print(\"Generating data\")\n",
    "print(\"Running: !\"+shell_command)\n",
    "!{shell_command}\n",
    "\n",
    "#Rename results to  test_gen.jsonl and train_gen.jsonl and move to local data directory\n",
    "if not os.path.exists(gen_directory):\n",
    "    print(\"Create directory: \" + gen_directory)\n",
    "    !mkdir {gen_directory}\n",
    "file_cnt=0\n",
    "try:\n",
    "    for dirname in os.listdir(il_data_path):\n",
    "        date_path=il_data_path+'/'+ dirname + '/'\n",
    "        for filename in os.listdir(date_path):\n",
    "            if filename[:6]=='train_':\n",
    "                train_name= 'train_gen.jsonl'\n",
    "                print('Renaming '+ filename+ ' to ' + train_name)\n",
    "                !mv {date_path+filename} {gen_directory+train_name}\n",
    "                file_cnt+=1\n",
    "            elif filename[:5]=='test_':\n",
    "                test_name= 'test_gen.jsonl'\n",
    "                print('Renaming '+ filename+ ' to ' + test_name)\n",
    "                !mv {date_path+filename} {gen_directory+test_name}\n",
    "                file_cnt+=1\n",
    "    if file_cnt < 2:\n",
    "        print(\"ERROR: train_gen.jsonl and/or test.jsonl not created\")\n",
    "    elif os.path.getsize(gen_directory+train_name) == 0:\n",
    "        print(\"ERROR: train_gen.jsonl file is empty\")\n",
    "    elif os.path.getsize(gen_directory+test_name) == 0:\n",
    "        print(\"ERROR: test_gen.jsonl file is empty\")\n",
    "    else:\n",
    "        print(\"Training and test files successfully created in: \" + gen_directory)\n",
    "except:\n",
    "    print(\"Error running ilab generate, no synthetic data generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1743973264942,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "qZdLxDKoZKc0",
    "outputId": "ad555f05-2c30-4e19-dca9-00e372a35c3b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file: data/cybersecurity/ilab_generated/test_gen.jsonl to /content/drive/MyDrive/ilab_data/test_gen.jsonl\n",
      "Copying file: data/cybersecurity/ilab_generated/train_gen.jsonl to /content/drive/MyDrive/ilab_data/train_gen.jsonl\n",
      "Data successfully copied to /content/drive/MyDrive/ilab_data\n"
     ]
    }
   ],
   "source": [
    "# prompt: I need to move all generated data in previous code block to my mounted google drive\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define source and destination directories\n",
    "source_dir = \"data/cybersecurity/ilab_generated/\"  # Example source directory\n",
    "destination_dir = \"/content/drive/MyDrive/ilab_data\"  # Your desired Google Drive destination\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through files and directories in source directory\n",
    "for item in os.listdir(source_dir):\n",
    "    source_item_path = os.path.join(source_dir, item)\n",
    "    destination_item_path = os.path.join(destination_dir, item)\n",
    "\n",
    "    # Check if it's a file\n",
    "    if os.path.isfile(source_item_path):\n",
    "        print(f\"Copying file: {source_item_path} to {destination_item_path}\")\n",
    "        shutil.copy2(source_item_path, destination_item_path)  # copy2 preserves metadata\n",
    "\n",
    "    # Check if it's a directory (recursively copy subdirectories)\n",
    "    elif os.path.isdir(source_item_path):\n",
    "        print(f\"Copying directory: {source_item_path} to {destination_item_path}\")\n",
    "        shutil.copytree(source_item_path, destination_item_path, dirs_exist_ok=True)\n",
    "\n",
    "print(f\"Data successfully copied to {destination_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0B_39Sqvxgj"
   },
   "source": [
    "### 2.5 Show examples of generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "executionInfo": {
     "elapsed": 80,
     "status": "error",
     "timestamp": 1744451686404,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "hLqnVW7rvxgj",
    "outputId": "02fba79a-febb-407e-938b-5e9d573855c7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.3 Show examples of generated data\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'user'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8a00815443d9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mjsonLine\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0msyn_user\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjsonLine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0msyn_assist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjsonLine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;31m#Remove \"Answer:\" and \"Response:\" from answers for displaying\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'user'"
     ]
    }
   ],
   "source": [
    "print(\"2.4.3 Show examples of generated data\")\n",
    "gen_directory = \"data/\"+ use_case+\"/ilab_generated/\"\n",
    "for filename in os.listdir(gen_directory):\n",
    "    if filename[:9]=='train_gen':\n",
    "        with open(gen_directory+filename, 'r') as syn_file:\n",
    "            cnt=0\n",
    "            for line_number, line in enumerate(syn_file):\n",
    "                if cnt >= 8:\n",
    "                    break\n",
    "                jsonLine= json.loads(line)\n",
    "                syn_user=jsonLine[\"user\"]\n",
    "                syn_assist=jsonLine[\"assistant\"]\n",
    "                #Remove \"Answer:\" and \"Response:\" from answers for displaying\n",
    "                if syn_user[:10]==\"Question: \":\n",
    "                    syn_user=syn_user[10:]\n",
    "                if syn_assist[:8]==\"Answer: \":\n",
    "                    syn_assist=syn_assist[8:]\n",
    "                cnt+=1\n",
    "                print(\"\\nQuestion: \"+syn_user+\"\\nAnswer: \"+syn_assist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvTYe8oHdm13",
    "tags": []
   },
   "source": [
    "## Section 3. Training with InstructLab\n",
    "\n",
    "### 3.1 Select the model training pipeline\n",
    "\n",
    "InstructLab has three primary model training pipelines: simple, full (default), and accelerated. For all of the models, the training time can be limited by adjusting the num_epoch paramater. The maximum number of epochs for running the InstructLab end-to-end workflow is 10.\n",
    "\n",
    "#### **Simple pipeline**\n",
    "\n",
    "The simple pipeline uses an SFT Trainer on Linux and MLX on MacOS. This type of training takes roughly an hour and produces the lowest fidelity model but should indicate if your data is being picked up by the training process. The simple pipeline only works with Merlinite 7b Lab as the teacher model. For this Linux system, the trained model is saved in the models directory as ggml-model-f16.gguf.\n",
    "\n",
    "The command form is:\n",
    "\n",
    "    ilab model train --pipeline simple\n",
    "\n",
    "**Note:** This process will take a little while to complete (time can vary based on hardware and output of ilab data generate but on the order of 5 to 15 minutes)\n",
    "\n",
    "#### **Accelerated pipeline**\n",
    "\n",
    "The accelerated uses the instructlab-training library which supports GPU accelerated and distributed training. The full loop and data processing functions are either pulled directly from or based off of the work in this library. For the accelerated pipeline, the models are saved in the ~/.local/share/instructlab/checkpoints directory. The instructlab command \"ilab model evaluate\" can be used to choose the best one. Training is support for GPU acceleration with Nvidia CUDA or AMD ROCm. Please see the GPU acceleration documentation for more details. At present, hardware acceleration requires a data center GPU or high-end consumer GPU with at least 18 GB free memory.\n",
    "\n",
    "The command form is:\n",
    "\n",
    "    ilab model train --pipeline accelerated --device cuda --data-path <path-to-sdg-data>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147,
     "referenced_widgets": [
      "3742b3381d4a4f2f97ec523a3f504315",
      "27f83a1a921649999712e0e395240ce1",
      "78f1c61cc82b4338a45f21c434bcdb0c",
      "efdae66b8ff84194b5b3f42869d5729d",
      "cb9a8f915633416ea6afb8041d6c9325",
      "93c8387fae0e4c929296b9e37ee26147",
      "01c1d7674c674814965f15c257155ea0",
      "5c2e9fc3b8614403b7b5eafd28ed0d49",
      "e745a9162f7441acbb580f00144517d5"
     ]
    },
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1744451531510,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "8UrXh9-nvxgj",
    "outputId": "b1ed9730-7d42-4004-f4f6-20fb87359e26"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select to Continue or to Train the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3742b3381d4a4f2f97ec523a3f504315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Processing', options=('Simple with GPU', 'Accelerated GPU'), style=ToggleButtonsSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdae66b8ff84194b5b3f42869d5729d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Epochs:', index=5, options=('1', '2', '3', '4', '5', '10', '15'), style=ToggleButto…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c1d7674c674814965f15c257155ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Iterations:', index=4, options=('1', '3', '5', '10', '20', '50', '100', '200'), sty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After choosing your training options, please select and run the following cell\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Select to Continue or to Train the model\")\n",
    "display(train_pipe)\n",
    "display(epoch)\n",
    "display(it)\n",
    "print(\"After choosing your training options, please select and run the following cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUPA7SpTvxgj"
   },
   "source": [
    "### 3.2 Run the model training\n",
    "\n",
    "Model training can take 30 minutes or more for 1 epoch and 1 iteration and takes 1 hour for the default paramter values. This minimal training could be used for testing the generation and training for a new set of data.\n",
    "\n",
    "To produce a higher quality model, more epochs and iterations are needed for refining the model. This will require a proportionally longer time to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1744093474462,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "04PA1HfjBVfA",
    "outputId": "1a7dc355-2fd1-477c-97b4-bf92fd498504"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##to copy config.yaml to local directory\n",
    "!cp ./config.yaml /root/.config/instructlab/config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1317197,
     "status": "ok",
     "timestamp": 1744453009978,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "LcIjCQUjvxgj",
    "outputId": "b4535388-3c01-4874-8f11-7042d48de9c2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "\n",
      "Epoch 7:  44%|████▍     | 46/104 [00:42<00:53,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 774,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.303291566761985,\n",
      "    \"lr\": 3.355993196827075e-06,\n",
      "    \"cuda_mem_allocated\": 19.11925220489502,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1774,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.04255918827508456,\n",
      "    \"samples_seen\": 24256,\n",
      "    \"gradnorm\": 5.78125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:28.370172\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4782 num samples 30 - rank: 0 num_loss_counted_tokens: 1812\u001b[0m\n",
      "Epoch: 7, Step: 775, Rank: 0, loss = 0.064453125\n",
      "\n",
      "Epoch 7:  45%|████▌     | 47/104 [00:43<00:51,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 775,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.31844761274511,\n",
      "    \"lr\": 3.355993196827075e-06,\n",
      "    \"cuda_mem_allocated\": 25.091779232025146,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1687,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.06431535269709543,\n",
      "    \"samples_seen\": 24286,\n",
      "    \"gradnorm\": 5.78125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:29.225279\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4833 num samples 31 - rank: 0 num_loss_counted_tokens: 1717\u001b[0m\n",
      "Epoch: 7, Step: 776, Rank: 0, loss = 0.06689453125\n",
      "\n",
      "Epoch 7:  46%|████▌     | 48/104 [00:44<00:51,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 776,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.59022155976574,\n",
      "    \"lr\": 3.308693936411421e-06,\n",
      "    \"cuda_mem_allocated\": 19.11039400100708,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1837,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.06668481219379423,\n",
      "    \"samples_seen\": 24318,\n",
      "    \"gradnorm\": 7.59375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:30.182832\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4855 num samples 32 - rank: 0 num_loss_counted_tokens: 1696\u001b[0m\n",
      "Epoch: 7, Step: 777, Rank: 0, loss = 0.061279296875\n",
      "\n",
      "Epoch 7:  47%|████▋     | 49/104 [00:44<00:49,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 777,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.494939960701345,\n",
      "    \"lr\": 3.308693936411421e-06,\n",
      "    \"cuda_mem_allocated\": 25.10829782485962,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1877,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.06126798082045818,\n",
      "    \"samples_seen\": 24350,\n",
      "    \"gradnorm\": 7.59375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:31.056070\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4791 num samples 31 - rank: 0 num_loss_counted_tokens: 1681\u001b[0m\n",
      "Epoch: 7, Step: 778, Rank: 0, loss = 0.0595703125\n",
      "\n",
      "Epoch 7:  48%|████▊     | 50/104 [00:45<00:49,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 778,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.12148156547882,\n",
      "    \"lr\": 3.2616642008283218e-06,\n",
      "    \"cuda_mem_allocated\": 19.10081672668457,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1622,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.0594944512946979,\n",
      "    \"samples_seen\": 24383,\n",
      "    \"gradnorm\": 6.4375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:32.025014\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4845 num samples 32 - rank: 0 num_loss_counted_tokens: 1823\u001b[0m\n",
      "Epoch: 7, Step: 779, Rank: 0, loss = 0.0732421875\n",
      "\n",
      "Epoch 7:  49%|████▉     | 51/104 [00:46<00:48,  1.10it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 779,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.25870299034081,\n",
      "    \"lr\": 3.2616642008283218e-06,\n",
      "    \"cuda_mem_allocated\": 25.09943962097168,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1702,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.07344300822561692,\n",
      "    \"samples_seen\": 24415,\n",
      "    \"gradnorm\": 6.4375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:32.903069\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4669 num samples 29 - rank: 0 num_loss_counted_tokens: 1685\u001b[0m\n",
      "Epoch: 7, Step: 780, Rank: 0, loss = 0.048095703125\n",
      "\n",
      "Epoch 7:  50%|█████     | 52/104 [00:47<00:48,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 780,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 36.78931660947077,\n",
      "    \"lr\": 3.2149058844286796e-06,\n",
      "    \"cuda_mem_allocated\": 19.12236452102661,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1675,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.04805970149253731,\n",
      "    \"samples_seen\": 24448,\n",
      "    \"gradnorm\": 6.4375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:33.880186\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4858 num samples 30 - rank: 0 num_loss_counted_tokens: 1578\u001b[0m\n",
      "Epoch: 7, Step: 781, Rank: 0, loss = 0.053466796875\n",
      "\n",
      "Epoch 7:  51%|█████     | 53/104 [00:48<00:46,  1.10it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 781,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.173669664817496,\n",
      "    \"lr\": 3.2149058844286796e-06,\n",
      "    \"cuda_mem_allocated\": 25.083159923553467,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1804,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.05349223946784922,\n",
      "    \"samples_seen\": 24478,\n",
      "    \"gradnorm\": 6.4375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:34.739042\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4846 num samples 31 - rank: 0 num_loss_counted_tokens: 1763\u001b[0m\n",
      "Epoch: 7, Step: 782, Rank: 0, loss = 0.047607421875\n",
      "\n",
      "Epoch 7:  52%|█████▏    | 54/104 [00:49<00:46,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 782,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.30273171678015,\n",
      "    \"lr\": 3.1684208706306572e-06,\n",
      "    \"cuda_mem_allocated\": 19.120449542999268,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1877,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.04768247202983484,\n",
      "    \"samples_seen\": 24509,\n",
      "    \"gradnorm\": 6.375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:35.703309\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4747 num samples 32 - rank: 0 num_loss_counted_tokens: 1619\u001b[0m\n",
      "Epoch: 7, Step: 783, Rank: 0, loss = 0.06396484375\n",
      "\n",
      "Epoch 7:  53%|█████▎    | 55/104 [00:50<00:44,  1.10it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 783,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.97164679046128,\n",
      "    \"lr\": 3.1684208706306572e-06,\n",
      "    \"cuda_mem_allocated\": 25.107580184936523,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1607,\n",
      "    \"batch_size\": 34,\n",
      "    \"total_loss\": 0.06378344741754823,\n",
      "    \"samples_seen\": 24543,\n",
      "    \"gradnorm\": 6.375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:36.565289\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4803 num samples 32 - rank: 0 num_loss_counted_tokens: 1738\u001b[0m\n",
      "Epoch: 7, Step: 784, Rank: 0, loss = 0.04052734375\n",
      "\n",
      "Epoch 7:  54%|█████▍    | 56/104 [00:51<00:44,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 784,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.34365536637022,\n",
      "    \"lr\": 3.12221103184383e-06,\n",
      "    \"cuda_mem_allocated\": 19.114941596984863,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1619,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.04045707226683138,\n",
      "    \"samples_seen\": 24574,\n",
      "    \"gradnorm\": 6.65625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:37.528683\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4783 num samples 32 - rank: 0 num_loss_counted_tokens: 1603\u001b[0m\n",
      "Epoch: 7, Step: 785, Rank: 0, loss = 0.045654296875\n",
      "\n",
      "Epoch 7:  55%|█████▍    | 57/104 [00:52<00:42,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 785,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.14853213853716,\n",
      "    \"lr\": 3.12221103184383e-06,\n",
      "    \"cuda_mem_allocated\": 25.110453128814697,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1620,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.04567901234567901,\n",
      "    \"samples_seen\": 24607,\n",
      "    \"gradnorm\": 6.65625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:38.387788\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4852 num samples 31 - rank: 0 num_loss_counted_tokens: 1980\u001b[0m\n",
      "Epoch: 7, Step: 786, Rank: 0, loss = 0.08056640625\n",
      "\n",
      "Epoch 7:  56%|█████▌    | 58/104 [00:53<00:42,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 786,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.1990766313396,\n",
      "    \"lr\": 3.076278229393773e-06,\n",
      "    \"cuda_mem_allocated\": 19.125475883483887,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1746,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.08075601374570447,\n",
      "    \"samples_seen\": 24640,\n",
      "    \"gradnorm\": 7.15625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:39.354683\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4743 num samples 32 - rank: 0 num_loss_counted_tokens: 1785\u001b[0m\n",
      "Epoch: 7, Step: 787, Rank: 0, loss = 0.06982421875\n",
      "\n",
      "Epoch 7:  57%|█████▋    | 59/104 [00:54<00:40,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 787,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.46727045770988,\n",
      "    \"lr\": 3.076278229393773e-06,\n",
      "    \"cuda_mem_allocated\": 25.08028745651245,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1703,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.06958308866705813,\n",
      "    \"samples_seen\": 24672,\n",
      "    \"gradnorm\": 7.15625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:40.208181\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4838 num samples 31 - rank: 0 num_loss_counted_tokens: 1948\u001b[0m\n",
      "Epoch: 7, Step: 788, Rank: 0, loss = 0.042236328125\n",
      "\n",
      "Epoch 7:  58%|█████▊    | 60/104 [00:55<00:40,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 788,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.44211201412235,\n",
      "    \"lr\": 3.0306243134470668e-06,\n",
      "    \"cuda_mem_allocated\": 19.108238697052002,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1919,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.042209484106305366,\n",
      "    \"samples_seen\": 24702,\n",
      "    \"gradnorm\": 6.46875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:41.170716\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4815 num samples 32 - rank: 0 num_loss_counted_tokens: 1870\u001b[0m\n",
      "Epoch: 7, Step: 789, Rank: 0, loss = 0.045166015625\n",
      "\n",
      "Epoch 7:  59%|█████▊    | 61/104 [00:55<00:38,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 789,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.96332009568267,\n",
      "    \"lr\": 3.0306243134470668e-06,\n",
      "    \"cuda_mem_allocated\": 25.109495162963867,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1912,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.045240585774058574,\n",
      "    \"samples_seen\": 24733,\n",
      "    \"gradnorm\": 6.46875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:42.033762\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4728 num samples 32 - rank: 0 num_loss_counted_tokens: 1738\u001b[0m\n",
      "Epoch: 7, Step: 790, Rank: 0, loss = 0.054443359375\n",
      "\n",
      "Epoch 7:  60%|█████▉    | 62/104 [00:56<00:38,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 790,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.320997712652925,\n",
      "    \"lr\": 2.9852511229367862e-06,\n",
      "    \"cuda_mem_allocated\": 19.116856575012207,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1783,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.05440269209197981,\n",
      "    \"samples_seen\": 24766,\n",
      "    \"gradnorm\": 6.4375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:42.999009\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4842 num samples 32 - rank: 0 num_loss_counted_tokens: 1576\u001b[0m\n",
      "Epoch: 7, Step: 791, Rank: 0, loss = 0.042236328125\n",
      "\n",
      "Epoch 7:  61%|██████    | 63/104 [00:57<00:37,  1.10it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 791,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.75000339234278,\n",
      "    \"lr\": 2.9852511229367862e-06,\n",
      "    \"cuda_mem_allocated\": 25.097763538360596,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1812,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.042218543046357616,\n",
      "    \"samples_seen\": 24796,\n",
      "    \"gradnorm\": 6.4375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:43.866809\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4756 num samples 32 - rank: 0 num_loss_counted_tokens: 1639\u001b[0m\n",
      "Epoch: 7, Step: 792, Rank: 0, loss = 0.04638671875\n",
      "\n",
      "Epoch 7:  62%|██████▏   | 64/104 [00:58<00:36,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 792,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.15171147949743,\n",
      "    \"lr\": 2.940160485488436e-06,\n",
      "    \"cuda_mem_allocated\": 19.12571620941162,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1717,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.046301688992428654,\n",
      "    \"samples_seen\": 24827,\n",
      "    \"gradnorm\": 6.71875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:44.836560\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4839 num samples 33 - rank: 0 num_loss_counted_tokens: 1851\u001b[0m\n",
      "Epoch: 7, Step: 793, Rank: 0, loss = 0.05224609375\n",
      "\n",
      "Epoch 7:  62%|██████▎   | 65/104 [00:59<00:35,  1.10it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 793,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.78017722107098,\n",
      "    \"lr\": 2.940160485488436e-06,\n",
      "    \"cuda_mem_allocated\": 25.115240573883057,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1696,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.052181603773584904,\n",
      "    \"samples_seen\": 24859,\n",
      "    \"gradnorm\": 6.71875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:45.703722\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4775 num samples 32 - rank: 0 num_loss_counted_tokens: 1700\u001b[0m\n",
      "Epoch: 7, Step: 794, Rank: 0, loss = 0.04638671875\n",
      "\n",
      "Epoch 7:  63%|██████▎   | 66/104 [01:00<00:35,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 794,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.281024576116295,\n",
      "    \"lr\": 2.8953542173463133e-06,\n",
      "    \"cuda_mem_allocated\": 19.115660190582275,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1681,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.04640095181439619,\n",
      "    \"samples_seen\": 24890,\n",
      "    \"gradnorm\": 6.625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:46.670044\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4655 num samples 29 - rank: 0 num_loss_counted_tokens: 1864\u001b[0m\n",
      "Epoch: 7, Step: 795, Rank: 0, loss = 0.049072265625\n",
      "\n",
      "Epoch 7:  64%|██████▍   | 67/104 [01:01<00:33,  1.10it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 795,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.84530871718881,\n",
      "    \"lr\": 2.8953542173463133e-06,\n",
      "    \"cuda_mem_allocated\": 25.112846851348877,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1823,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04909489851892485,\n",
      "    \"samples_seen\": 24922,\n",
      "    \"gradnorm\": 6.625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:47.535821\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4689 num samples 31 - rank: 0 num_loss_counted_tokens: 1623\u001b[0m\n",
      "Epoch: 7, Step: 796, Rank: 0, loss = 0.045654296875\n",
      "\n",
      "Epoch 7:  65%|██████▌   | 68/104 [01:02<00:33,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 796,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.86007894278834,\n",
      "    \"lr\": 2.8508341233003656e-06,\n",
      "    \"cuda_mem_allocated\": 19.0864520072937,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1685,\n",
      "    \"batch_size\": 29,\n",
      "    \"total_loss\": 0.0456973293768546,\n",
      "    \"samples_seen\": 24951,\n",
      "    \"gradnorm\": 6.40625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:48.488774\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4768 num samples 30 - rank: 0 num_loss_counted_tokens: 1846\u001b[0m\n",
      "Epoch: 7, Step: 797, Rank: 0, loss = 0.055419921875\n",
      "\n",
      "Epoch 7:  66%|██████▋   | 69/104 [01:03<00:31,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 797,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.8924161112721,\n",
      "    \"lr\": 2.8508341233003656e-06,\n",
      "    \"cuda_mem_allocated\": 25.115958213806152,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1578,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.05544993662864385,\n",
      "    \"samples_seen\": 24981,\n",
      "    \"gradnorm\": 6.40625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:49.353310\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4779 num samples 32 - rank: 0 num_loss_counted_tokens: 1789\u001b[0m\n",
      "Epoch: 7, Step: 798, Rank: 0, loss = 0.0546875\n",
      "\n",
      "Epoch 7:  67%|██████▋   | 70/104 [01:04<00:31,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 798,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.135110787459176,\n",
      "    \"lr\": 2.8066019966134907e-06,\n",
      "    \"cuda_mem_allocated\": 19.128827571868896,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1763,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.054736245036868976,\n",
      "    \"samples_seen\": 25012,\n",
      "    \"gradnorm\": 7.0625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:50.322970\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4777 num samples 29 - rank: 0 num_loss_counted_tokens: 1881\u001b[0m\n",
      "Epoch: 7, Step: 799, Rank: 0, loss = 0.046630859375\n",
      "\n",
      "Epoch 7:  68%|██████▊   | 71/104 [01:05<00:29,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 799,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.32938137955811,\n",
      "    \"lr\": 2.8066019966134907e-06,\n",
      "    \"cuda_mem_allocated\": 25.089385509490967,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1619,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04663372452130945,\n",
      "    \"samples_seen\": 25044,\n",
      "    \"gradnorm\": 7.0625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:51.180254\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4822 num samples 32 - rank: 0 num_loss_counted_tokens: 1663\u001b[0m\n",
      "Epoch: 7, Step: 800, Rank: 0, loss = 0.0615234375\n",
      "\n",
      "Epoch 7:  69%|██████▉   | 72/104 [01:05<00:29,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 800,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.140567334398334,\n",
      "    \"lr\": 2.7626596189492983e-06,\n",
      "    \"cuda_mem_allocated\": 19.118534564971924,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1738,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.061565017261219795,\n",
      "    \"samples_seen\": 25076,\n",
      "    \"gradnorm\": 6.59375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:52.148986\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4788 num samples 32 - rank: 0 num_loss_counted_tokens: 1687\u001b[0m\n",
      "Epoch: 7, Step: 801, Rank: 0, loss = 0.055419921875\n",
      "\n",
      "Epoch 7:  70%|███████   | 73/104 [01:06<00:27,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 801,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.33916816562709,\n",
      "    \"lr\": 2.7626596189492983e-06,\n",
      "    \"cuda_mem_allocated\": 25.098003387451172,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1603,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.05552089831565814,\n",
      "    \"samples_seen\": 25108,\n",
      "    \"gradnorm\": 6.59375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:53.005215\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4781 num samples 31 - rank: 0 num_loss_counted_tokens: 1765\u001b[0m\n",
      "Epoch: 7, Step: 802, Rank: 0, loss = 0.057861328125\n",
      "\n",
      "Epoch 7:  71%|███████   | 74/104 [01:07<00:27,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 802,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.2030773026065,\n",
      "    \"lr\": 2.719008760300359e-06,\n",
      "    \"cuda_mem_allocated\": 19.130263805389404,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1980,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.057828282828282826,\n",
      "    \"samples_seen\": 25139,\n",
      "    \"gradnorm\": 7.3125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:53.973338\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4824 num samples 30 - rank: 0 num_loss_counted_tokens: 1732\u001b[0m\n",
      "Epoch: 7, Step: 803, Rank: 0, loss = 0.055419921875\n",
      "\n",
      "Epoch 7:  72%|███████▏  | 75/104 [01:08<00:26,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 803,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.311319168678345,\n",
      "    \"lr\": 2.719008760300359e-06,\n",
      "    \"cuda_mem_allocated\": 25.088428020477295,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1785,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.05546218487394958,\n",
      "    \"samples_seen\": 25171,\n",
      "    \"gradnorm\": 7.3125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:54.830157\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4739 num samples 30 - rank: 0 num_loss_counted_tokens: 1769\u001b[0m\n",
      "Epoch: 7, Step: 804, Rank: 0, loss = 0.0712890625\n",
      "\n",
      "Epoch 7:  73%|███████▎  | 76/104 [01:09<00:25,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 804,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.121091426132836,\n",
      "    \"lr\": 2.6756511789168926e-06,\n",
      "    \"cuda_mem_allocated\": 19.126912593841553,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1948,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.07135523613963039,\n",
      "    \"samples_seen\": 25202,\n",
      "    \"gradnorm\": 6.0,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:55.799656\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4798 num samples 32 - rank: 0 num_loss_counted_tokens: 1702\u001b[0m\n",
      "Epoch: 7, Step: 805, Rank: 0, loss = 0.0390625\n",
      "\n",
      "Epoch 7:  74%|███████▍  | 77/104 [01:10<00:24,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 805,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.35873327364812,\n",
      "    \"lr\": 2.6756511789168926e-06,\n",
      "    \"cuda_mem_allocated\": 25.10566520690918,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1870,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.039037433155080216,\n",
      "    \"samples_seen\": 25234,\n",
      "    \"gradnorm\": 6.0,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:56.654622\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4691 num samples 30 - rank: 0 num_loss_counted_tokens: 1655\u001b[0m\n",
      "Epoch: 7, Step: 806, Rank: 0, loss = 0.07666015625\n",
      "\n",
      "Epoch 7:  75%|███████▌  | 78/104 [01:11<00:23,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 806,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.608548958490076,\n",
      "    \"lr\": 2.6325886212359496e-06,\n",
      "    \"cuda_mem_allocated\": 19.100576877593994,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1738,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.07652474108170311,\n",
      "    \"samples_seen\": 25266,\n",
      "    \"gradnorm\": 6.6875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:57.612747\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4813 num samples 30 - rank: 0 num_loss_counted_tokens: 1820\u001b[0m\n",
      "Epoch: 7, Step: 807, Rank: 0, loss = 0.054931640625\n",
      "\n",
      "Epoch 7:  76%|███████▌  | 79/104 [01:12<00:22,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 807,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.185003526768405,\n",
      "    \"lr\": 2.6325886212359496e-06,\n",
      "    \"cuda_mem_allocated\": 25.112128257751465,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1576,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.054885786802030455,\n",
      "    \"samples_seen\": 25298,\n",
      "    \"gradnorm\": 6.6875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:58.470723\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4767 num samples 32 - rank: 0 num_loss_counted_tokens: 1656\u001b[0m\n",
      "Epoch: 7, Step: 808, Rank: 0, loss = 0.052734375\n",
      "\n",
      "Epoch 7:  77%|███████▋  | 80/104 [01:13<00:22,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 808,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.603185818691415,\n",
      "    \"lr\": 2.5898228218110834e-06,\n",
      "    \"cuda_mem_allocated\": 19.10728120803833,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1639,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.05277608297742526,\n",
      "    \"samples_seen\": 25330,\n",
      "    \"gradnorm\": 7.15625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:11:59.428782\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4705 num samples 32 - rank: 0 num_loss_counted_tokens: 1816\u001b[0m\n",
      "Epoch: 7, Step: 809, Rank: 0, loss = 0.047607421875\n",
      "\n",
      "Epoch 7:  78%|███████▊  | 81/104 [01:14<00:20,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 809,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.01175749750831,\n",
      "    \"lr\": 2.5898228218110834e-06,\n",
      "    \"cuda_mem_allocated\": 25.11141061782837,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1851,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.04754186925985954,\n",
      "    \"samples_seen\": 25363,\n",
      "    \"gradnorm\": 7.15625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:00.291195\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4792 num samples 34 - rank: 0 num_loss_counted_tokens: 1719\u001b[0m\n",
      "Epoch: 7, Step: 810, Rank: 0, loss = 0.0478515625\n",
      "\n",
      "Epoch 7:  79%|███████▉  | 82/104 [01:15<00:20,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 810,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.51458403442612,\n",
      "    \"lr\": 2.5473555032424534e-06,\n",
      "    \"cuda_mem_allocated\": 19.111830234527588,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1700,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04794117647058824,\n",
      "    \"samples_seen\": 25395,\n",
      "    \"gradnorm\": 5.96875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:01.252932\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4792 num samples 31 - rank: 0 num_loss_counted_tokens: 1745\u001b[0m\n",
      "Epoch: 7, Step: 811, Rank: 0, loss = 0.06591796875\n",
      "\n",
      "Epoch 7:  80%|███████▉  | 83/104 [01:15<00:18,  1.12it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 811,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.89667709332538,\n",
      "    \"lr\": 2.5473555032424534e-06,\n",
      "    \"cuda_mem_allocated\": 25.06735897064209,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1864,\n",
      "    \"batch_size\": 29,\n",
      "    \"total_loss\": 0.06571888412017167,\n",
      "    \"samples_seen\": 25424,\n",
      "    \"gradnorm\": 5.96875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:02.096358\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4770 num samples 30 - rank: 0 num_loss_counted_tokens: 1768\u001b[0m\n",
      "Epoch: 7, Step: 812, Rank: 0, loss = 0.046875\n",
      "\n",
      "Epoch 7:  81%|████████  | 84/104 [01:16<00:18,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 812,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.721762582143604,\n",
      "    \"lr\": 2.5051883761074613e-06,\n",
      "    \"cuda_mem_allocated\": 19.091241359710693,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1623,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.04682686383240912,\n",
      "    \"samples_seen\": 25455,\n",
      "    \"gradnorm\": 6.5625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:03.051968\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4760 num samples 31 - rank: 0 num_loss_counted_tokens: 1782\u001b[0m\n",
      "Epoch: 7, Step: 813, Rank: 0, loss = 0.056396484375\n",
      "\n",
      "Epoch 7:  82%|████████▏ | 85/104 [01:17<00:17,  1.12it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 813,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.37273842001819,\n",
      "    \"lr\": 2.5051883761074613e-06,\n",
      "    \"cuda_mem_allocated\": 25.094411849975586,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1846,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.056338028169014086,\n",
      "    \"samples_seen\": 25485,\n",
      "    \"gradnorm\": 6.5625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:03.905889\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4746 num samples 32 - rank: 0 num_loss_counted_tokens: 1795\u001b[0m\n",
      "Epoch: 7, Step: 814, Rank: 0, loss = 0.05615234375\n",
      "\n",
      "Epoch 7:  83%|████████▎ | 86/104 [01:18<00:16,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 814,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.487330983655504,\n",
      "    \"lr\": 2.4633231388918377e-06,\n",
      "    \"cuda_mem_allocated\": 19.11278772354126,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1789,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.056176634991615425,\n",
      "    \"samples_seen\": 25517,\n",
      "    \"gradnorm\": 7.0,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:04.866826\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4832 num samples 31 - rank: 0 num_loss_counted_tokens: 1649\u001b[0m\n",
      "Epoch: 7, Step: 815, Rank: 0, loss = 0.03271484375\n",
      "\n",
      "Epoch 7:  84%|████████▎ | 87/104 [01:19<00:15,  1.12it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 815,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.54885140789458,\n",
      "    \"lr\": 2.4633231388918377e-06,\n",
      "    \"cuda_mem_allocated\": 25.096567153930664,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1881,\n",
      "    \"batch_size\": 29,\n",
      "    \"total_loss\": 0.03269537480063796,\n",
      "    \"samples_seen\": 25546,\n",
      "    \"gradnorm\": 7.0,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:05.717054\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 1594 num samples 10 - rank: 0 num_loss_counted_tokens: 543\u001b[0m\n",
      "Epoch: 7, Step: 816, Rank: 0, loss = 0.06591796875\n",
      "\n",
      "Epoch 7:  85%|████████▍ | 88/104 [01:20<00:14,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 816,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.26336667529182,\n",
      "    \"lr\": 2.421761477921232e-06,\n",
      "    \"cuda_mem_allocated\": 19.123082160949707,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1663,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.0661455201443175,\n",
      "    \"samples_seen\": 25578,\n",
      "    \"gradnorm\": 5.40625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:06.683023\"\n",
      "}\u001b[0m\n",
      "Epoch: 7, Step: 817, Rank: 0, loss = 0.046630859375\n",
      "\n",
      "Epoch 7:  86%|████████▌ | 89/104 [01:21<00:13,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 817,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.40777757453384,\n",
      "    \"lr\": 2.421761477921232e-06,\n",
      "    \"cuda_mem_allocated\": 25.099199771881104,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1687,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.046532305868405455,\n",
      "    \"samples_seen\": 25610,\n",
      "    \"gradnorm\": 5.40625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:07.536755\"\n",
      "}\u001b[0m\n",
      "Epoch: 7, Step: 818, Rank: 0, loss = 0.07470703125\n",
      "\n",
      "Epoch 7:  87%|████████▋ | 90/104 [01:22<00:12,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 818,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.609370950066,\n",
      "    \"lr\": 2.380505067293293e-06,\n",
      "    \"cuda_mem_allocated\": 19.113266468048096,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1765,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.07478753541076487,\n",
      "    \"samples_seen\": 25641,\n",
      "    \"gradnorm\": 7.09375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:08.495244\"\n",
      "}\u001b[0m\n",
      "Epoch: 7, Step: 819, Rank: 0, loss = 0.052734375\n",
      "\n",
      "Epoch 7:  88%|████████▊ | 91/104 [01:23<00:11,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 819,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.834945793055596,\n",
      "    \"lr\": 2.380505067293293e-06,\n",
      "    \"cuda_mem_allocated\": 25.107819080352783,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1732,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.05282909930715935,\n",
      "    \"samples_seen\": 25671,\n",
      "    \"gradnorm\": 7.09375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:09.359849\"\n",
      "}\u001b[0m\n",
      "Epoch: 7, Step: 820, Rank: 0, loss = 0.04248046875\n",
      "\n",
      "Epoch 7:  88%|████████▊ | 92/104 [01:24<00:11,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 820,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.58753714849653,\n",
      "    \"lr\": 2.339555568810221e-06,\n",
      "    \"cuda_mem_allocated\": 19.103212356567383,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1769,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.0423968343697004,\n",
      "    \"samples_seen\": 25701,\n",
      "    \"gradnorm\": 6.15625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:10.317681\"\n",
      "}\u001b[0m\n",
      "Epoch: 7, Step: 821, Rank: 0, loss = 0.0537109375\n",
      "\n",
      "Epoch 7:  89%|████████▉ | 93/104 [01:25<00:09,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 821,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.17441285544916,\n",
      "    \"lr\": 2.339555568810221e-06,\n",
      "    \"cuda_mem_allocated\": 25.101593494415283,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1702,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.05376028202115159,\n",
      "    \"samples_seen\": 25733,\n",
      "    \"gradnorm\": 6.15625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:11.177811\"\n",
      "}\u001b[0m\n",
      "Epoch: 7, Step: 822, Rank: 0, loss = 0.0517578125\n",
      "\n",
      "Epoch 7:  90%|█████████ | 94/104 [01:25<00:09,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 822,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.5737947599598,\n",
      "    \"lr\": 2.2989146319118428e-06,\n",
      "    \"cuda_mem_allocated\": 19.09172010421753,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1655,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.051661631419939576,\n",
      "    \"samples_seen\": 25763,\n",
      "    \"gradnorm\": 7.0625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:12.137736\"\n",
      "}\u001b[0m\n",
      "Epoch: 7, Step: 823, Rank: 0, loss = 0.0625\n",
      "\n",
      "Epoch 7:  91%|█████████▏| 95/104 [01:26<00:08,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 823,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.197568786735566,\n",
      "    \"lr\": 2.2989146319118428e-06,\n",
      "    \"cuda_mem_allocated\": 25.105186462402344,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1820,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.06263736263736264,\n",
      "    \"samples_seen\": 25793,\n",
      "    \"gradnorm\": 7.0625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:12.995427\"\n",
      "}\u001b[0m\n",
      "Epoch: 7, Step: 824, Rank: 0, loss = 0.0537109375\n",
      "\n",
      "Epoch 7:  92%|█████████▏| 96/104 [01:27<00:07,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 824,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.33284230348259,\n",
      "    \"lr\": 2.2585838936091753e-06,\n",
      "    \"cuda_mem_allocated\": 19.109914779663086,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1656,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.05374396135265701,\n",
      "    \"samples_seen\": 25825,\n",
      "    \"gradnorm\": 7.25,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:13.960179\"\n",
      "}\u001b[0m\n",
      "Epoch: 7, Step: 825, Rank: 0, loss = 0.06494140625\n",
      "\n",
      "Epoch 7:  93%|█████████▎| 97/104 [01:28<00:06,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 825,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.400767068597304,\n",
      "    \"lr\": 2.2585838936091753e-06,\n",
      "    \"cuda_mem_allocated\": 25.07932996749878,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1816,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.06470264317180617,\n",
      "    \"samples_seen\": 25857,\n",
      "    \"gradnorm\": 7.25,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:14.814624\"\n",
      "}\u001b[0m\n",
      "Epoch: 7, Step: 826, Rank: 0, loss = 0.055908203125\n",
      "\n",
      "Epoch 7:  94%|█████████▍| 98/104 [01:29<00:05,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 826,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.454671183330355,\n",
      "    \"lr\": 2.218564978418475e-06,\n",
      "    \"cuda_mem_allocated\": 19.115899085998535,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1719,\n",
      "    \"batch_size\": 34,\n",
      "    \"total_loss\": 0.055846422338568937,\n",
      "    \"samples_seen\": 25891,\n",
      "    \"gradnorm\": 6.59375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:15.776394\"\n",
      "}\u001b[0m\n",
      "Epoch: 7, Step: 827, Rank: 0, loss = 0.059326171875\n",
      "\n",
      "Epoch 7:  95%|█████████▌| 99/104 [01:30<00:04,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 827,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.54548441282333,\n",
      "    \"lr\": 2.218564978418475e-06,\n",
      "    \"cuda_mem_allocated\": 25.100157260894775,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1745,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.05931232091690544,\n",
      "    \"samples_seen\": 25922,\n",
      "    \"gradnorm\": 6.59375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:16.648636\"\n",
      "}\u001b[0m\n",
      "Epoch: 7, Step: 828, Rank: 0, loss = 0.04541015625\n",
      "\n",
      "Epoch 7:  96%|█████████▌| 100/104 [01:31<00:03,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 828,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 36.93394320408895,\n",
      "    \"lr\": 2.1788594982958087e-06,\n",
      "    \"cuda_mem_allocated\": 19.11063289642334,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1768,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.045531674208144794,\n",
      "    \"samples_seen\": 25952,\n",
      "    \"gradnorm\": 6.71875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:17.622987\"\n",
      "}\u001b[0m\n",
      "Epoch: 7, Step: 829, Rank: 0, loss = 0.0458984375\n",
      "\n",
      "Epoch 7:  97%|█████████▋| 101/104 [01:32<00:02,  1.10it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 829,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.88917065165237,\n",
      "    \"lr\": 2.1788594982958087e-06,\n",
      "    \"cuda_mem_allocated\": 25.092496871948242,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1782,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.04601571268237935,\n",
      "    \"samples_seen\": 25983,\n",
      "    \"gradnorm\": 6.71875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:18.488698\"\n",
      "}\u001b[0m\n",
      "Epoch: 7, Step: 830, Rank: 0, loss = 0.06298828125\n",
      "\n",
      "Epoch 7:  98%|█████████▊| 102/104 [01:33<00:01,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 830,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.011133033588266,\n",
      "    \"lr\": 2.1394690525721275e-06,\n",
      "    \"cuda_mem_allocated\": 19.10488748550415,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1795,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.06295264623955432,\n",
      "    \"samples_seen\": 26015,\n",
      "    \"gradnorm\": 6.125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:19.461176\"\n",
      "}\u001b[0m\n",
      "Epoch: 7, Step: 831, Rank: 0, loss = 0.052490234375\n",
      "\n",
      "Epoch 7:  99%|█████████▉| 103/104 [01:34<00:00,  1.10it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 831,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.37505893453228,\n",
      "    \"lr\": 2.1394690525721275e-06,\n",
      "    \"cuda_mem_allocated\": 25.109734058380127,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1649,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.052456033959975744,\n",
      "    \"samples_seen\": 26046,\n",
      "    \"gradnorm\": 6.125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:20.337224\"\n",
      "}\u001b[0m\n",
      "Epoch: 7, Step: 832, Rank: 0, loss = 0.04443359375\n",
      "\n",
      "Epoch 7: 100%|██████████| 104/104 [01:34<00:00,  1.27it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 7,\n",
      "    \"step\": 832,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 81.80396459124142,\n",
      "    \"lr\": 2.1003952278888382e-06,\n",
      "    \"cuda_mem_allocated\": 18.35026741027832,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 543,\n",
      "    \"batch_size\": 10,\n",
      "    \"total_loss\": 0.044429097605893184,\n",
      "    \"samples_seen\": 26056,\n",
      "    \"gradnorm\": 8.625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:20.836501\"\n",
      "}\u001b[0m\n",
      "\u001b[93mSaving model in huggingface format at: samples_26056\u001b[0m\n",
      "[10:12:44] INFO     The model is bigger than the maximum     accelerator.py:2924\n",
      "                    size per checkpoint (5GB) and is going                      \n",
      "                    to be split in 2 checkpoint shards. You                     \n",
      "                    can find where each parameters has been                     \n",
      "                    saved in the index located at                               \n",
      "                    /root/.local/share/instructlab/checkpoin                    \n",
      "                    ts/hf_format/samples_26056/model.safeten                    \n",
      "                    sors.index.json.                                            \n",
      "\u001b[93mModel saved in /root/.local/share/instructlab/checkpoints/hf_format/samples_26056\u001b[0m\n",
      "           INFO     saving took 23.857912302017212 seconds          utils.py:879\n",
      "Epoch 7: 100%|██████████| 104/104 [01:58<00:00,  1.14s/it]\n",
      "\u001b[96m total length: 4809 num samples 32 - rank: 0 num_loss_counted_tokens: 1768\u001b[0m\n",
      "\u001b[96m total length: 4846 num samples 33 - rank: 0 num_loss_counted_tokens: 1754\u001b[0m\n",
      "\u001b[96m total length: 4753 num samples 32 - rank: 0 num_loss_counted_tokens: 1678\u001b[0m\n",
      "\u001b[96m total length: 4724 num samples 30 - rank: 0 num_loss_counted_tokens: 1658\u001b[0m\n",
      "\u001b[96m total length: 4774 num samples 30 - rank: 0 num_loss_counted_tokens: 1796\u001b[0m\n",
      "\u001b[96m total length: 4767 num samples 30 - rank: 0 num_loss_counted_tokens: 1660\u001b[0m\n",
      "\u001b[96m total length: 4792 num samples 32 - rank: 0 num_loss_counted_tokens: 1733\u001b[0m\n",
      "\u001b[96m total length: 4822 num samples 30 - rank: 0 num_loss_counted_tokens: 1554\u001b[0m\n",
      "\u001b[96m total length: 4768 num samples 30 - rank: 0 num_loss_counted_tokens: 1777\u001b[0m\n",
      "\u001b[96m total length: 4786 num samples 32 - rank: 0 num_loss_counted_tokens: 1739\u001b[0m\n",
      "\u001b[96m total length: 4721 num samples 30 - rank: 0 num_loss_counted_tokens: 1861\u001b[0m\n",
      "\u001b[96m total length: 4705 num samples 31 - rank: 0 num_loss_counted_tokens: 1732\u001b[0m\n",
      "\u001b[96m total length: 4786 num samples 32 - rank: 0 num_loss_counted_tokens: 1841\u001b[0m\n",
      "\u001b[96m total length: 4808 num samples 33 - rank: 0 num_loss_counted_tokens: 1720\u001b[0m\n",
      "\u001b[96m total length: 4771 num samples 32 - rank: 0 num_loss_counted_tokens: 1811\u001b[0m\n",
      "\u001b[96m total length: 4799 num samples 32 - rank: 0 num_loss_counted_tokens: 1530\u001b[0m\n",
      "\u001b[96m total length: 4852 num samples 33 - rank: 0 num_loss_counted_tokens: 1724\u001b[0m\n",
      "Epoch: 8, Step: 833, Rank: 0, loss = 0.051513671875\n",
      "Epoch 8:   1%|          | 1/104 [00:01<02:03,  1.20s/it]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 833,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.04800312244156,\n",
      "    \"lr\": 2.1003952278888382e-06,\n",
      "    \"cuda_mem_allocated\": 25.104228973388672,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1768,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.051470588235294115,\n",
      "    \"samples_seen\": 26088,\n",
      "    \"gradnorm\": 8.625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:46.005087\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4736 num samples 31 - rank: 0 num_loss_counted_tokens: 1693\u001b[0m\n",
      "Epoch: 8, Step: 834, Rank: 0, loss = 0.05322265625\n",
      "Epoch 8:   2%|▏         | 2/104 [00:02<01:47,  1.06s/it]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 834,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.564782434318595,\n",
      "    \"lr\": 2.0616395981339076e-06,\n",
      "    \"cuda_mem_allocated\": 19.106563568115234,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1678,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.0533373063170441,\n",
      "    \"samples_seen\": 26120,\n",
      "    \"gradnorm\": 6.125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:46.966033\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4764 num samples 33 - rank: 0 num_loss_counted_tokens: 1719\u001b[0m\n",
      "Epoch: 8, Step: 835, Rank: 0, loss = 0.048828125\n",
      "Epoch 8:   3%|▎         | 3/104 [00:03<01:38,  1.03it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 835,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.79340356459087,\n",
      "    \"lr\": 2.0616395981339076e-06,\n",
      "    \"cuda_mem_allocated\": 25.113085746765137,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1754,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.04874572405929305,\n",
      "    \"samples_seen\": 26153,\n",
      "    \"gradnorm\": 6.125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:47.834566\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4812 num samples 32 - rank: 0 num_loss_counted_tokens: 1738\u001b[0m\n",
      "Epoch: 8, Step: 836, Rank: 0, loss = 0.043701171875\n",
      "Epoch 8:   4%|▍         | 4/104 [00:03<01:37,  1.03it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 836,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.45836112570114,\n",
      "    \"lr\": 2.0232037243784475e-06,\n",
      "    \"cuda_mem_allocated\": 19.11159038543701,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1796,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.04370824053452116,\n",
      "    \"samples_seen\": 26183,\n",
      "    \"gradnorm\": 5.78125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:48.806668\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4715 num samples 30 - rank: 0 num_loss_counted_tokens: 1816\u001b[0m\n",
      "Epoch: 8, Step: 837, Rank: 0, loss = 0.0546875\n",
      "Epoch 8:   5%|▍         | 5/104 [00:04<01:32,  1.08it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 837,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.252249248690696,\n",
      "    \"lr\": 2.0232037243784475e-06,\n",
      "    \"cuda_mem_allocated\": 25.083877563476562,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1658,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.054583835946924,\n",
      "    \"samples_seen\": 26213,\n",
      "    \"gradnorm\": 5.78125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:49.661269\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4672 num samples 31 - rank: 0 num_loss_counted_tokens: 1759\u001b[0m\n",
      "Epoch: 8, Step: 838, Rank: 0, loss = 0.033203125\n",
      "Epoch 8:   6%|▌         | 6/104 [00:05<01:31,  1.07it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 838,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.695668744335514,\n",
      "    \"lr\": 1.9850891548138463e-06,\n",
      "    \"cuda_mem_allocated\": 19.109914779663086,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1660,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.03313253012048193,\n",
      "    \"samples_seen\": 26243,\n",
      "    \"gradnorm\": 6.21875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:50.614759\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4809 num samples 30 - rank: 0 num_loss_counted_tokens: 1764\u001b[0m\n",
      "Epoch: 8, Step: 839, Rank: 0, loss = 0.044921875\n",
      "Epoch 8:   7%|▋         | 7/104 [00:06<01:28,  1.10it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 839,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.571063869933944,\n",
      "    \"lr\": 1.9850891548138463e-06,\n",
      "    \"cuda_mem_allocated\": 25.100157260894775,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1733,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04500865551067513,\n",
      "    \"samples_seen\": 26275,\n",
      "    \"gradnorm\": 6.21875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:51.462211\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4731 num samples 31 - rank: 0 num_loss_counted_tokens: 1709\u001b[0m\n",
      "Epoch: 8, Step: 840, Rank: 0, loss = 0.04443359375\n",
      "Epoch 8:   8%|▊         | 8/104 [00:07<01:28,  1.08it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 840,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.60994004220612,\n",
      "    \"lr\": 1.947297424689414e-06,\n",
      "    \"cuda_mem_allocated\": 19.110153675079346,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1777,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.04445694991558807,\n",
      "    \"samples_seen\": 26305,\n",
      "    \"gradnorm\": 5.625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:52.418199\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4748 num samples 32 - rank: 0 num_loss_counted_tokens: 1719\u001b[0m\n",
      "Epoch: 8, Step: 841, Rank: 0, loss = 0.057373046875\n",
      "Epoch 8:   9%|▊         | 9/104 [00:08<01:25,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 841,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.2745089432807,\n",
      "    \"lr\": 1.947297424689414e-06,\n",
      "    \"cuda_mem_allocated\": 25.107340335845947,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1554,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.05727155727155727,\n",
      "    \"samples_seen\": 26335,\n",
      "    \"gradnorm\": 5.625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:53.271784\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4724 num samples 31 - rank: 0 num_loss_counted_tokens: 1652\u001b[0m\n",
      "Epoch: 8, Step: 842, Rank: 0, loss = 0.048828125\n",
      "Epoch 8:  10%|▉         | 10/104 [00:09<01:26,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 842,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.629626472954826,\n",
      "    \"lr\": 1.9098300562505266e-06,\n",
      "    \"cuda_mem_allocated\": 19.114462852478027,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1739,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.0488786658999425,\n",
      "    \"samples_seen\": 26367,\n",
      "    \"gradnorm\": 6.65625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:54.226919\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4727 num samples 31 - rank: 0 num_loss_counted_tokens: 1726\u001b[0m\n",
      "Epoch: 8, Step: 843, Rank: 0, loss = 0.052734375\n",
      "Epoch 8:  11%|█         | 11/104 [00:10<01:23,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 843,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.42450411582106,\n",
      "    \"lr\": 1.9098300562505266e-06,\n",
      "    \"cuda_mem_allocated\": 25.083159923553467,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1861,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.052659860290166574,\n",
      "    \"samples_seen\": 26397,\n",
      "    \"gradnorm\": 6.65625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:55.077656\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4856 num samples 31 - rank: 0 num_loss_counted_tokens: 1865\u001b[0m\n",
      "Epoch: 8, Step: 844, Rank: 0, loss = 0.035888671875\n",
      "Epoch 8:  12%|█▏        | 12/104 [00:11<01:24,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 844,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.40839442749862,\n",
      "    \"lr\": 1.8726885586773213e-06,\n",
      "    \"cuda_mem_allocated\": 19.114462852478027,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1841,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.035850081477457905,\n",
      "    \"samples_seen\": 26429,\n",
      "    \"gradnorm\": 5.40625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:56.040859\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4768 num samples 32 - rank: 0 num_loss_counted_tokens: 1771\u001b[0m\n",
      "Epoch: 8, Step: 845, Rank: 0, loss = 0.04541015625\n",
      "Epoch 8:  12%|█▎        | 13/104 [00:12<01:21,  1.12it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 845,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.693298278508415,\n",
      "    \"lr\": 1.8726885586773213e-06,\n",
      "    \"cuda_mem_allocated\": 25.07932996749878,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1732,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.045323325635103925,\n",
      "    \"samples_seen\": 26460,\n",
      "    \"gradnorm\": 5.40625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:56.885717\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4748 num samples 32 - rank: 0 num_loss_counted_tokens: 1721\u001b[0m\n",
      "Epoch: 8, Step: 846, Rank: 0, loss = 0.05322265625\n",
      "Epoch 8:  13%|█▎        | 14/104 [00:13<01:22,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 846,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.63206366661162,\n",
      "    \"lr\": 1.8358744280239048e-06,\n",
      "    \"cuda_mem_allocated\": 19.119730949401855,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1720,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.05319767441860465,\n",
      "    \"samples_seen\": 26493,\n",
      "    \"gradnorm\": 5.96875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:57.842974\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4669 num samples 31 - rank: 0 num_loss_counted_tokens: 1864\u001b[0m\n",
      "Epoch: 8, Step: 847, Rank: 0, loss = 0.046630859375\n",
      "Epoch 8:  14%|█▍        | 15/104 [00:13<01:19,  1.12it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 847,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.47104260956767,\n",
      "    \"lr\": 1.8358744280239048e-06,\n",
      "    \"cuda_mem_allocated\": 25.095130920410156,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1811,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04665930425179459,\n",
      "    \"samples_seen\": 26525,\n",
      "    \"gradnorm\": 5.96875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:58.694970\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4757 num samples 32 - rank: 0 num_loss_counted_tokens: 1813\u001b[0m\n",
      "Epoch: 8, Step: 848, Rank: 0, loss = 0.051513671875\n",
      "Epoch 8:  15%|█▌        | 16/104 [00:14<01:20,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 848,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.279678426083215,\n",
      "    \"lr\": 1.7993891471580894e-06,\n",
      "    \"cuda_mem_allocated\": 19.130263805389404,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1724,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.05162412993039443,\n",
      "    \"samples_seen\": 26558,\n",
      "    \"gradnorm\": 5.875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:12:59.662244\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4812 num samples 32 - rank: 0 num_loss_counted_tokens: 1749\u001b[0m\n",
      "Epoch: 8, Step: 849, Rank: 0, loss = 0.052978515625\n",
      "Epoch 8:  16%|█▋        | 17/104 [00:15<01:18,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 849,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.183870989402195,\n",
      "    \"lr\": 1.7993891471580894e-06,\n",
      "    \"cuda_mem_allocated\": 25.10183334350586,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1530,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.052941176470588235,\n",
      "    \"samples_seen\": 26590,\n",
      "    \"gradnorm\": 5.875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:00.519390\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4813 num samples 31 - rank: 0 num_loss_counted_tokens: 1850\u001b[0m\n",
      "Epoch: 8, Step: 850, Rank: 0, loss = 0.05029296875\n",
      "Epoch 8:  17%|█▋        | 18/104 [00:16<01:18,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 850,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.5408266897364,\n",
      "    \"lr\": 1.7632341857016733e-06,\n",
      "    \"cuda_mem_allocated\": 19.102491855621338,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1693,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.05020673360897814,\n",
      "    \"samples_seen\": 26621,\n",
      "    \"gradnorm\": 6.625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:01.478641\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4783 num samples 29 - rank: 0 num_loss_counted_tokens: 1741\u001b[0m\n",
      "Epoch: 8, Step: 851, Rank: 0, loss = 0.05810546875\n",
      "Epoch 8:  18%|█▊        | 19/104 [00:17<01:16,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 851,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.11542275942966,\n",
      "    \"lr\": 1.7632341857016733e-06,\n",
      "    \"cuda_mem_allocated\": 25.093454360961914,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1719,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.058173356602675974,\n",
      "    \"samples_seen\": 26654,\n",
      "    \"gradnorm\": 6.625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:02.338471\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4610 num samples 31 - rank: 0 num_loss_counted_tokens: 1669\u001b[0m\n",
      "Epoch: 8, Step: 852, Rank: 0, loss = 0.04345703125\n",
      "Epoch 8:  19%|█▉        | 20/104 [00:18<01:17,  1.08it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 852,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 36.86843415829522,\n",
      "    \"lr\": 1.7274109999712295e-06,\n",
      "    \"cuda_mem_allocated\": 19.120688438415527,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1738,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04344073647871116,\n",
      "    \"samples_seen\": 26686,\n",
      "    \"gradnorm\": 6.0625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:03.314116\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4804 num samples 31 - rank: 0 num_loss_counted_tokens: 1719\u001b[0m\n",
      "Epoch: 8, Step: 853, Rank: 0, loss = 0.049072265625\n",
      "Epoch 8:  20%|██        | 21/104 [00:19<01:15,  1.10it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 853,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.942790094169915,\n",
      "    \"lr\": 1.7274109999712295e-06,\n",
      "    \"cuda_mem_allocated\": 25.08172369003296,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1816,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.049008810572687224,\n",
      "    \"samples_seen\": 26716,\n",
      "    \"gradnorm\": 6.0625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:04.178679\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4693 num samples 32 - rank: 0 num_loss_counted_tokens: 1588\u001b[0m\n",
      "Epoch: 8, Step: 854, Rank: 0, loss = 0.041748046875\n",
      "Epoch 8:  21%|██        | 22/104 [00:20<01:15,  1.08it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 854,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.32369608175201,\n",
      "    \"lr\": 1.6919210329194535e-06,\n",
      "    \"cuda_mem_allocated\": 19.087169647216797,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1759,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.041785105173393976,\n",
      "    \"samples_seen\": 26747,\n",
      "    \"gradnorm\": 5.6875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:05.143885\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4700 num samples 31 - rank: 0 num_loss_counted_tokens: 1746\u001b[0m\n",
      "Epoch: 8, Step: 855, Rank: 0, loss = 0.03564453125\n",
      "Epoch 8:  22%|██▏       | 23/104 [00:21<01:13,  1.10it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 855,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.53709538164343,\n",
      "    \"lr\": 1.6919210329194535e-06,\n",
      "    \"cuda_mem_allocated\": 25.104228973388672,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1764,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.03571428571428571,\n",
      "    \"samples_seen\": 26777,\n",
      "    \"gradnorm\": 5.6875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:06.018200\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4800 num samples 33 - rank: 0 num_loss_counted_tokens: 1628\u001b[0m\n",
      "Epoch: 8, Step: 856, Rank: 0, loss = 0.049072265625\n",
      "Epoch 8:  23%|██▎       | 24/104 [00:22<01:13,  1.08it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 856,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.45005194043494,\n",
      "    \"lr\": 1.6567657140770477e-06,\n",
      "    \"cuda_mem_allocated\": 19.101295471191406,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1709,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.049151550614394385,\n",
      "    \"samples_seen\": 26808,\n",
      "    \"gradnorm\": 5.71875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:06.980148\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4724 num samples 32 - rank: 0 num_loss_counted_tokens: 1696\u001b[0m\n",
      "Epoch: 8, Step: 857, Rank: 0, loss = 0.04541015625\n",
      "Epoch 8:  24%|██▍       | 25/104 [00:23<01:11,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 857,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.23141372275063,\n",
      "    \"lr\": 1.6567657140770477e-06,\n",
      "    \"cuda_mem_allocated\": 25.089624404907227,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1719,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04537521815008726,\n",
      "    \"samples_seen\": 26840,\n",
      "    \"gradnorm\": 5.71875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:07.837376\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4833 num samples 32 - rank: 0 num_loss_counted_tokens: 1817\u001b[0m\n",
      "Epoch: 8, Step: 858, Rank: 0, loss = 0.06640625\n",
      "Epoch 8:  25%|██▌       | 26/104 [00:23<01:11,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 858,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.72367097836431,\n",
      "    \"lr\": 1.6219464594951273e-06,\n",
      "    \"cuda_mem_allocated\": 19.099619388580322,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1652,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.06628329297820823,\n",
      "    \"samples_seen\": 26871,\n",
      "    \"gradnorm\": 5.875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:08.792571\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4828 num samples 32 - rank: 0 num_loss_counted_tokens: 1914\u001b[0m\n",
      "Epoch: 8, Step: 859, Rank: 0, loss = 0.046142578125\n",
      "Epoch 8:  26%|██▌       | 27/104 [00:24<01:09,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 859,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.57034055185374,\n",
      "    \"lr\": 1.6219464594951273e-06,\n",
      "    \"cuda_mem_allocated\": 25.084596157073975,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1726,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.04606025492468135,\n",
      "    \"samples_seen\": 26902,\n",
      "    \"gradnorm\": 5.875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:09.642582\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4747 num samples 33 - rank: 0 num_loss_counted_tokens: 1723\u001b[0m\n",
      "Epoch: 8, Step: 860, Rank: 0, loss = 0.04443359375\n",
      "Epoch 8:  27%|██▋       | 28/104 [00:25<01:09,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 860,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.28793289058604,\n",
      "    \"lr\": 1.587464671688187e-06,\n",
      "    \"cuda_mem_allocated\": 19.131221294403076,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1865,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.04450402144772118,\n",
      "    \"samples_seen\": 26933,\n",
      "    \"gradnorm\": 5.78125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:10.607494\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4757 num samples 31 - rank: 0 num_loss_counted_tokens: 1835\u001b[0m\n",
      "Epoch: 8, Step: 861, Rank: 0, loss = 0.0458984375\n",
      "Epoch 8:  28%|██▊       | 29/104 [00:26<01:07,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 861,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.24810562252813,\n",
      "    \"lr\": 1.587464671688187e-06,\n",
      "    \"cuda_mem_allocated\": 25.094411849975586,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1771,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04601919819311124,\n",
      "    \"samples_seen\": 26965,\n",
      "    \"gradnorm\": 5.78125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:11.464907\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4803 num samples 33 - rank: 0 num_loss_counted_tokens: 1809\u001b[0m\n",
      "Epoch: 8, Step: 862, Rank: 0, loss = 0.0693359375\n",
      "Epoch 8:  29%|██▉       | 30/104 [00:27<01:08,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 862,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.37765184057836,\n",
      "    \"lr\": 1.553321739577619e-06,\n",
      "    \"cuda_mem_allocated\": 19.105366230010986,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1721,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.06914584543869844,\n",
      "    \"samples_seen\": 26997,\n",
      "    \"gradnorm\": 6.0625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:12.428436\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4803 num samples 32 - rank: 0 num_loss_counted_tokens: 1851\u001b[0m\n",
      "Epoch: 8, Step: 863, Rank: 0, loss = 0.0673828125\n",
      "Epoch 8:  30%|██▉       | 31/104 [00:28<01:05,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 863,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.737814803608025,\n",
      "    \"lr\": 1.553321739577619e-06,\n",
      "    \"cuda_mem_allocated\": 25.07071018218994,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1864,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.06759656652360516,\n",
      "    \"samples_seen\": 27028,\n",
      "    \"gradnorm\": 6.0625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:13.276085\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4751 num samples 31 - rank: 0 num_loss_counted_tokens: 1763\u001b[0m\n",
      "Epoch: 8, Step: 864, Rank: 0, loss = 0.038818359375\n",
      "Epoch 8:  31%|███       | 32/104 [00:29<01:06,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 864,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.2833028976578,\n",
      "    \"lr\": 1.5195190384357405e-06,\n",
      "    \"cuda_mem_allocated\": 19.107521057128906,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1813,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.038885824600110315,\n",
      "    \"samples_seen\": 27060,\n",
      "    \"gradnorm\": 6.0,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:14.241575\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4743 num samples 32 - rank: 0 num_loss_counted_tokens: 1596\u001b[0m\n",
      "Epoch: 8, Step: 865, Rank: 0, loss = 0.036865234375\n",
      "Epoch 8:  32%|███▏      | 33/104 [00:30<01:03,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 865,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.31004115879598,\n",
      "    \"lr\": 1.5195190384357405e-06,\n",
      "    \"cuda_mem_allocated\": 25.104946613311768,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1749,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.03687821612349914,\n",
      "    \"samples_seen\": 27092,\n",
      "    \"gradnorm\": 6.0,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:15.096792\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4757 num samples 31 - rank: 0 num_loss_counted_tokens: 1789\u001b[0m\n",
      "Epoch: 8, Step: 866, Rank: 0, loss = 0.041015625\n",
      "Epoch 8:  33%|███▎      | 34/104 [00:31<01:04,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 866,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.34063206391026,\n",
      "    \"lr\": 1.4860579298304311e-06,\n",
      "    \"cuda_mem_allocated\": 19.120928287506104,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1850,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.04108108108108108,\n",
      "    \"samples_seen\": 27123,\n",
      "    \"gradnorm\": 5.90625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:16.060423\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4785 num samples 32 - rank: 0 num_loss_counted_tokens: 1734\u001b[0m\n",
      "Epoch: 8, Step: 867, Rank: 0, loss = 0.06298828125\n",
      "Epoch 8:  34%|███▎      | 35/104 [00:32<01:02,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 867,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.26552392364654,\n",
      "    \"lr\": 1.4860579298304311e-06,\n",
      "    \"cuda_mem_allocated\": 25.098003387451172,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1741,\n",
      "    \"batch_size\": 29,\n",
      "    \"total_loss\": 0.06289488799540494,\n",
      "    \"samples_seen\": 27152,\n",
      "    \"gradnorm\": 5.90625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:16.916556\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4697 num samples 27 - rank: 0 num_loss_counted_tokens: 1825\u001b[0m\n",
      "Epoch: 8, Step: 868, Rank: 0, loss = 0.0654296875\n",
      "Epoch 8:  35%|███▍      | 36/104 [00:33<01:02,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 868,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.07634007042341,\n",
      "    \"lr\": 1.4529397615702656e-06,\n",
      "    \"cuda_mem_allocated\": 19.072328090667725,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1669,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.06560814859197124,\n",
      "    \"samples_seen\": 27183,\n",
      "    \"gradnorm\": 6.65625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:17.863740\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4694 num samples 29 - rank: 0 num_loss_counted_tokens: 1715\u001b[0m\n",
      "Epoch: 8, Step: 869, Rank: 0, loss = 0.059814453125\n",
      "Epoch 8:  36%|███▌      | 37/104 [00:33<01:00,  1.12it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 869,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.326189335571875,\n",
      "    \"lr\": 1.4529397615702656e-06,\n",
      "    \"cuda_mem_allocated\": 25.103031635284424,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1719,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.05991855730075625,\n",
      "    \"samples_seen\": 27214,\n",
      "    \"gradnorm\": 6.65625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:18.719715\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4761 num samples 32 - rank: 0 num_loss_counted_tokens: 1638\u001b[0m\n",
      "Epoch: 8, Step: 870, Rank: 0, loss = 0.040283203125\n",
      "Epoch 8:  37%|███▋      | 38/104 [00:34<01:00,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 870,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.62483741040753,\n",
      "    \"lr\": 1.4201658676502294e-06,\n",
      "    \"cuda_mem_allocated\": 19.092198848724365,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1588,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04030226700251889,\n",
      "    \"samples_seen\": 27246,\n",
      "    \"gradnorm\": 5.875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:19.677603\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4721 num samples 31 - rank: 0 num_loss_counted_tokens: 1796\u001b[0m\n",
      "Epoch: 8, Step: 871, Rank: 0, loss = 0.06103515625\n",
      "Epoch 8:  38%|███▊      | 39/104 [00:35<00:58,  1.12it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 871,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.7022452765893,\n",
      "    \"lr\": 1.4201658676502294e-06,\n",
      "    \"cuda_mem_allocated\": 25.078132152557373,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1746,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.06099656357388316,\n",
      "    \"samples_seen\": 27277,\n",
      "    \"gradnorm\": 5.875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:20.525356\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4837 num samples 32 - rank: 0 num_loss_counted_tokens: 1764\u001b[0m\n",
      "Epoch: 8, Step: 872, Rank: 0, loss = 0.0439453125\n",
      "Epoch 8:  38%|███▊      | 40/104 [00:36<00:58,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 872,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.17284613609945,\n",
      "    \"lr\": 1.3877375681979944e-06,\n",
      "    \"cuda_mem_allocated\": 19.11781406402588,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1628,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.04391891891891892,\n",
      "    \"samples_seen\": 27310,\n",
      "    \"gradnorm\": 6.4375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:21.493571\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4800 num samples 31 - rank: 0 num_loss_counted_tokens: 1751\u001b[0m\n",
      "Epoch: 8, Step: 873, Rank: 0, loss = 0.04296875\n",
      "Epoch 8:  39%|███▉      | 41/104 [00:37<00:56,  1.12it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 873,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.64501661208074,\n",
      "    \"lr\": 1.3877375681979944e-06,\n",
      "    \"cuda_mem_allocated\": 25.083877563476562,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1696,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04304245283018868,\n",
      "    \"samples_seen\": 27342,\n",
      "    \"gradnorm\": 6.4375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:22.341731\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4782 num samples 32 - rank: 0 num_loss_counted_tokens: 1831\u001b[0m\n",
      "Epoch: 8, Step: 874, Rank: 0, loss = 0.058349609375\n",
      "Epoch 8:  40%|████      | 42/104 [00:38<00:56,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 874,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.23949406480315,\n",
      "    \"lr\": 1.3556561694207337e-06,\n",
      "    \"cuda_mem_allocated\": 19.12571620941162,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1817,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.058337919647771054,\n",
      "    \"samples_seen\": 27374,\n",
      "    \"gradnorm\": 5.96875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:23.307438\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4821 num samples 32 - rank: 0 num_loss_counted_tokens: 1846\u001b[0m\n",
      "Epoch: 8, Step: 875, Rank: 0, loss = 0.0458984375\n",
      "Epoch 8:  41%|████▏     | 43/104 [00:39<00:54,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 875,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.93371902212112,\n",
      "    \"lr\": 1.3556561694207337e-06,\n",
      "    \"cuda_mem_allocated\": 25.108776569366455,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1914,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04597701149425287,\n",
      "    \"samples_seen\": 27406,\n",
      "    \"gradnorm\": 5.96875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:24.170397\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4724 num samples 31 - rank: 0 num_loss_counted_tokens: 1590\u001b[0m\n",
      "Epoch: 8, Step: 876, Rank: 0, loss = 0.046630859375\n",
      "Epoch 8:  42%|████▏     | 44/104 [00:40<00:55,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 876,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.34608683070434,\n",
      "    \"lr\": 1.3239229635525074e-06,\n",
      "    \"cuda_mem_allocated\": 19.105127334594727,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1723,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.04672083575159605,\n",
      "    \"samples_seen\": 27439,\n",
      "    \"gradnorm\": 6.21875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:25.135498\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4744 num samples 30 - rank: 0 num_loss_counted_tokens: 1824\u001b[0m\n",
      "Epoch: 8, Step: 877, Rank: 0, loss = 0.0390625\n",
      "Epoch 8:  43%|████▎     | 45/104 [00:41<00:53,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 877,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.39144672834439,\n",
      "    \"lr\": 1.3239229635525074e-06,\n",
      "    \"cuda_mem_allocated\": 25.091779232025146,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1835,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.03896457765667575,\n",
      "    \"samples_seen\": 27470,\n",
      "    \"gradnorm\": 6.21875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:25.990573\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4765 num samples 30 - rank: 0 num_loss_counted_tokens: 1723\u001b[0m\n",
      "Epoch: 8, Step: 878, Rank: 0, loss = 0.04541015625\n",
      "Epoch 8:  44%|████▍     | 46/104 [00:42<00:53,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 878,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.325014273124616,\n",
      "    \"lr\": 1.2925392288022299e-06,\n",
      "    \"cuda_mem_allocated\": 19.118534564971924,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1809,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.04532891100055279,\n",
      "    \"samples_seen\": 27503,\n",
      "    \"gradnorm\": 5.84375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:26.955210\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4727 num samples 31 - rank: 0 num_loss_counted_tokens: 1603\u001b[0m\n",
      "Epoch: 8, Step: 879, Rank: 0, loss = 0.057373046875\n",
      "Epoch 8:  45%|████▌     | 47/104 [00:43<00:51,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 879,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.17182896020995,\n",
      "    \"lr\": 1.2925392288022299e-06,\n",
      "    \"cuda_mem_allocated\": 25.102792739868164,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1851,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.05726634251755808,\n",
      "    \"samples_seen\": 27535,\n",
      "    \"gradnorm\": 5.84375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:27.814979\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4823 num samples 32 - rank: 0 num_loss_counted_tokens: 1630\u001b[0m\n",
      "Epoch: 8, Step: 880, Rank: 0, loss = 0.05859375\n",
      "Epoch 8:  46%|████▌     | 48/104 [00:43<00:51,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 880,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.46294059195909,\n",
      "    \"lr\": 1.2615062293021508e-06,\n",
      "    \"cuda_mem_allocated\": 19.1060848236084,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1763,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.05870674985819626,\n",
      "    \"samples_seen\": 27566,\n",
      "    \"gradnorm\": 5.96875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:28.775525\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4775 num samples 32 - rank: 0 num_loss_counted_tokens: 1768\u001b[0m\n",
      "Epoch: 8, Step: 881, Rank: 0, loss = 0.046142578125\n",
      "Epoch 8:  47%|████▋     | 49/104 [00:44<00:49,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 881,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.56833406592888,\n",
      "    \"lr\": 1.2615062293021508e-06,\n",
      "    \"cuda_mem_allocated\": 25.088428020477295,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1596,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.046052631578947366,\n",
      "    \"samples_seen\": 27598,\n",
      "    \"gradnorm\": 5.96875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:29.628653\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4814 num samples 33 - rank: 0 num_loss_counted_tokens: 1758\u001b[0m\n",
      "Epoch: 8, Step: 882, Rank: 0, loss = 0.047607421875\n",
      "Epoch 8:  48%|████▊     | 50/104 [00:45<00:49,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 882,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.69184722383697,\n",
      "    \"lr\": 1.230825215056971e-06,\n",
      "    \"cuda_mem_allocated\": 19.107521057128906,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1789,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.047512576858580215,\n",
      "    \"samples_seen\": 27629,\n",
      "    \"gradnorm\": 6.5,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:30.584688\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4766 num samples 32 - rank: 0 num_loss_counted_tokens: 1776\u001b[0m\n",
      "Epoch: 8, Step: 883, Rank: 0, loss = 0.049560546875\n",
      "Epoch 8:  49%|████▉     | 51/104 [00:46<00:47,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 883,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.52219443470262,\n",
      "    \"lr\": 1.230825215056971e-06,\n",
      "    \"cuda_mem_allocated\": 25.098482131958008,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1734,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.049596309111880045,\n",
      "    \"samples_seen\": 27661,\n",
      "    \"gradnorm\": 6.5,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:31.435949\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4820 num samples 32 - rank: 0 num_loss_counted_tokens: 1841\u001b[0m\n",
      "Epoch: 8, Step: 884, Rank: 0, loss = 0.053466796875\n",
      "Epoch 8:  50%|█████     | 52/104 [00:47<00:47,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 884,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.67725635761829,\n",
      "    \"lr\": 1.2004974218934695e-06,\n",
      "    \"cuda_mem_allocated\": 19.093156337738037,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1825,\n",
      "    \"batch_size\": 27,\n",
      "    \"total_loss\": 0.05342465753424658,\n",
      "    \"samples_seen\": 27688,\n",
      "    \"gradnorm\": 5.5625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:32.392646\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4791 num samples 32 - rank: 0 num_loss_counted_tokens: 1792\u001b[0m\n",
      "Epoch: 8, Step: 885, Rank: 0, loss = 0.0576171875\n",
      "Epoch 8:  51%|█████     | 53/104 [00:48<00:45,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 885,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.726170701704746,\n",
      "    \"lr\": 1.2004974218934695e-06,\n",
      "    \"cuda_mem_allocated\": 25.076695919036865,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1715,\n",
      "    \"batch_size\": 29,\n",
      "    \"total_loss\": 0.05772594752186589,\n",
      "    \"samples_seen\": 27717,\n",
      "    \"gradnorm\": 5.5625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:33.248322\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4805 num samples 31 - rank: 0 num_loss_counted_tokens: 1745\u001b[0m\n",
      "Epoch: 8, Step: 886, Rank: 0, loss = 0.038818359375\n",
      "Epoch 8:  52%|█████▏    | 54/104 [00:49<00:45,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 886,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.4533647208,\n",
      "    \"lr\": 1.1705240714107301e-06,\n",
      "    \"cuda_mem_allocated\": 19.108478546142578,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1638,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.03891941391941392,\n",
      "    \"samples_seen\": 27749,\n",
      "    \"gradnorm\": 5.96875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:34.209704\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4827 num samples 32 - rank: 0 num_loss_counted_tokens: 1714\u001b[0m\n",
      "Epoch: 8, Step: 887, Rank: 0, loss = 0.06640625\n",
      "Epoch 8:  53%|█████▎    | 55/104 [00:50<00:44,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 887,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.63655188645953,\n",
      "    \"lr\": 1.1705240714107301e-06,\n",
      "    \"cuda_mem_allocated\": 25.083159923553467,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1796,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.06625835189309577,\n",
      "    \"samples_seen\": 27780,\n",
      "    \"gradnorm\": 5.96875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:35.067461\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4795 num samples 28 - rank: 0 num_loss_counted_tokens: 1760\u001b[0m\n",
      "Epoch: 8, Step: 888, Rank: 0, loss = 0.041015625\n",
      "Epoch 8:  54%|█████▍    | 56/104 [00:51<00:44,  1.08it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 888,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.114829785168915,\n",
      "    \"lr\": 1.1409063709309442e-06,\n",
      "    \"cuda_mem_allocated\": 19.126673698425293,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1764,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04109977324263039,\n",
      "    \"samples_seen\": 27812,\n",
      "    \"gradnorm\": 6.625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:36.044390\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4803 num samples 30 - rank: 0 num_loss_counted_tokens: 1749\u001b[0m\n",
      "Epoch: 8, Step: 889, Rank: 0, loss = 0.043212890625\n",
      "Epoch 8:  55%|█████▍    | 57/104 [00:52<00:42,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 889,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.47009079862561,\n",
      "    \"lr\": 1.1409063709309442e-06,\n",
      "    \"cuda_mem_allocated\": 25.10207223892212,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1751,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.043118218161050825,\n",
      "    \"samples_seen\": 27843,\n",
      "    \"gradnorm\": 6.625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:36.897984\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4740 num samples 30 - rank: 0 num_loss_counted_tokens: 1825\u001b[0m\n",
      "Epoch: 8, Step: 890, Rank: 0, loss = 0.058349609375\n",
      "Epoch 8:  56%|█████▌    | 58/104 [00:53<00:42,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 890,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.38993864347051,\n",
      "    \"lr\": 1.1116455134507665e-06,\n",
      "    \"cuda_mem_allocated\": 19.113505363464355,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1831,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.05843801201529219,\n",
      "    \"samples_seen\": 27875,\n",
      "    \"gradnorm\": 5.5625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:37.860516\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4778 num samples 32 - rank: 0 num_loss_counted_tokens: 1733\u001b[0m\n",
      "Epoch: 8, Step: 891, Rank: 0, loss = 0.05126953125\n",
      "Epoch 8:  57%|█████▋    | 59/104 [00:53<00:40,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 891,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.39765594362242,\n",
      "    \"lr\": 1.1116455134507665e-06,\n",
      "    \"cuda_mem_allocated\": 25.107101440429688,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1846,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.05119176598049838,\n",
      "    \"samples_seen\": 27907,\n",
      "    \"gradnorm\": 5.5625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:38.713471\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4855 num samples 32 - rank: 0 num_loss_counted_tokens: 1727\u001b[0m\n",
      "Epoch: 8, Step: 892, Rank: 0, loss = 0.048828125\n",
      "Epoch 8:  58%|█████▊    | 60/104 [00:54<00:40,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 892,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.61685483353377,\n",
      "    \"lr\": 1.0827426775932658e-06,\n",
      "    \"cuda_mem_allocated\": 19.099619388580322,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1590,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.04874213836477988,\n",
      "    \"samples_seen\": 27938,\n",
      "    \"gradnorm\": 6.375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:39.671224\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4780 num samples 31 - rank: 0 num_loss_counted_tokens: 1663\u001b[0m\n",
      "Epoch: 8, Step: 893, Rank: 0, loss = 0.049072265625\n",
      "Epoch 8:  59%|█████▊    | 61/104 [00:55<00:38,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 893,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.54128970656991,\n",
      "    \"lr\": 1.0827426775932658e-06,\n",
      "    \"cuda_mem_allocated\": 25.088666915893555,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1824,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.04906798245614035,\n",
      "    \"samples_seen\": 27968,\n",
      "    \"gradnorm\": 6.375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:40.521684\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4763 num samples 32 - rank: 0 num_loss_counted_tokens: 1753\u001b[0m\n",
      "Epoch: 8, Step: 894, Rank: 0, loss = 0.04443359375\n",
      "Epoch 8:  60%|█████▉    | 62/104 [00:56<00:38,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 894,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.37686076396344,\n",
      "    \"lr\": 1.054199027560463e-06,\n",
      "    \"cuda_mem_allocated\": 19.10943603515625,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1723,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.044399303540336624,\n",
      "    \"samples_seen\": 27998,\n",
      "    \"gradnorm\": 6.0625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:41.484582\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4835 num samples 32 - rank: 0 num_loss_counted_tokens: 1757\u001b[0m\n",
      "Epoch: 8, Step: 895, Rank: 0, loss = 0.045166015625\n",
      "Epoch 8:  61%|██████    | 63/104 [00:57<00:36,  1.12it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 895,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.698158745632895,\n",
      "    \"lr\": 1.054199027560463e-06,\n",
      "    \"cuda_mem_allocated\": 25.084596157073975,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1603,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.04522769806612601,\n",
      "    \"samples_seen\": 28029,\n",
      "    \"gradnorm\": 6.0625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:42.330903\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4821 num samples 32 - rank: 0 num_loss_counted_tokens: 1633\u001b[0m\n",
      "Epoch: 8, Step: 896, Rank: 0, loss = 0.06005859375\n",
      "Epoch 8:  62%|██████▏   | 64/104 [00:58<00:36,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 896,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.24279036521417,\n",
      "    \"lr\": 1.0260157130864178e-06,\n",
      "    \"cuda_mem_allocated\": 19.123322010040283,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1630,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.06012269938650307,\n",
      "    \"samples_seen\": 28061,\n",
      "    \"gradnorm\": 6.875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:43.298395\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4825 num samples 31 - rank: 0 num_loss_counted_tokens: 1746\u001b[0m\n",
      "Epoch: 8, Step: 897, Rank: 0, loss = 0.04638671875\n",
      "Epoch 8:  62%|██████▎   | 65/104 [00:59<00:35,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 897,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.54329345055694,\n",
      "    \"lr\": 1.0260157130864178e-06,\n",
      "    \"cuda_mem_allocated\": 25.096088409423828,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1768,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04638009049773756,\n",
      "    \"samples_seen\": 28093,\n",
      "    \"gradnorm\": 6.875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:44.149737\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4729 num samples 31 - rank: 0 num_loss_counted_tokens: 1746\u001b[0m\n",
      "Epoch: 8, Step: 898, Rank: 0, loss = 0.052734375\n",
      "Epoch 8:  63%|██████▎   | 66/104 [01:00<00:34,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 898,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.39775225171883,\n",
      "    \"lr\": 9.981938693909221e-07,\n",
      "    \"cuda_mem_allocated\": 19.121167182922363,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1758,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.05261660978384528,\n",
      "    \"samples_seen\": 28126,\n",
      "    \"gradnorm\": 6.3125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:45.112014\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4727 num samples 31 - rank: 0 num_loss_counted_tokens: 1626\u001b[0m\n",
      "Epoch: 8, Step: 899, Rank: 0, loss = 0.045654296875\n",
      "Epoch 8:  64%|██████▍   | 67/104 [01:01<00:33,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 899,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.53977538364277,\n",
      "    \"lr\": 9.981938693909221e-07,\n",
      "    \"cuda_mem_allocated\": 25.09393310546875,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1776,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04560810810810811,\n",
      "    \"samples_seen\": 28158,\n",
      "    \"gradnorm\": 6.3125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:45.962169\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4708 num samples 32 - rank: 0 num_loss_counted_tokens: 1677\u001b[0m\n",
      "Epoch: 8, Step: 900, Rank: 0, loss = 0.0498046875\n",
      "Epoch 8:  65%|██████▌   | 68/104 [01:02<00:33,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 900,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.3222534435798,\n",
      "    \"lr\": 9.707346171337895e-07,\n",
      "    \"cuda_mem_allocated\": 19.12260341644287,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1841,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.049701249321021185,\n",
      "    \"samples_seen\": 28190,\n",
      "    \"gradnorm\": 5.90625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:46.926587\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4842 num samples 34 - rank: 0 num_loss_counted_tokens: 1661\u001b[0m\n",
      "Epoch: 8, Step: 901, Rank: 0, loss = 0.042236328125\n",
      "Epoch 8:  66%|██████▋   | 69/104 [01:02<00:31,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 901,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.21418606397379,\n",
      "    \"lr\": 9.707346171337895e-07,\n",
      "    \"cuda_mem_allocated\": 25.099918365478516,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1792,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04213169642857143,\n",
      "    \"samples_seen\": 28222,\n",
      "    \"gradnorm\": 5.90625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:47.784311\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4753 num samples 29 - rank: 0 num_loss_counted_tokens: 1609\u001b[0m\n",
      "Epoch: 8, Step: 902, Rank: 0, loss = 0.050048828125\n",
      "Epoch 8:  67%|██████▋   | 70/104 [01:03<00:31,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 902,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 36.96614876826994,\n",
      "    \"lr\": 9.436390623696911e-07,\n",
      "    \"cuda_mem_allocated\": 19.11901330947876,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1745,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.050143266475644696,\n",
      "    \"samples_seen\": 28253,\n",
      "    \"gradnorm\": 5.75,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:48.756885\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4854 num samples 33 - rank: 0 num_loss_counted_tokens: 1641\u001b[0m\n",
      "Epoch: 8, Step: 903, Rank: 0, loss = 0.049560546875\n",
      "Epoch 8:  68%|██████▊   | 71/104 [01:04<00:30,  1.10it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 903,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.93504806120133,\n",
      "    \"lr\": 9.436390623696911e-07,\n",
      "    \"cuda_mem_allocated\": 25.108537673950195,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1714,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.049591598599766626,\n",
      "    \"samples_seen\": 28285,\n",
      "    \"gradnorm\": 5.75,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:49.643913\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4851 num samples 31 - rank: 0 num_loss_counted_tokens: 1820\u001b[0m\n",
      "Epoch: 8, Step: 904, Rank: 0, loss = 0.0517578125\n",
      "Epoch 8:  69%|██████▉   | 72/104 [01:05<00:29,  1.07it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 904,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 36.77127507143699,\n",
      "    \"lr\": 9.16908296503628e-07,\n",
      "    \"cuda_mem_allocated\": 19.116617679595947,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1760,\n",
      "    \"batch_size\": 28,\n",
      "    \"total_loss\": 0.051704545454545454,\n",
      "    \"samples_seen\": 28313,\n",
      "    \"gradnorm\": 5.9375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:50.622309\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4729 num samples 32 - rank: 0 num_loss_counted_tokens: 1683\u001b[0m\n",
      "Epoch: 8, Step: 905, Rank: 0, loss = 0.044921875\n",
      "Epoch 8:  70%|███████   | 73/104 [01:06<00:28,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 905,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.35183417279991,\n",
      "    \"lr\": 9.16908296503628e-07,\n",
      "    \"cuda_mem_allocated\": 25.102792739868164,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1749,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.04488279016580903,\n",
      "    \"samples_seen\": 28343,\n",
      "    \"gradnorm\": 5.9375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:51.498966\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4821 num samples 31 - rank: 0 num_loss_counted_tokens: 1870\u001b[0m\n",
      "Epoch: 8, Step: 906, Rank: 0, loss = 0.04541015625\n",
      "Epoch 8:  71%|███████   | 74/104 [01:07<00:27,  1.07it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 906,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.06590784566712,\n",
      "    \"lr\": 8.905433962469489e-07,\n",
      "    \"cuda_mem_allocated\": 19.103451251983643,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1825,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.04547945205479452,\n",
      "    \"samples_seen\": 28373,\n",
      "    \"gradnorm\": 6.125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:52.469896\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4808 num samples 32 - rank: 0 num_loss_counted_tokens: 1943\u001b[0m\n",
      "Epoch: 8, Step: 907, Rank: 0, loss = 0.044189453125\n",
      "Epoch 8:  72%|███████▏  | 75/104 [01:08<00:26,  1.10it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 907,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.94677908863226,\n",
      "    \"lr\": 8.905433962469489e-07,\n",
      "    \"cuda_mem_allocated\": 25.096806049346924,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1733,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04414310444316215,\n",
      "    \"samples_seen\": 28405,\n",
      "    \"gradnorm\": 6.125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:53.332993\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4736 num samples 32 - rank: 0 num_loss_counted_tokens: 1772\u001b[0m\n",
      "Epoch: 8, Step: 908, Rank: 0, loss = 0.0634765625\n",
      "Epoch 8:  73%|███████▎  | 76/104 [01:09<00:26,  1.08it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 908,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.09097280197999,\n",
      "    \"lr\": 8.645454235739903e-07,\n",
      "    \"cuda_mem_allocated\": 19.130982398986816,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1727,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.06340474811812391,\n",
      "    \"samples_seen\": 28437,\n",
      "    \"gradnorm\": 6.03125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:54.303599\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4754 num samples 30 - rank: 0 num_loss_counted_tokens: 1761\u001b[0m\n",
      "Epoch: 8, Step: 909, Rank: 0, loss = 0.0341796875\n",
      "Epoch 8:  74%|███████▍  | 77/104 [01:10<00:24,  1.10it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 909,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.13935356277085,\n",
      "    \"lr\": 8.645454235739903e-07,\n",
      "    \"cuda_mem_allocated\": 25.09728479385376,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1663,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.034275405892964524,\n",
      "    \"samples_seen\": 28468,\n",
      "    \"gradnorm\": 6.03125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:55.163296\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4662 num samples 32 - rank: 0 num_loss_counted_tokens: 1800\u001b[0m\n",
      "Epoch: 8, Step: 910, Rank: 0, loss = 0.033447265625\n",
      "Epoch 8:  75%|███████▌  | 78/104 [01:11<00:24,  1.08it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 910,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.361337459425016,\n",
      "    \"lr\": 8.389154256793042e-07,\n",
      "    \"cuda_mem_allocated\": 19.108957290649414,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1753,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.033371363377067885,\n",
      "    \"samples_seen\": 28500,\n",
      "    \"gradnorm\": 5.78125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:56.127028\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4813 num samples 32 - rank: 0 num_loss_counted_tokens: 1678\u001b[0m\n",
      "Epoch: 8, Step: 911, Rank: 0, loss = 0.037353515625\n",
      "Epoch 8:  76%|███████▌  | 79/104 [01:12<00:22,  1.10it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 911,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.14808680977068,\n",
      "    \"lr\": 8.389154256793042e-07,\n",
      "    \"cuda_mem_allocated\": 25.110453128814697,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1757,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.03727945361411497,\n",
      "    \"samples_seen\": 28532,\n",
      "    \"gradnorm\": 5.78125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:56.991863\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4787 num samples 32 - rank: 0 num_loss_counted_tokens: 1650\u001b[0m\n",
      "Epoch: 8, Step: 912, Rank: 0, loss = 0.044921875\n",
      "Epoch 8:  77%|███████▋  | 80/104 [01:13<00:22,  1.08it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 912,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.149654861076705,\n",
      "    \"lr\": 8.136544349354669e-07,\n",
      "    \"cuda_mem_allocated\": 19.122843265533447,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1633,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04500918554807103,\n",
      "    \"samples_seen\": 28564,\n",
      "    \"gradnorm\": 5.53125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:57.960479\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4719 num samples 32 - rank: 0 num_loss_counted_tokens: 1626\u001b[0m\n",
      "Epoch: 8, Step: 913, Rank: 0, loss = 0.03662109375\n",
      "Epoch 8:  78%|███████▊  | 81/104 [01:14<00:20,  1.10it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 913,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.02762744530166,\n",
      "    \"lr\": 8.136544349354669e-07,\n",
      "    \"cuda_mem_allocated\": 25.10805892944336,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1746,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.03651202749140894,\n",
      "    \"samples_seen\": 28595,\n",
      "    \"gradnorm\": 5.53125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:58.821349\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4779 num samples 32 - rank: 0 num_loss_counted_tokens: 1710\u001b[0m\n",
      "Epoch: 8, Step: 914, Rank: 0, loss = 0.056640625\n",
      "Epoch 8:  79%|███████▉  | 82/104 [01:14<00:20,  1.08it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 914,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.44252982104444,\n",
      "    \"lr\": 7.887634688515e-07,\n",
      "    \"cuda_mem_allocated\": 19.10081672668457,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1746,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.05670103092783505,\n",
      "    \"samples_seen\": 28626,\n",
      "    \"gradnorm\": 6.375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:13:59.782991\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4762 num samples 32 - rank: 0 num_loss_counted_tokens: 1719\u001b[0m\n",
      "Epoch: 8, Step: 915, Rank: 0, loss = 0.04248046875\n",
      "Epoch 8:  80%|███████▉  | 83/104 [01:15<00:18,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 915,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.54783757591401,\n",
      "    \"lr\": 7.887634688515e-07,\n",
      "    \"cuda_mem_allocated\": 25.084596157073975,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1626,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.042435424354243544,\n",
      "    \"samples_seen\": 28657,\n",
      "    \"gradnorm\": 6.375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:00.633911\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4841 num samples 33 - rank: 0 num_loss_counted_tokens: 1843\u001b[0m\n",
      "Epoch: 8, Step: 916, Rank: 0, loss = 0.060546875\n",
      "Epoch 8:  81%|████████  | 84/104 [01:16<00:18,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 916,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.445036858402524,\n",
      "    \"lr\": 7.642435300318906e-07,\n",
      "    \"cuda_mem_allocated\": 19.095789432525635,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1677,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.0605247465712582,\n",
      "    \"samples_seen\": 28689,\n",
      "    \"gradnorm\": 6.5625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:01.595112\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4828 num samples 33 - rank: 0 num_loss_counted_tokens: 1742\u001b[0m\n",
      "Epoch: 8, Step: 917, Rank: 0, loss = 0.055419921875\n",
      "Epoch 8:  82%|████████▏ | 85/104 [01:17<00:17,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 917,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.07268714131091,\n",
      "    \"lr\": 7.642435300318906e-07,\n",
      "    \"cuda_mem_allocated\": 25.112128257751465,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1661,\n",
      "    \"batch_size\": 34,\n",
      "    \"total_loss\": 0.055388320288982544,\n",
      "    \"samples_seen\": 28723,\n",
      "    \"gradnorm\": 6.5625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:02.455210\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4653 num samples 30 - rank: 0 num_loss_counted_tokens: 1661\u001b[0m\n",
      "Epoch: 8, Step: 918, Rank: 0, loss = 0.048583984375\n",
      "Epoch 8:  83%|████████▎ | 86/104 [01:18<00:16,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 918,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.40304652441453,\n",
      "    \"lr\": 7.400956061361975e-07,\n",
      "    \"cuda_mem_allocated\": 19.106563568115234,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1609,\n",
      "    \"batch_size\": 29,\n",
      "    \"total_loss\": 0.048477315102548164,\n",
      "    \"samples_seen\": 28752,\n",
      "    \"gradnorm\": 6.21875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:03.417385\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4742 num samples 31 - rank: 0 num_loss_counted_tokens: 1772\u001b[0m\n",
      "Epoch: 8, Step: 919, Rank: 0, loss = 0.052490234375\n",
      "Epoch 8:  84%|████████▎ | 87/104 [01:19<00:15,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 919,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.84274557050055,\n",
      "    \"lr\": 7.400956061361975e-07,\n",
      "    \"cuda_mem_allocated\": 25.11500072479248,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1641,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.05240706886045095,\n",
      "    \"samples_seen\": 28785,\n",
      "    \"gradnorm\": 6.21875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:04.282435\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 3063 num samples 21 - rank: 0 num_loss_counted_tokens: 1165\u001b[0m\n",
      "Epoch: 8, Step: 920, Rank: 0, loss = 0.047607421875\n",
      "Epoch 8:  85%|████████▍ | 88/104 [01:20<00:14,  1.08it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 920,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.021903479930266,\n",
      "    \"lr\": 7.163206698392744e-07,\n",
      "    \"cuda_mem_allocated\": 19.130024909973145,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1820,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.047527472527472525,\n",
      "    \"samples_seen\": 28816,\n",
      "    \"gradnorm\": 5.71875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:05.254050\"\n",
      "}\u001b[0m\n",
      "Epoch: 8, Step: 921, Rank: 0, loss = 0.03857421875\n",
      "Epoch 8:  86%|████████▌ | 89/104 [01:21<00:13,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 921,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.415774447033314,\n",
      "    \"lr\": 7.163206698392744e-07,\n",
      "    \"cuda_mem_allocated\": 25.08507490158081,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1683,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.038621509209744505,\n",
      "    \"samples_seen\": 28848,\n",
      "    \"gradnorm\": 5.71875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:06.107876\"\n",
      "}\u001b[0m\n",
      "Epoch: 8, Step: 922, Rank: 0, loss = 0.04443359375\n",
      "Epoch 8:  87%|████████▋ | 90/104 [01:22<00:12,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 922,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.26225973250786,\n",
      "    \"lr\": 6.9291967879209e-07,\n",
      "    \"cuda_mem_allocated\": 19.122843265533447,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1870,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.04438502673796792,\n",
      "    \"samples_seen\": 28879,\n",
      "    \"gradnorm\": 5.59375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:07.072745\"\n",
      "}\u001b[0m\n",
      "Epoch: 8, Step: 923, Rank: 0, loss = 0.044189453125\n",
      "Epoch 8:  88%|████████▊ | 91/104 [01:23<00:11,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 923,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.18202932600631,\n",
      "    \"lr\": 6.9291967879209e-07,\n",
      "    \"cuda_mem_allocated\": 25.103989124298096,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1943,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.0442614513638703,\n",
      "    \"samples_seen\": 28911,\n",
      "    \"gradnorm\": 5.59375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:07.934103\"\n",
      "}\u001b[0m\n",
      "Epoch: 8, Step: 924, Rank: 0, loss = 0.04150390625\n",
      "Epoch 8:  88%|████████▊ | 92/104 [01:24<00:11,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 924,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.418917475978525,\n",
      "    \"lr\": 6.698935755831493e-07,\n",
      "    \"cuda_mem_allocated\": 19.102491855621338,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1772,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04147855530474041,\n",
      "    \"samples_seen\": 28943,\n",
      "    \"gradnorm\": 5.28125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:08.896472\"\n",
      "}\u001b[0m\n",
      "Epoch: 8, Step: 925, Rank: 0, loss = 0.042236328125\n",
      "Epoch 8:  89%|████████▉ | 93/104 [01:24<00:09,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 925,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.29870389966158,\n",
      "    \"lr\": 6.698935755831493e-07,\n",
      "    \"cuda_mem_allocated\": 25.091060638427734,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1761,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.04230550823395798,\n",
      "    \"samples_seen\": 28973,\n",
      "    \"gradnorm\": 5.28125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:09.751681\"\n",
      "}\u001b[0m\n",
      "Epoch: 8, Step: 926, Rank: 0, loss = 0.049072265625\n",
      "Epoch 8:  90%|█████████ | 94/104 [01:25<00:09,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 926,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.607442488107026,\n",
      "    \"lr\": 6.472432877005341e-07,\n",
      "    \"cuda_mem_allocated\": 19.084775924682617,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1800,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.049166666666666664,\n",
      "    \"samples_seen\": 29005,\n",
      "    \"gradnorm\": 6.0,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:10.709390\"\n",
      "}\u001b[0m\n",
      "Epoch: 8, Step: 927, Rank: 0, loss = 0.059814453125\n",
      "Epoch 8:  91%|█████████▏| 95/104 [01:26<00:08,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 927,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.27383090196446,\n",
      "    \"lr\": 6.472432877005341e-07,\n",
      "    \"cuda_mem_allocated\": 25.105186462402344,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1678,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.0598927294398093,\n",
      "    \"samples_seen\": 29037,\n",
      "    \"gradnorm\": 6.0,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:11.565169\"\n",
      "}\u001b[0m\n",
      "Epoch: 8, Step: 928, Rank: 0, loss = 0.041015625\n",
      "Epoch 8:  92%|█████████▏| 96/104 [01:27<00:07,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 928,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.38401289717356,\n",
      "    \"lr\": 6.249697274945377e-07,\n",
      "    \"cuda_mem_allocated\": 19.114702701568604,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1650,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04090909090909091,\n",
      "    \"samples_seen\": 29069,\n",
      "    \"gradnorm\": 6.5,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:12.527404\"\n",
      "}\u001b[0m\n",
      "Epoch: 8, Step: 929, Rank: 0, loss = 0.044921875\n",
      "Epoch 8:  93%|█████████▎| 97/104 [01:28<00:06,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 929,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.64465359668636,\n",
      "    \"lr\": 6.249697274945377e-07,\n",
      "    \"cuda_mem_allocated\": 25.08268117904663,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1626,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04489544895448955,\n",
      "    \"samples_seen\": 29101,\n",
      "    \"gradnorm\": 6.5,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:13.375403\"\n",
      "}\u001b[0m\n",
      "Epoch: 8, Step: 930, Rank: 0, loss = 0.07470703125\n",
      "Epoch 8:  94%|█████████▍| 98/104 [01:29<00:05,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 930,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.407987808056035,\n",
      "    \"lr\": 6.030737921409169e-07,\n",
      "    \"cuda_mem_allocated\": 19.11278772354126,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1710,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.07485380116959064,\n",
      "    \"samples_seen\": 29133,\n",
      "    \"gradnorm\": 6.90625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:14.337374\"\n",
      "}\u001b[0m\n",
      "Epoch: 8, Step: 931, Rank: 0, loss = 0.049072265625\n",
      "Epoch 8:  95%|█████████▌| 99/104 [01:30<00:04,  1.12it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 931,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.58002587401472,\n",
      "    \"lr\": 6.030737921409169e-07,\n",
      "    \"cuda_mem_allocated\": 25.092975616455078,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1719,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.0491564863292612,\n",
      "    \"samples_seen\": 29165,\n",
      "    \"gradnorm\": 6.90625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:15.186107\"\n",
      "}\u001b[0m\n",
      "Epoch: 8, Step: 932, Rank: 0, loss = 0.038818359375\n",
      "Epoch 8:  96%|█████████▌| 100/104 [01:31<00:03,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 932,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.18867685091749,\n",
      "    \"lr\": 5.815563636047539e-07,\n",
      "    \"cuda_mem_allocated\": 19.127631187438965,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1843,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.03879544221378188,\n",
      "    \"samples_seen\": 29198,\n",
      "    \"gradnorm\": 6.125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:16.153218\"\n",
      "}\u001b[0m\n",
      "Epoch: 8, Step: 933, Rank: 0, loss = 0.044677734375\n",
      "Epoch 8:  97%|█████████▋| 101/104 [01:32<00:02,  1.11it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 933,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.22107301929059,\n",
      "    \"lr\": 5.815563636047539e-07,\n",
      "    \"cuda_mem_allocated\": 25.108776569366455,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1742,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.04477611940298507,\n",
      "    \"samples_seen\": 29231,\n",
      "    \"gradnorm\": 6.125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:17.009522\"\n",
      "}\u001b[0m\n",
      "Epoch: 8, Step: 934, Rank: 0, loss = 0.052490234375\n",
      "Epoch 8:  98%|█████████▊| 102/104 [01:33<00:01,  1.09it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 934,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.84342616530092,\n",
      "    \"lr\": 5.604183086049342e-07,\n",
      "    \"cuda_mem_allocated\": 19.082622051239014,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1661,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.052378085490668275,\n",
      "    \"samples_seen\": 29261,\n",
      "    \"gradnorm\": 5.75,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:17.961985\"\n",
      "}\u001b[0m\n",
      "Epoch: 8, Step: 935, Rank: 0, loss = 0.06884765625\n",
      "Epoch 8:  99%|█████████▉| 103/104 [01:34<00:00,  1.12it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 935,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.47658027378775,\n",
      "    \"lr\": 5.604183086049342e-07,\n",
      "    \"cuda_mem_allocated\": 25.08818817138672,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1772,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.06884875846501129,\n",
      "    \"samples_seen\": 29292,\n",
      "    \"gradnorm\": 5.75,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:18.813643\"\n",
      "}\u001b[0m\n",
      "Epoch: 8, Step: 936, Rank: 0, loss = 0.039794921875\n",
      "Epoch 8: 100%|██████████| 104/104 [01:34<00:00,  1.20it/s]\u001b[92m{\n",
      "    \"epoch\": 8,\n",
      "    \"step\": 936,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 54.173394931622184,\n",
      "    \"lr\": 5.396604785792281e-07,\n",
      "    \"cuda_mem_allocated\": 18.701960563659668,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1165,\n",
      "    \"batch_size\": 21,\n",
      "    \"total_loss\": 0.039914163090128754,\n",
      "    \"samples_seen\": 29313,\n",
      "    \"gradnorm\": 6.5625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:19.510702\"\n",
      "}\u001b[0m\n",
      "\u001b[93mSaving model in huggingface format at: samples_29313\u001b[0m\n",
      "[10:14:44] INFO     The model is bigger than the maximum     accelerator.py:2924\n",
      "                    size per checkpoint (5GB) and is going                      \n",
      "                    to be split in 2 checkpoint shards. You                     \n",
      "                    can find where each parameters has been                     \n",
      "                    saved in the index located at                               \n",
      "                    /root/.local/share/instructlab/checkpoin                    \n",
      "                    ts/hf_format/samples_29313/model.safeten                    \n",
      "                    sors.index.json.                                            \n",
      "\u001b[93mModel saved in /root/.local/share/instructlab/checkpoints/hf_format/samples_29313\u001b[0m\n",
      "[10:14:45] INFO     saving took 25.3818576335907 seconds            utils.py:879\n",
      "\n",
      "Epoch 8: 100%|██████████| 104/104 [02:00<00:00,  1.16s/it]\n",
      "\u001b[96m total length: 4742 num samples 32 - rank: 0 num_loss_counted_tokens: 1829\u001b[0m\n",
      "\u001b[96m total length: 4784 num samples 33 - rank: 0 num_loss_counted_tokens: 1674\u001b[0m\n",
      "\u001b[96m total length: 4772 num samples 31 - rank: 0 num_loss_counted_tokens: 1800\u001b[0m\n",
      "\u001b[96m total length: 4744 num samples 33 - rank: 0 num_loss_counted_tokens: 1677\u001b[0m\n",
      "\u001b[96m total length: 4667 num samples 29 - rank: 0 num_loss_counted_tokens: 1663\u001b[0m\n",
      "\u001b[96m total length: 4845 num samples 34 - rank: 0 num_loss_counted_tokens: 1704\u001b[0m\n",
      "\u001b[96m total length: 4753 num samples 32 - rank: 0 num_loss_counted_tokens: 1649\u001b[0m\n",
      "\u001b[96m total length: 4709 num samples 31 - rank: 0 num_loss_counted_tokens: 1646\u001b[0m\n",
      "\u001b[96m total length: 4733 num samples 30 - rank: 0 num_loss_counted_tokens: 1934\u001b[0m\n",
      "\u001b[96m total length: 4822 num samples 33 - rank: 0 num_loss_counted_tokens: 1742\u001b[0m\n",
      "\u001b[96m total length: 4776 num samples 32 - rank: 0 num_loss_counted_tokens: 1764\u001b[0m\n",
      "\u001b[96m total length: 4793 num samples 31 - rank: 0 num_loss_counted_tokens: 1753\u001b[0m\n",
      "\u001b[96m total length: 4731 num samples 30 - rank: 0 num_loss_counted_tokens: 1724\u001b[0m\n",
      "\u001b[96m total length: 4778 num samples 31 - rank: 0 num_loss_counted_tokens: 1880\u001b[0m\n",
      "\u001b[96m total length: 4688 num samples 30 - rank: 0 num_loss_counted_tokens: 1710\u001b[0m\n",
      "\u001b[96m total length: 4799 num samples 33 - rank: 0 num_loss_counted_tokens: 1667\u001b[0m\n",
      "\u001b[96m total length: 4849 num samples 32 - rank: 0 num_loss_counted_tokens: 1787\u001b[0m\n",
      "Epoch: 9, Step: 937, Rank: 0, loss = 0.047119140625\n",
      "\n",
      "Epoch 9:   1%|          | 1/104 [00:01<02:03,  1.20s/it]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 937,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.140279566827765,\n",
      "    \"lr\": 5.396604785792281e-07,\n",
      "    \"cuda_mem_allocated\": 25.08818817138672,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1829,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.047020229633679606,\n",
      "    \"samples_seen\": 29345,\n",
      "    \"gradnorm\": 6.5625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:46.205556\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4847 num samples 33 - rank: 0 num_loss_counted_tokens: 1679\u001b[0m\n",
      "Epoch: 9, Step: 938, Rank: 0, loss = 0.03759765625\n",
      "\n",
      "Epoch 9:   2%|▏         | 2/104 [00:02<01:49,  1.07s/it]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 938,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 36.98814303536641,\n",
      "    \"lr\": 5.192837096500058e-07,\n",
      "    \"cuda_mem_allocated\": 19.128588676452637,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1704,\n",
      "    \"batch_size\": 34,\n",
      "    \"total_loss\": 0.03755868544600939,\n",
      "    \"samples_seen\": 29379,\n",
      "    \"gradnorm\": 5.5,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:47.182957\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4848 num samples 30 - rank: 0 num_loss_counted_tokens: 1867\u001b[0m\n",
      "Epoch: 9, Step: 939, Rank: 0, loss = 0.046875\n",
      "\n",
      "Epoch 9:   3%|▎         | 3/104 [00:03<01:38,  1.03it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 939,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.10207848749132,\n",
      "    \"lr\": 5.192837096500058e-07,\n",
      "    \"cuda_mem_allocated\": 25.09824228286743,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1674,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.0468936678614098,\n",
      "    \"samples_seen\": 29412,\n",
      "    \"gradnorm\": 5.5,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:48.042053\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4747 num samples 32 - rank: 0 num_loss_counted_tokens: 1692\u001b[0m\n",
      "Epoch: 9, Step: 940, Rank: 0, loss = 0.05029296875\n",
      "\n",
      "Epoch 9:   4%|▍         | 4/104 [00:04<01:37,  1.03it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 940,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.48035906021058,\n",
      "    \"lr\": 4.992888225905467e-07,\n",
      "    \"cuda_mem_allocated\": 19.111111640930176,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1800,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.050277777777777775,\n",
      "    \"samples_seen\": 29443,\n",
      "    \"gradnorm\": 6.46875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:49.006941\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4769 num samples 31 - rank: 0 num_loss_counted_tokens: 1821\u001b[0m\n",
      "Epoch: 9, Step: 941, Rank: 0, loss = 0.05712890625\n",
      "\n",
      "Epoch 9:   5%|▍         | 5/104 [00:04<01:31,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 941,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.53075009557402,\n",
      "    \"lr\": 4.992888225905467e-07,\n",
      "    \"cuda_mem_allocated\": 25.088666915893555,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1677,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.057245080500894455,\n",
      "    \"samples_seen\": 29476,\n",
      "    \"gradnorm\": 6.46875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:49.855796\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4714 num samples 31 - rank: 0 num_loss_counted_tokens: 1640\u001b[0m\n",
      "Epoch: 9, Step: 942, Rank: 0, loss = 0.041748046875\n",
      "\n",
      "Epoch 9:   6%|▌         | 6/104 [00:05<01:31,  1.07it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 942,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.897719039366066,\n",
      "    \"lr\": 4.796766227919858e-07,\n",
      "    \"cuda_mem_allocated\": 19.085973262786865,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1663,\n",
      "    \"batch_size\": 29,\n",
      "    \"total_loss\": 0.0417919422730006,\n",
      "    \"samples_seen\": 29505,\n",
      "    \"gradnorm\": 6.125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:50.805131\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4848 num samples 32 - rank: 0 num_loss_counted_tokens: 1804\u001b[0m\n",
      "Epoch: 9, Step: 943, Rank: 0, loss = 0.05224609375\n",
      "\n",
      "Epoch 9:   7%|▋         | 7/104 [00:06<01:27,  1.10it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 943,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.52565117418674,\n",
      "    \"lr\": 4.796766227919858e-07,\n",
      "    \"cuda_mem_allocated\": 25.090821743011475,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1649,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.05215281989084294,\n",
      "    \"samples_seen\": 29537,\n",
      "    \"gradnorm\": 6.125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:51.655608\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4772 num samples 32 - rank: 0 num_loss_counted_tokens: 1764\u001b[0m\n",
      "Epoch: 9, Step: 944, Rank: 0, loss = 0.0439453125\n",
      "\n",
      "Epoch 9:   8%|▊         | 8/104 [00:07<01:28,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 944,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.73178381700197,\n",
      "    \"lr\": 4.6044790023087373e-07,\n",
      "    \"cuda_mem_allocated\": 19.09602928161621,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1646,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.04404617253948967,\n",
      "    \"samples_seen\": 29568,\n",
      "    \"gradnorm\": 6.5625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:52.607778\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4799 num samples 32 - rank: 0 num_loss_counted_tokens: 1828\u001b[0m\n",
      "Epoch: 9, Step: 945, Rank: 0, loss = 0.038818359375\n",
      "\n",
      "Epoch 9:   9%|▊         | 9/104 [00:08<01:25,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 945,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.435902498648424,\n",
      "    \"lr\": 4.6044790023087373e-07,\n",
      "    \"cuda_mem_allocated\": 25.086032390594482,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1934,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.03877973112719752,\n",
      "    \"samples_seen\": 29598,\n",
      "    \"gradnorm\": 6.5625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:53.458298\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4792 num samples 33 - rank: 0 num_loss_counted_tokens: 1824\u001b[0m\n",
      "Epoch: 9, Step: 946, Rank: 0, loss = 0.04833984375\n",
      "\n",
      "Epoch 9:  10%|▉         | 10/104 [00:09<01:26,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 946,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.43009361307192,\n",
      "    \"lr\": 4.4160342943734723e-07,\n",
      "    \"cuda_mem_allocated\": 19.112547874450684,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1880,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.04840425531914894,\n",
      "    \"samples_seen\": 29629,\n",
      "    \"gradnorm\": 5.65625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:54.424328\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4847 num samples 33 - rank: 0 num_loss_counted_tokens: 1858\u001b[0m\n",
      "Epoch: 9, Step: 947, Rank: 0, loss = 0.04833984375\n",
      "\n",
      "Epoch 9:  11%|█         | 11/104 [00:10<01:23,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 947,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.224007299026894,\n",
      "    \"lr\": 4.4160342943734723e-07,\n",
      "    \"cuda_mem_allocated\": 25.107340335845947,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1742,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.04822043628013777,\n",
      "    \"samples_seen\": 29662,\n",
      "    \"gradnorm\": 5.65625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:55.279747\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4712 num samples 31 - rank: 0 num_loss_counted_tokens: 1695\u001b[0m\n",
      "Epoch: 9, Step: 948, Rank: 0, loss = 0.042236328125\n",
      "\n",
      "Epoch 9:  12%|█▏        | 12/104 [00:11<01:24,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 948,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.34891354618104,\n",
      "    \"lr\": 4.2314396946394833e-07,\n",
      "    \"cuda_mem_allocated\": 19.112069129943848,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1764,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04223356009070295,\n",
      "    \"samples_seen\": 29694,\n",
      "    \"gradnorm\": 5.875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:56.240993\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4858 num samples 33 - rank: 0 num_loss_counted_tokens: 1841\u001b[0m\n",
      "Epoch: 9, Step: 949, Rank: 0, loss = 0.0284423828125\n",
      "\n",
      "Epoch 9:  12%|█▎        | 13/104 [00:12<01:22,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 949,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.74116727132814,\n",
      "    \"lr\": 4.2314396946394833e-07,\n",
      "    \"cuda_mem_allocated\": 25.10183334350586,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1667,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.028494301139772044,\n",
      "    \"samples_seen\": 29727,\n",
      "    \"gradnorm\": 5.875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:57.110065\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4742 num samples 30 - rank: 0 num_loss_counted_tokens: 1756\u001b[0m\n",
      "Epoch: 9, Step: 950, Rank: 0, loss = 0.042724609375\n",
      "\n",
      "Epoch 9:  13%|█▎        | 14/104 [00:13<01:22,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 950,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.64325137518549,\n",
      "    \"lr\": 4.0507026385502747e-07,\n",
      "    \"cuda_mem_allocated\": 19.101295471191406,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1724,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.04263341067285383,\n",
      "    \"samples_seen\": 29757,\n",
      "    \"gradnorm\": 4.90625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:58.064913\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4763 num samples 32 - rank: 0 num_loss_counted_tokens: 1796\u001b[0m\n",
      "Epoch: 9, Step: 951, Rank: 0, loss = 0.04443359375\n",
      "\n",
      "Epoch 9:  14%|█▍        | 15/104 [00:13<01:19,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 951,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.57121553675598,\n",
      "    \"lr\": 4.0507026385502747e-07,\n",
      "    \"cuda_mem_allocated\": 25.075259685516357,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1710,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.044444444444444446,\n",
      "    \"samples_seen\": 29787,\n",
      "    \"gradnorm\": 4.90625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:58.912964\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4773 num samples 28 - rank: 0 num_loss_counted_tokens: 1668\u001b[0m\n",
      "Epoch: 9, Step: 952, Rank: 0, loss = 0.06298828125\n",
      "\n",
      "Epoch 9:  15%|█▌        | 16/104 [00:14<01:20,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 952,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.460702995958016,\n",
      "    \"lr\": 3.8738304061681107e-07,\n",
      "    \"cuda_mem_allocated\": 19.11613893508911,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1753,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.06303479749001711,\n",
      "    \"samples_seen\": 29818,\n",
      "    \"gradnorm\": 6.25,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:14:59.871269\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4780 num samples 32 - rank: 0 num_loss_counted_tokens: 1722\u001b[0m\n",
      "Epoch: 9, Step: 953, Rank: 0, loss = 0.049560546875\n",
      "\n",
      "Epoch 9:  16%|█▋        | 17/104 [00:15<01:18,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 953,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.77969541445887,\n",
      "    \"lr\": 3.8738304061681107e-07,\n",
      "    \"cuda_mem_allocated\": 25.11380434036255,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1787,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.049524342473419136,\n",
      "    \"samples_seen\": 29850,\n",
      "    \"gradnorm\": 6.25,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:00.737554\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4849 num samples 33 - rank: 0 num_loss_counted_tokens: 1588\u001b[0m\n",
      "Epoch: 9, Step: 954, Rank: 0, loss = 0.033447265625\n",
      "\n",
      "Epoch 9:  17%|█▋        | 18/104 [00:16<01:19,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 954,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.4377151617224,\n",
      "    \"lr\": 3.7008301218807716e-07,\n",
      "    \"cuda_mem_allocated\": 19.129067420959473,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1679,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.033502084574151283,\n",
      "    \"samples_seen\": 29883,\n",
      "    \"gradnorm\": 5.40625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:01.698914\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4746 num samples 31 - rank: 0 num_loss_counted_tokens: 1729\u001b[0m\n",
      "Epoch: 9, Step: 955, Rank: 0, loss = 0.043701171875\n",
      "\n",
      "Epoch 9:  18%|█▊        | 19/104 [00:17<01:16,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 955,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.207828377693865,\n",
      "    \"lr\": 3.7008301218807716e-07,\n",
      "    \"cuda_mem_allocated\": 25.113564491271973,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1867,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.043652919121585435,\n",
      "    \"samples_seen\": 29913,\n",
      "    \"gradnorm\": 5.40625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:02.555620\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4741 num samples 32 - rank: 0 num_loss_counted_tokens: 1592\u001b[0m\n",
      "Epoch: 9, Step: 956, Rank: 0, loss = 0.04638671875\n",
      "\n",
      "Epoch 9:  19%|█▉        | 20/104 [00:18<01:16,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 956,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.754433379962244,\n",
      "    \"lr\": 3.531708754114438e-07,\n",
      "    \"cuda_mem_allocated\": 19.105127334594727,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1692,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04639479905437352,\n",
      "    \"samples_seen\": 29945,\n",
      "    \"gradnorm\": 6.03125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:03.509995\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4781 num samples 31 - rank: 0 num_loss_counted_tokens: 1721\u001b[0m\n",
      "Epoch: 9, Step: 957, Rank: 0, loss = 0.05322265625\n",
      "\n",
      "Epoch 9:  20%|██        | 21/104 [00:19<01:14,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 957,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.04087551707057,\n",
      "    \"lr\": 3.531708754114438e-07,\n",
      "    \"cuda_mem_allocated\": 25.09465217590332,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1821,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.05326743547501373,\n",
      "    \"samples_seen\": 29976,\n",
      "    \"gradnorm\": 6.03125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:04.369939\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4764 num samples 29 - rank: 0 num_loss_counted_tokens: 1695\u001b[0m\n",
      "Epoch: 9, Step: 958, Rank: 0, loss = 0.03076171875\n",
      "\n",
      "Epoch 9:  21%|██        | 22/104 [00:20<01:15,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 958,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.895215217521425,\n",
      "    \"lr\": 3.3664731150531484e-07,\n",
      "    \"cuda_mem_allocated\": 19.097225666046143,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1640,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.030792682926829268,\n",
      "    \"samples_seen\": 30007,\n",
      "    \"gradnorm\": 5.21875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:05.320665\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4782 num samples 33 - rank: 0 num_loss_counted_tokens: 1819\u001b[0m\n",
      "Epoch: 9, Step: 959, Rank: 0, loss = 0.04150390625\n",
      "\n",
      "Epoch 9:  22%|██▏       | 23/104 [00:21<01:12,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 959,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.28287721068704,\n",
      "    \"lr\": 3.3664731150531484e-07,\n",
      "    \"cuda_mem_allocated\": 25.113564491271973,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1804,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.041574279379157426,\n",
      "    \"samples_seen\": 30039,\n",
      "    \"gradnorm\": 5.21875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:06.175602\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4738 num samples 31 - rank: 0 num_loss_counted_tokens: 1711\u001b[0m\n",
      "Epoch: 9, Step: 960, Rank: 0, loss = 0.038330078125\n",
      "\n",
      "Epoch 9:  23%|██▎       | 24/104 [00:22<01:13,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 960,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.54463865121218,\n",
      "    \"lr\": 3.2051298603643754e-07,\n",
      "    \"cuda_mem_allocated\": 19.111111640930176,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1764,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.03826530612244898,\n",
      "    \"samples_seen\": 30071,\n",
      "    \"gradnorm\": 5.34375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:07.134966\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4851 num samples 33 - rank: 0 num_loss_counted_tokens: 1659\u001b[0m\n",
      "Epoch: 9, Step: 961, Rank: 0, loss = 0.04248046875\n",
      "\n",
      "Epoch 9:  24%|██▍       | 25/104 [00:22<01:11,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 961,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.14816673982887,\n",
      "    \"lr\": 3.2051298603643754e-07,\n",
      "    \"cuda_mem_allocated\": 25.10183334350586,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1828,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04239606126914661,\n",
      "    \"samples_seen\": 30103,\n",
      "    \"gradnorm\": 5.34375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:07.994863\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4853 num samples 30 - rank: 0 num_loss_counted_tokens: 1753\u001b[0m\n",
      "Epoch: 9, Step: 962, Rank: 0, loss = 0.054931640625\n",
      "\n",
      "Epoch 9:  25%|██▌       | 26/104 [00:23<01:11,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 962,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.444336943361066,\n",
      "    \"lr\": 3.0476854889308737e-07,\n",
      "    \"cuda_mem_allocated\": 19.115899085998535,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1824,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.05482456140350877,\n",
      "    \"samples_seen\": 30136,\n",
      "    \"gradnorm\": 5.8125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:08.956248\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4820 num samples 33 - rank: 0 num_loss_counted_tokens: 1691\u001b[0m\n",
      "Epoch: 9, Step: 963, Rank: 0, loss = 0.07666015625\n",
      "\n",
      "Epoch 9:  26%|██▌       | 27/104 [00:24<01:09,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 963,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.052053118907104,\n",
      "    \"lr\": 3.0476854889308737e-07,\n",
      "    \"cuda_mem_allocated\": 25.113325595855713,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1858,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.07642626480086114,\n",
      "    \"samples_seen\": 30169,\n",
      "    \"gradnorm\": 5.8125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:09.818707\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4815 num samples 31 - rank: 0 num_loss_counted_tokens: 1721\u001b[0m\n",
      "Epoch: 9, Step: 964, Rank: 0, loss = 0.0634765625\n",
      "\n",
      "Epoch 9:  27%|██▋       | 28/104 [00:25<01:09,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 964,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.808502972589636,\n",
      "    \"lr\": 2.894146342588977e-07,\n",
      "    \"cuda_mem_allocated\": 19.096746921539307,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1695,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.06371681415929203,\n",
      "    \"samples_seen\": 30200,\n",
      "    \"gradnorm\": 7.0625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:10.771904\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4713 num samples 31 - rank: 0 num_loss_counted_tokens: 1657\u001b[0m\n",
      "Epoch: 9, Step: 965, Rank: 0, loss = 0.048095703125\n",
      "\n",
      "Epoch 9:  28%|██▊       | 29/104 [00:26<01:07,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 965,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.06847898731112,\n",
      "    \"lr\": 2.894146342588977e-07,\n",
      "    \"cuda_mem_allocated\": 25.115958213806152,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1841,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.048071700162954915,\n",
      "    \"samples_seen\": 30233,\n",
      "    \"gradnorm\": 7.0625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:11.631084\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4760 num samples 33 - rank: 0 num_loss_counted_tokens: 1648\u001b[0m\n",
      "Epoch: 9, Step: 966, Rank: 0, loss = 0.03564453125\n",
      "\n",
      "Epoch 9:  29%|██▉       | 30/104 [00:27<01:07,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 966,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.61167904157251,\n",
      "    \"lr\": 2.744518605873092e-07,\n",
      "    \"cuda_mem_allocated\": 19.10392999649048,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1756,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.03559225512528474,\n",
      "    \"samples_seen\": 30263,\n",
      "    \"gradnorm\": 5.5625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:12.588057\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4821 num samples 31 - rank: 0 num_loss_counted_tokens: 1851\u001b[0m\n",
      "Epoch: 9, Step: 967, Rank: 0, loss = 0.038818359375\n",
      "\n",
      "Epoch 9:  30%|██▉       | 31/104 [00:28<01:05,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 967,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.41512624389396,\n",
      "    \"lr\": 2.744518605873092e-07,\n",
      "    \"cuda_mem_allocated\": 25.093215465545654,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1796,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.03869710467706013,\n",
      "    \"samples_seen\": 30295,\n",
      "    \"gradnorm\": 5.5625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:13.440664\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4742 num samples 32 - rank: 0 num_loss_counted_tokens: 1711\u001b[0m\n",
      "Epoch: 9, Step: 968, Rank: 0, loss = 0.029541015625\n",
      "\n",
      "Epoch 9:  31%|███       | 32/104 [00:29<01:06,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 968,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.38418991333974,\n",
      "    \"lr\": 2.5988083057666534e-07,\n",
      "    \"cuda_mem_allocated\": 19.111351490020752,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1668,\n",
      "    \"batch_size\": 28,\n",
      "    \"total_loss\": 0.029526378896882494,\n",
      "    \"samples_seen\": 30323,\n",
      "    \"gradnorm\": 5.28125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:14.402869\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4757 num samples 32 - rank: 0 num_loss_counted_tokens: 1625\u001b[0m\n",
      "Epoch: 9, Step: 969, Rank: 0, loss = 0.050537109375\n",
      "\n",
      "Epoch 9:  32%|███▏      | 33/104 [00:30<01:03,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 969,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.37242657293629,\n",
      "    \"lr\": 2.5988083057666534e-07,\n",
      "    \"cuda_mem_allocated\": 25.09728479385376,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1722,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.050522648083623695,\n",
      "    \"samples_seen\": 30355,\n",
      "    \"gradnorm\": 5.28125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:15.256399\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4697 num samples 31 - rank: 0 num_loss_counted_tokens: 1839\u001b[0m\n",
      "Epoch: 9, Step: 970, Rank: 0, loss = 0.07421875\n",
      "\n",
      "Epoch 9:  33%|███▎      | 34/104 [00:31<01:04,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 970,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.24575650098194,\n",
      "    \"lr\": 2.4570213114592957e-07,\n",
      "    \"cuda_mem_allocated\": 19.12954616546631,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1588,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.07399244332493703,\n",
      "    \"samples_seen\": 30388,\n",
      "    \"gradnorm\": 6.59375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:16.222230\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4795 num samples 29 - rank: 0 num_loss_counted_tokens: 1840\u001b[0m\n",
      "Epoch: 9, Step: 971, Rank: 0, loss = 0.056640625\n",
      "\n",
      "Epoch 9:  34%|███▎      | 35/104 [00:32<01:01,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 971,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.546497532942716,\n",
      "    \"lr\": 2.4570213114592957e-07,\n",
      "    \"cuda_mem_allocated\": 25.08914566040039,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1729,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.05668016194331984,\n",
      "    \"samples_seen\": 30419,\n",
      "    \"gradnorm\": 6.59375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:17.072034\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4814 num samples 33 - rank: 0 num_loss_counted_tokens: 1663\u001b[0m\n",
      "Epoch: 9, Step: 972, Rank: 0, loss = 0.041259765625\n",
      "\n",
      "Epoch 9:  35%|███▍      | 36/104 [00:33<01:02,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 972,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.49427407878792,\n",
      "    \"lr\": 2.3191633341104859e-07,\n",
      "    \"cuda_mem_allocated\": 19.10369110107422,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1592,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04114321608040201,\n",
      "    \"samples_seen\": 30451,\n",
      "    \"gradnorm\": 6.1875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:18.032137\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4832 num samples 31 - rank: 0 num_loss_counted_tokens: 1804\u001b[0m\n",
      "Epoch: 9, Step: 973, Rank: 0, loss = 0.0380859375\n",
      "\n",
      "Epoch 9:  36%|███▌      | 37/104 [00:33<01:00,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 973,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.22450020939376,\n",
      "    \"lr\": 2.3191633341104859e-07,\n",
      "    \"cuda_mem_allocated\": 25.097524642944336,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1721,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.038059267867518884,\n",
      "    \"samples_seen\": 30482,\n",
      "    \"gradnorm\": 6.1875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:18.888778\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4751 num samples 32 - rank: 0 num_loss_counted_tokens: 1640\u001b[0m\n",
      "Epoch: 9, Step: 974, Rank: 0, loss = 0.035400390625\n",
      "\n",
      "Epoch 9:  37%|███▋      | 38/104 [00:34<01:00,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 974,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.16648468940316,\n",
      "    \"lr\": 2.1852399266194312e-07,\n",
      "    \"cuda_mem_allocated\": 19.109196186065674,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1695,\n",
      "    \"batch_size\": 29,\n",
      "    \"total_loss\": 0.035398230088495575,\n",
      "    \"samples_seen\": 30511,\n",
      "    \"gradnorm\": 5.3125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:19.857287\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4844 num samples 33 - rank: 0 num_loss_counted_tokens: 1777\u001b[0m\n",
      "Epoch: 9, Step: 975, Rank: 0, loss = 0.0537109375\n",
      "\n",
      "Epoch 9:  38%|███▊      | 39/104 [00:35<00:58,  1.10it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 975,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.515055618495346,\n",
      "    \"lr\": 2.1852399266194312e-07,\n",
      "    \"cuda_mem_allocated\": 25.097763538360596,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1819,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.05360087960417812,\n",
      "    \"samples_seen\": 30544,\n",
      "    \"gradnorm\": 5.3125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:20.730542\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4737 num samples 30 - rank: 0 num_loss_counted_tokens: 1503\u001b[0m\n",
      "Epoch: 9, Step: 976, Rank: 0, loss = 0.044189453125\n",
      "\n",
      "Epoch 9:  38%|███▊      | 40/104 [00:36<00:59,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 976,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 36.95976626448628,\n",
      "    \"lr\": 2.0552564834014797e-07,\n",
      "    \"cuda_mem_allocated\": 19.102972507476807,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1711,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.04412624196376388,\n",
      "    \"samples_seen\": 30575,\n",
      "    \"gradnorm\": 5.96875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:21.703277\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4753 num samples 30 - rank: 0 num_loss_counted_tokens: 1894\u001b[0m\n",
      "Epoch: 9, Step: 977, Rank: 0, loss = 0.060546875\n",
      "\n",
      "Epoch 9:  39%|███▉      | 41/104 [00:37<00:57,  1.10it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 977,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.3420942873826,\n",
      "    \"lr\": 2.0552564834014797e-07,\n",
      "    \"cuda_mem_allocated\": 25.114283084869385,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1659,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.06057866184448463,\n",
      "    \"samples_seen\": 30608,\n",
      "    \"gradnorm\": 5.96875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:22.580164\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4856 num samples 33 - rank: 0 num_loss_counted_tokens: 1592\u001b[0m\n",
      "Epoch: 9, Step: 978, Rank: 0, loss = 0.047119140625\n",
      "\n",
      "Epoch 9:  40%|████      | 42/104 [00:38<00:57,  1.07it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 978,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 36.64391731253733,\n",
      "    \"lr\": 1.9292182401707603e-07,\n",
      "    \"cuda_mem_allocated\": 19.13050365447998,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1753,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.04706217912150599,\n",
      "    \"samples_seen\": 30638,\n",
      "    \"gradnorm\": 5.625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:23.560397\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4810 num samples 32 - rank: 0 num_loss_counted_tokens: 1797\u001b[0m\n",
      "Epoch: 9, Step: 979, Rank: 0, loss = 0.059814453125\n",
      "\n",
      "Epoch 9:  41%|████▏     | 43/104 [00:39<00:55,  1.10it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 979,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.76586258825867,\n",
      "    \"lr\": 1.9292182401707603e-07,\n",
      "    \"cuda_mem_allocated\": 25.10686159133911,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1691,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.05972797161442933,\n",
      "    \"samples_seen\": 30671,\n",
      "    \"gradnorm\": 5.625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:24.429103\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4763 num samples 32 - rank: 0 num_loss_counted_tokens: 1674\u001b[0m\n",
      "Epoch: 9, Step: 980, Rank: 0, loss = 0.047119140625\n",
      "\n",
      "Epoch 9:  42%|████▏     | 44/104 [00:40<00:55,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 980,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.210678885978474,\n",
      "    \"lr\": 1.8071302737293294e-07,\n",
      "    \"cuda_mem_allocated\": 19.12140703201294,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1721,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.04706565950029053,\n",
      "    \"samples_seen\": 30702,\n",
      "    \"gradnorm\": 6.53125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:25.396188\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4832 num samples 33 - rank: 0 num_loss_counted_tokens: 1763\u001b[0m\n",
      "Epoch: 9, Step: 981, Rank: 0, loss = 0.04052734375\n",
      "\n",
      "Epoch 9:  43%|████▎     | 45/104 [00:41<00:53,  1.10it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 981,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.601233436139346,\n",
      "    \"lr\": 1.8071302737293294e-07,\n",
      "    \"cuda_mem_allocated\": 25.081244945526123,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1657,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.04043452021726011,\n",
      "    \"samples_seen\": 30733,\n",
      "    \"gradnorm\": 6.53125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:26.245559\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4714 num samples 31 - rank: 0 num_loss_counted_tokens: 1798\u001b[0m\n",
      "Epoch: 9, Step: 982, Rank: 0, loss = 0.0546875\n",
      "\n",
      "Epoch 9:  44%|████▍     | 46/104 [00:42<00:53,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 982,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.400565950195826,\n",
      "    \"lr\": 1.6889975017626902e-07,\n",
      "    \"cuda_mem_allocated\": 19.108238697052002,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1648,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.05461165048543689,\n",
      "    \"samples_seen\": 30766,\n",
      "    \"gradnorm\": 6.09375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:27.207227\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4719 num samples 29 - rank: 0 num_loss_counted_tokens: 1683\u001b[0m\n",
      "Epoch: 9, Step: 983, Rank: 0, loss = 0.05615234375\n",
      "\n",
      "Epoch 9:  45%|████▌     | 47/104 [00:43<00:51,  1.10it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 983,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.83660888338085,\n",
      "    \"lr\": 1.6889975017626902e-07,\n",
      "    \"cuda_mem_allocated\": 25.107101440429688,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1851,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.056185845488924906,\n",
      "    \"samples_seen\": 30797,\n",
      "    \"gradnorm\": 6.09375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:28.073070\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4744 num samples 30 - rank: 0 num_loss_counted_tokens: 1761\u001b[0m\n",
      "Epoch: 9, Step: 984, Rank: 0, loss = 0.048828125\n",
      "\n",
      "Epoch 9:  46%|████▌     | 48/104 [00:44<00:51,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 984,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.34327093304947,\n",
      "    \"lr\": 1.574824682641629e-07,\n",
      "    \"cuda_mem_allocated\": 19.10392999649048,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1711,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04880187025131502,\n",
      "    \"samples_seen\": 30829,\n",
      "    \"gradnorm\": 5.5,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:29.037077\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4801 num samples 31 - rank: 0 num_loss_counted_tokens: 1736\u001b[0m\n",
      "Epoch: 9, Step: 985, Rank: 0, loss = 0.047607421875\n",
      "\n",
      "Epoch 9:  47%|████▋     | 49/104 [00:44<00:49,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 985,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.55387472154597,\n",
      "    \"lr\": 1.574824682641629e-07,\n",
      "    \"cuda_mem_allocated\": 25.091779232025146,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1625,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.047692307692307694,\n",
      "    \"samples_seen\": 30861,\n",
      "    \"gradnorm\": 5.5,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:29.887808\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4790 num samples 31 - rank: 0 num_loss_counted_tokens: 1631\u001b[0m\n",
      "Epoch: 9, Step: 986, Rank: 0, loss = 0.052001953125\n",
      "\n",
      "Epoch 9:  48%|████▊     | 50/104 [00:45<00:49,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 986,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.581495993983275,\n",
      "    \"lr\": 1.464616415230702e-07,\n",
      "    \"cuda_mem_allocated\": 19.093156337738037,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1839,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.05193039695486677,\n",
      "    \"samples_seen\": 30892,\n",
      "    \"gradnorm\": 6.125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:30.846006\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4795 num samples 33 - rank: 0 num_loss_counted_tokens: 1663\u001b[0m\n",
      "Epoch: 9, Step: 987, Rank: 0, loss = 0.048828125\n",
      "\n",
      "Epoch 9:  49%|████▉     | 51/104 [00:46<00:47,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 987,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.16084549410695,\n",
      "    \"lr\": 1.464616415230702e-07,\n",
      "    \"cuda_mem_allocated\": 25.100875854492188,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1840,\n",
      "    \"batch_size\": 29,\n",
      "    \"total_loss\": 0.04891304347826087,\n",
      "    \"samples_seen\": 30921,\n",
      "    \"gradnorm\": 6.125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:31.704939\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4801 num samples 33 - rank: 0 num_loss_counted_tokens: 1695\u001b[0m\n",
      "Epoch: 9, Step: 988, Rank: 0, loss = 0.051025390625\n",
      "\n",
      "Epoch 9:  50%|█████     | 52/104 [00:47<00:47,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 988,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.270009713358725,\n",
      "    \"lr\": 1.3583771387028267e-07,\n",
      "    \"cuda_mem_allocated\": 19.121167182922363,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1663,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.05111244738424534,\n",
      "    \"samples_seen\": 30954,\n",
      "    \"gradnorm\": 6.09375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:32.674837\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4841 num samples 33 - rank: 0 num_loss_counted_tokens: 1700\u001b[0m\n",
      "Epoch: 9, Step: 989, Rank: 0, loss = 0.04833984375\n",
      "\n",
      "Epoch 9:  51%|█████     | 53/104 [00:48<00:46,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 989,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.09178307217336,\n",
      "    \"lr\": 1.3583771387028267e-07,\n",
      "    \"cuda_mem_allocated\": 25.109734058380127,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1804,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.048226164079822616,\n",
      "    \"samples_seen\": 30985,\n",
      "    \"gradnorm\": 6.09375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:33.535296\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4785 num samples 33 - rank: 0 num_loss_counted_tokens: 1633\u001b[0m\n",
      "Epoch: 9, Step: 990, Rank: 0, loss = 0.0439453125\n",
      "\n",
      "Epoch 9:  52%|█████▏    | 54/104 [00:49<00:46,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 990,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.34219040610528,\n",
      "    \"lr\": 1.2561111323605714e-07,\n",
      "    \"cuda_mem_allocated\": 19.1060848236084,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1640,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04390243902439024,\n",
      "    \"samples_seen\": 31017,\n",
      "    \"gradnorm\": 5.9375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:34.499308\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4744 num samples 31 - rank: 0 num_loss_counted_tokens: 1758\u001b[0m\n",
      "Epoch: 9, Step: 991, Rank: 0, loss = 0.044189453125\n",
      "\n",
      "Epoch 9:  53%|█████▎    | 55/104 [00:50<00:44,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 991,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.93203631165649,\n",
      "    \"lr\": 1.2561111323605714e-07,\n",
      "    \"cuda_mem_allocated\": 25.1126070022583,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1777,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.0441755768148565,\n",
      "    \"samples_seen\": 31050,\n",
      "    \"gradnorm\": 5.9375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:35.362352\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4748 num samples 29 - rank: 0 num_loss_counted_tokens: 1932\u001b[0m\n",
      "Epoch: 9, Step: 992, Rank: 0, loss = 0.035888671875\n",
      "\n",
      "Epoch 9:  54%|█████▍    | 56/104 [00:51<00:44,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 992,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.499155681964226,\n",
      "    \"lr\": 1.1578225154637579e-07,\n",
      "    \"cuda_mem_allocated\": 19.102733612060547,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1503,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.03592814371257485,\n",
      "    \"samples_seen\": 31080,\n",
      "    \"gradnorm\": 5.59375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:36.323480\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4752 num samples 33 - rank: 0 num_loss_counted_tokens: 1705\u001b[0m\n",
      "Epoch: 9, Step: 993, Rank: 0, loss = 0.036865234375\n",
      "\n",
      "Epoch 9:  55%|█████▍    | 57/104 [00:52<00:42,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 993,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.30489541944674,\n",
      "    \"lr\": 1.1578225154637579e-07,\n",
      "    \"cuda_mem_allocated\": 25.090821743011475,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1894,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.03695881731784583,\n",
      "    \"samples_seen\": 31110,\n",
      "    \"gradnorm\": 5.59375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:37.179122\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4783 num samples 32 - rank: 0 num_loss_counted_tokens: 1756\u001b[0m\n",
      "Epoch: 9, Step: 994, Rank: 0, loss = 0.040283203125\n",
      "\n",
      "Epoch 9:  56%|█████▌    | 58/104 [00:53<00:42,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 994,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.1111251204155,\n",
      "    \"lr\": 1.0635152470635513e-07,\n",
      "    \"cuda_mem_allocated\": 19.131221294403076,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1592,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.04020100502512563,\n",
      "    \"samples_seen\": 31143,\n",
      "    \"gradnorm\": 5.15625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:38.147980\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4780 num samples 32 - rank: 0 num_loss_counted_tokens: 1840\u001b[0m\n",
      "Epoch: 9, Step: 995, Rank: 0, loss = 0.04833984375\n",
      "\n",
      "Epoch 9:  57%|█████▋    | 59/104 [00:54<00:40,  1.10it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 995,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.67365389807467,\n",
      "    \"lr\": 1.0635152470635513e-07,\n",
      "    \"cuda_mem_allocated\": 25.10446786880493,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1797,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.048414023372287146,\n",
      "    \"samples_seen\": 31175,\n",
      "    \"gradnorm\": 5.15625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:39.017006\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4774 num samples 31 - rank: 0 num_loss_counted_tokens: 1911\u001b[0m\n",
      "Epoch: 9, Step: 996, Rank: 0, loss = 0.0517578125\n",
      "\n",
      "Epoch 9:  58%|█████▊    | 60/104 [00:54<00:40,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 996,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.39436595389009,\n",
      "    \"lr\": 9.731931258429638e-08,\n",
      "    \"cuda_mem_allocated\": 19.108957290649414,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1674,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.0516726403823178,\n",
      "    \"samples_seen\": 31207,\n",
      "    \"gradnorm\": 6.0,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:39.980008\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4803 num samples 32 - rank: 0 num_loss_counted_tokens: 1833\u001b[0m\n",
      "Epoch: 9, Step: 997, Rank: 0, loss = 0.0625\n",
      "\n",
      "Epoch 9:  59%|█████▊    | 61/104 [00:55<00:39,  1.10it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 997,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.587406757021725,\n",
      "    \"lr\": 9.731931258429638e-08,\n",
      "    \"cuda_mem_allocated\": 25.109734058380127,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1763,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.06267725467952354,\n",
      "    \"samples_seen\": 31240,\n",
      "    \"gradnorm\": 6.0,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:40.850881\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4757 num samples 31 - rank: 0 num_loss_counted_tokens: 1695\u001b[0m\n",
      "Epoch: 9, Step: 998, Rank: 0, loss = 0.038330078125\n",
      "\n",
      "Epoch 9:  60%|█████▉    | 62/104 [00:56<00:38,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 998,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.48990685463519,\n",
      "    \"lr\": 8.868597899638897e-08,\n",
      "    \"cuda_mem_allocated\": 19.097225666046143,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1798,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.03837597330367074,\n",
      "    \"samples_seen\": 31271,\n",
      "    \"gradnorm\": 6.03125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:41.812337\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4820 num samples 30 - rank: 0 num_loss_counted_tokens: 1963\u001b[0m\n",
      "Epoch: 9, Step: 999, Rank: 0, loss = 0.039794921875\n",
      "\n",
      "Epoch 9:  61%|██████    | 63/104 [00:57<00:37,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 999,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.40249054259633,\n",
      "    \"lr\": 8.868597899638897e-08,\n",
      "    \"cuda_mem_allocated\": 25.08268117904663,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1683,\n",
      "    \"batch_size\": 29,\n",
      "    \"total_loss\": 0.0398098633392751,\n",
      "    \"samples_seen\": 31300,\n",
      "    \"gradnorm\": 6.03125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:42.666023\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4810 num samples 31 - rank: 0 num_loss_counted_tokens: 1770\u001b[0m\n",
      "Epoch: 9, Step: 1000, Rank: 0, loss = 0.058837890625\n",
      "\n",
      "Epoch 9:  62%|██████▏   | 64/104 [00:58<00:36,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1000,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.23505169911727,\n",
      "    \"lr\": 8.04518716920466e-08,\n",
      "    \"cuda_mem_allocated\": 19.104408740997314,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1761,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.05877342419080068,\n",
      "    \"samples_seen\": 31330,\n",
      "    \"gradnorm\": 6.3125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:43.632683\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4777 num samples 32 - rank: 0 num_loss_counted_tokens: 1703\u001b[0m\n",
      "Epoch: 9, Step: 1001, Rank: 0, loss = 0.0546875\n",
      "\n",
      "Epoch 9:  62%|██████▎   | 65/104 [00:59<00:35,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1001,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.33787681485005,\n",
      "    \"lr\": 8.04518716920466e-08,\n",
      "    \"cuda_mem_allocated\": 25.102313995361328,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1736,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.054723502304147464,\n",
      "    \"samples_seen\": 31361,\n",
      "    \"gradnorm\": 6.3125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:44.487584\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4846 num samples 31 - rank: 0 num_loss_counted_tokens: 1778\u001b[0m\n",
      "Epoch: 9, Step: 1002, Rank: 0, loss = 0.04345703125\n",
      "\n",
      "Epoch 9:  63%|██████▎   | 66/104 [01:00<00:34,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1002,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.45774434252062,\n",
      "    \"lr\": 7.261732233991514e-08,\n",
      "    \"cuda_mem_allocated\": 19.1154203414917,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1631,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.043531575720416923,\n",
      "    \"samples_seen\": 31392,\n",
      "    \"gradnorm\": 5.96875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:45.448396\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4739 num samples 31 - rank: 0 num_loss_counted_tokens: 1879\u001b[0m\n",
      "Epoch: 9, Step: 1003, Rank: 0, loss = 0.052978515625\n",
      "\n",
      "Epoch 9:  64%|██████▍   | 67/104 [01:01<00:33,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1003,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.48400108963803,\n",
      "    \"lr\": 7.261732233991514e-08,\n",
      "    \"cuda_mem_allocated\": 25.100875854492188,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1663,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.052916416115454,\n",
      "    \"samples_seen\": 31425,\n",
      "    \"gradnorm\": 5.96875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:46.299821\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4741 num samples 33 - rank: 0 num_loss_counted_tokens: 1617\u001b[0m\n",
      "Epoch: 9, Step: 1004, Rank: 0, loss = 0.0654296875\n",
      "\n",
      "Epoch 9:  65%|██████▌   | 68/104 [01:02<00:33,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1004,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.36929518345009,\n",
      "    \"lr\": 6.51826465144978e-08,\n",
      "    \"cuda_mem_allocated\": 19.118055820465088,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1695,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.06548672566371681,\n",
      "    \"samples_seen\": 31458,\n",
      "    \"gradnorm\": 6.96875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:47.263517\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4798 num samples 30 - rank: 0 num_loss_counted_tokens: 1823\u001b[0m\n",
      "Epoch: 9, Step: 1005, Rank: 0, loss = 0.040771484375\n",
      "\n",
      "Epoch 9:  66%|██████▋   | 69/104 [01:03<00:31,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1005,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.331709478451884,\n",
      "    \"lr\": 6.51826465144978e-08,\n",
      "    \"cuda_mem_allocated\": 25.111889362335205,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1700,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.040882352941176474,\n",
      "    \"samples_seen\": 31491,\n",
      "    \"gradnorm\": 6.96875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:48.117848\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4691 num samples 31 - rank: 0 num_loss_counted_tokens: 1756\u001b[0m\n",
      "Epoch: 9, Step: 1006, Rank: 0, loss = 0.052001953125\n",
      "\n",
      "Epoch 9:  67%|██████▋   | 70/104 [01:04<00:31,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1006,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.456730353302596,\n",
      "    \"lr\": 5.814814368345412e-08,\n",
      "    \"cuda_mem_allocated\": 19.114223957061768,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1633,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.0520514390691978,\n",
      "    \"samples_seen\": 31524,\n",
      "    \"gradnorm\": 6.125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:49.079190\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4729 num samples 30 - rank: 0 num_loss_counted_tokens: 1735\u001b[0m\n",
      "Epoch: 9, Step: 1007, Rank: 0, loss = 0.0498046875\n",
      "\n",
      "Epoch 9:  68%|██████▊   | 71/104 [01:04<00:29,  1.12it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1007,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.67528736396677,\n",
      "    \"lr\": 5.814814368345412e-08,\n",
      "    \"cuda_mem_allocated\": 25.088666915893555,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1758,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.04977246871444824,\n",
      "    \"samples_seen\": 31555,\n",
      "    \"gradnorm\": 6.125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:49.927049\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4827 num samples 30 - rank: 0 num_loss_counted_tokens: 1763\u001b[0m\n",
      "Epoch: 9, Step: 1008, Rank: 0, loss = 0.054443359375\n",
      "\n",
      "Epoch 9:  69%|██████▉   | 72/104 [01:05<00:29,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1008,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.409510069828364,\n",
      "    \"lr\": 5.15140971955308e-08,\n",
      "    \"cuda_mem_allocated\": 19.105366230010986,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1932,\n",
      "    \"batch_size\": 29,\n",
      "    \"total_loss\": 0.05434782608695652,\n",
      "    \"samples_seen\": 31584,\n",
      "    \"gradnorm\": 5.8125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:50.888882\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4699 num samples 31 - rank: 0 num_loss_counted_tokens: 1749\u001b[0m\n",
      "Epoch: 9, Step: 1009, Rank: 0, loss = 0.0419921875\n",
      "\n",
      "Epoch 9:  70%|███████   | 73/104 [01:06<00:27,  1.12it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1009,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.56603618959238,\n",
      "    \"lr\": 5.15140971955308e-08,\n",
      "    \"cuda_mem_allocated\": 25.0905818939209,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1705,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.041935483870967745,\n",
      "    \"samples_seen\": 31617,\n",
      "    \"gradnorm\": 5.8125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:51.738036\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4841 num samples 29 - rank: 0 num_loss_counted_tokens: 1697\u001b[0m\n",
      "Epoch: 9, Step: 1010, Rank: 0, loss = 0.051513671875\n",
      "\n",
      "Epoch 9:  71%|███████   | 74/104 [01:07<00:27,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1010,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.463881716858474,\n",
      "    \"lr\": 4.528077426915412e-08,\n",
      "    \"cuda_mem_allocated\": 19.11374521255493,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1756,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.0515375854214123,\n",
      "    \"samples_seen\": 31649,\n",
      "    \"gradnorm\": 5.8125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:52.698823\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4706 num samples 31 - rank: 0 num_loss_counted_tokens: 1782\u001b[0m\n",
      "Epoch: 9, Step: 1011, Rank: 0, loss = 0.05517578125\n",
      "\n",
      "Epoch 9:  72%|███████▏  | 75/104 [01:08<00:26,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1011,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.3000962461653,\n",
      "    \"lr\": 4.528077426915412e-08,\n",
      "    \"cuda_mem_allocated\": 25.09728479385376,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1840,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.05516304347826087,\n",
      "    \"samples_seen\": 31681,\n",
      "    \"gradnorm\": 5.8125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:53.553869\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4816 num samples 30 - rank: 0 num_loss_counted_tokens: 1988\u001b[0m\n",
      "Epoch: 9, Step: 1012, Rank: 0, loss = 0.046875\n",
      "\n",
      "Epoch 9:  73%|███████▎  | 76/104 [01:09<00:25,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1012,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.26463922348751,\n",
      "    \"lr\": 3.9448425981661876e-08,\n",
      "    \"cuda_mem_allocated\": 19.11159038543701,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1911,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.04683411826268969,\n",
      "    \"samples_seen\": 31712,\n",
      "    \"gradnorm\": 5.875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:54.520175\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4823 num samples 33 - rank: 0 num_loss_counted_tokens: 1698\u001b[0m\n",
      "Epoch: 9, Step: 1013, Rank: 0, loss = 0.062255859375\n",
      "\n",
      "Epoch 9:  74%|███████▍  | 77/104 [01:10<00:24,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1013,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.361422689361504,\n",
      "    \"lr\": 3.9448425981661876e-08,\n",
      "    \"cuda_mem_allocated\": 25.102792739868164,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1833,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.062193126022913256,\n",
      "    \"samples_seen\": 31744,\n",
      "    \"gradnorm\": 5.875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:55.375999\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4770 num samples 31 - rank: 0 num_loss_counted_tokens: 1853\u001b[0m\n",
      "Epoch: 9, Step: 1014, Rank: 0, loss = 0.0400390625\n",
      "\n",
      "Epoch 9:  75%|███████▌  | 78/104 [01:11<00:23,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1014,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.47437323175453,\n",
      "    \"lr\": 3.401728725919373e-08,\n",
      "    \"cuda_mem_allocated\": 19.107521057128906,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1695,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.040117994100294985,\n",
      "    \"samples_seen\": 31775,\n",
      "    \"gradnorm\": 6.15625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:56.337236\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4818 num samples 33 - rank: 0 num_loss_counted_tokens: 1796\u001b[0m\n",
      "Epoch: 9, Step: 1015, Rank: 0, loss = 0.042236328125\n",
      "\n",
      "Epoch 9:  76%|███████▌  | 79/104 [01:12<00:22,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1015,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.37268067018588,\n",
      "    \"lr\": 3.401728725919373e-08,\n",
      "    \"cuda_mem_allocated\": 25.10686159133911,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1963,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.04228222109016811,\n",
      "    \"samples_seen\": 31805,\n",
      "    \"gradnorm\": 6.15625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:57.192190\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4750 num samples 32 - rank: 0 num_loss_counted_tokens: 1693\u001b[0m\n",
      "Epoch: 9, Step: 1016, Rank: 0, loss = 0.0625\n",
      "\n",
      "Epoch 9:  77%|███████▋  | 80/104 [01:13<00:22,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1016,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.39590794577585,\n",
      "    \"lr\": 2.898757686722542e-08,\n",
      "    \"cuda_mem_allocated\": 19.12020969390869,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1770,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.06242937853107345,\n",
      "    \"samples_seen\": 31836,\n",
      "    \"gradnorm\": 5.5,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:58.154717\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4817 num samples 33 - rank: 0 num_loss_counted_tokens: 1672\u001b[0m\n",
      "Epoch: 9, Step: 1017, Rank: 0, loss = 0.04833984375\n",
      "\n",
      "Epoch 9:  78%|███████▊  | 81/104 [01:14<00:20,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1017,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.49222647300966,\n",
      "    \"lr\": 2.898757686722542e-08,\n",
      "    \"cuda_mem_allocated\": 25.096567153930664,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1703,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04844392248972402,\n",
      "    \"samples_seen\": 31868,\n",
      "    \"gradnorm\": 5.5,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:59.006373\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4780 num samples 31 - rank: 0 num_loss_counted_tokens: 1703\u001b[0m\n",
      "Epoch: 9, Step: 1018, Rank: 0, loss = 0.034912109375\n",
      "\n",
      "Epoch 9:  79%|███████▉  | 82/104 [01:14<00:20,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1018,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.24449557717597,\n",
      "    \"lr\": 2.4359497401758026e-08,\n",
      "    \"cuda_mem_allocated\": 19.128827571868896,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1778,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.03487064116985377,\n",
      "    \"samples_seen\": 31899,\n",
      "    \"gradnorm\": 5.96875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:15:59.972370\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4809 num samples 32 - rank: 0 num_loss_counted_tokens: 1654\u001b[0m\n",
      "Epoch: 9, Step: 1019, Rank: 0, loss = 0.035400390625\n",
      "\n",
      "Epoch 9:  80%|███████▉  | 83/104 [01:15<00:18,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1019,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.56938394597341,\n",
      "    \"lr\": 2.4359497401758026e-08,\n",
      "    \"cuda_mem_allocated\": 25.087470531463623,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1879,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.03539116551357105,\n",
      "    \"samples_seen\": 31930,\n",
      "    \"gradnorm\": 5.96875,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:00.823142\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4831 num samples 32 - rank: 0 num_loss_counted_tokens: 1729\u001b[0m\n",
      "Epoch: 9, Step: 1020, Rank: 0, loss = 0.046142578125\n",
      "\n",
      "Epoch 9:  81%|████████  | 84/104 [01:16<00:18,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1020,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.51760411017873,\n",
      "    \"lr\": 2.013323528115674e-08,\n",
      "    \"cuda_mem_allocated\": 19.10369110107422,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1617,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.046072974644403214,\n",
      "    \"samples_seen\": 31963,\n",
      "    \"gradnorm\": 5.375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:01.783582\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4641 num samples 31 - rank: 0 num_loss_counted_tokens: 1707\u001b[0m\n",
      "Epoch: 9, Step: 1021, Rank: 0, loss = 0.043701171875\n",
      "\n",
      "Epoch 9:  82%|████████▏ | 85/104 [01:17<00:17,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1021,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.429206308544785,\n",
      "    \"lr\": 2.013323528115674e-08,\n",
      "    \"cuda_mem_allocated\": 25.101593494415283,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1823,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.043609434997257265,\n",
      "    \"samples_seen\": 31993,\n",
      "    \"gradnorm\": 5.375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:02.636617\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4804 num samples 30 - rank: 0 num_loss_counted_tokens: 1678\u001b[0m\n",
      "Epoch: 9, Step: 1022, Rank: 0, loss = 0.034912109375\n",
      "\n",
      "Epoch 9:  83%|████████▎ | 86/104 [01:18<00:16,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1022,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.62357178257233,\n",
      "    \"lr\": 1.630896073864352e-08,\n",
      "    \"cuda_mem_allocated\": 19.09172010421753,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1756,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.03502277904328018,\n",
      "    \"samples_seen\": 32024,\n",
      "    \"gradnorm\": 5.0625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:03.594410\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 4791 num samples 31 - rank: 0 num_loss_counted_tokens: 1726\u001b[0m\n",
      "Epoch: 9, Step: 1023, Rank: 0, loss = 0.061279296875\n",
      "\n",
      "Epoch 9:  84%|████████▎ | 87/104 [01:19<00:15,  1.12it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1023,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.47950645957665,\n",
      "    \"lr\": 1.630896073864352e-08,\n",
      "    \"cuda_mem_allocated\": 25.08507490158081,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1735,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.06138328530259366,\n",
      "    \"samples_seen\": 32054,\n",
      "    \"gradnorm\": 5.0625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:04.445914\"\n",
      "}\u001b[0m\n",
      "\u001b[96m total length: 2297 num samples 16 - rank: 0 num_loss_counted_tokens: 876\u001b[0m\n",
      "Epoch: 9, Step: 1024, Rank: 0, loss = 0.04345703125\n",
      "\n",
      "Epoch 9:  85%|████████▍ | 88/104 [01:20<00:14,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1024,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.064157535553555,\n",
      "    \"lr\": 1.2886827815440373e-08,\n",
      "    \"cuda_mem_allocated\": 19.124279499053955,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1763,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.04339194554736245,\n",
      "    \"samples_seen\": 32084,\n",
      "    \"gradnorm\": 5.78125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:05.415422\"\n",
      "}\u001b[0m\n",
      "Epoch: 9, Step: 1025, Rank: 0, loss = 0.055419921875\n",
      "\n",
      "Epoch 9:  86%|████████▌ | 89/104 [01:21<00:13,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1025,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.71917576136367,\n",
      "    \"lr\": 1.2886827815440373e-08,\n",
      "    \"cuda_mem_allocated\": 25.077893257141113,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1749,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.05546026300743282,\n",
      "    \"samples_seen\": 32115,\n",
      "    \"gradnorm\": 5.78125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:06.263313\"\n",
      "}\u001b[0m\n",
      "Epoch: 9, Step: 1026, Rank: 0, loss = 0.06787109375\n",
      "\n",
      "Epoch 9:  87%|████████▋ | 90/104 [01:22<00:12,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1026,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.10936027710561,\n",
      "    \"lr\": 9.866974354560966e-09,\n",
      "    \"cuda_mem_allocated\": 19.127631187438965,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1697,\n",
      "    \"batch_size\": 29,\n",
      "    \"total_loss\": 0.06776664702416028,\n",
      "    \"samples_seen\": 32144,\n",
      "    \"gradnorm\": 6.4375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:07.231914\"\n",
      "}\u001b[0m\n",
      "Epoch: 9, Step: 1027, Rank: 0, loss = 0.060791015625\n",
      "\n",
      "Epoch 9:  88%|████████▊ | 91/104 [01:23<00:11,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1027,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.60066090733135,\n",
      "    \"lr\": 9.866974354560966e-09,\n",
      "    \"cuda_mem_allocated\": 25.07956886291504,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1782,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.060886644219977554,\n",
      "    \"samples_seen\": 32175,\n",
      "    \"gradnorm\": 6.4375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:08.081754\"\n",
      "}\u001b[0m\n",
      "Epoch: 9, Step: 1028, Rank: 0, loss = 0.04052734375\n",
      "\n",
      "Epoch 9:  88%|████████▊ | 92/104 [01:24<00:11,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1028,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.22821459369606,\n",
      "    \"lr\": 7.2495219952639636e-09,\n",
      "    \"cuda_mem_allocated\": 19.1216459274292,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1988,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.040492957746478875,\n",
      "    \"samples_seen\": 32205,\n",
      "    \"gradnorm\": 5.65625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:09.048869\"\n",
      "}\u001b[0m\n",
      "Epoch: 9, Step: 1029, Rank: 0, loss = 0.06005859375\n",
      "\n",
      "Epoch 9:  89%|████████▉ | 93/104 [01:24<00:09,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1029,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.943976536887,\n",
      "    \"lr\": 7.2495219952639636e-09,\n",
      "    \"cuda_mem_allocated\": 25.107580184936523,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1698,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.06007067137809187,\n",
      "    \"samples_seen\": 32238,\n",
      "    \"gradnorm\": 5.65625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:09.912798\"\n",
      "}\u001b[0m\n",
      "Epoch: 9, Step: 1030, Rank: 0, loss = 0.057861328125\n",
      "\n",
      "Epoch 9:  90%|█████████ | 94/104 [01:25<00:09,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1030,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.23472114683329,\n",
      "    \"lr\": 5.034576168149175e-09,\n",
      "    \"cuda_mem_allocated\": 19.11063289642334,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1853,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.05774419859686994,\n",
      "    \"samples_seen\": 32269,\n",
      "    \"gradnorm\": 6.34375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:10.879929\"\n",
      "}\u001b[0m\n",
      "Epoch: 9, Step: 1031, Rank: 0, loss = 0.04541015625\n",
      "\n",
      "Epoch 9:  91%|█████████▏| 95/104 [01:26<00:08,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1031,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.9158490682016,\n",
      "    \"lr\": 5.034576168149175e-09,\n",
      "    \"cuda_mem_allocated\": 25.106382846832275,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1796,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.04537861915367483,\n",
      "    \"samples_seen\": 32302,\n",
      "    \"gradnorm\": 6.34375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:11.743228\"\n",
      "}\u001b[0m\n",
      "Epoch: 9, Step: 1032, Rank: 0, loss = 0.0439453125\n",
      "\n",
      "Epoch 9:  92%|█████████▏| 96/104 [01:27<00:07,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1032,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.19509743578943,\n",
      "    \"lr\": 3.22222609090872e-09,\n",
      "    \"cuda_mem_allocated\": 19.105844974517822,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1693,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.04400472533963379,\n",
      "    \"samples_seen\": 32334,\n",
      "    \"gradnorm\": 6.65625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:12.710211\"\n",
      "}\u001b[0m\n",
      "Epoch: 9, Step: 1033, Rank: 0, loss = 0.046630859375\n",
      "\n",
      "Epoch 9:  93%|█████████▎| 97/104 [01:28<00:06,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1033,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.344322883352746,\n",
      "    \"lr\": 3.22222609090872e-09,\n",
      "    \"cuda_mem_allocated\": 25.106143951416016,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1672,\n",
      "    \"batch_size\": 33,\n",
      "    \"total_loss\": 0.04665071770334928,\n",
      "    \"samples_seen\": 32367,\n",
      "    \"gradnorm\": 6.65625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:13.563650\"\n",
      "}\u001b[0m\n",
      "Epoch: 9, Step: 1034, Rank: 0, loss = 0.060546875\n",
      "\n",
      "Epoch 9:  94%|█████████▍| 98/104 [01:29<00:05,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1034,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.338575251355294,\n",
      "    \"lr\": 1.8125447647421302e-09,\n",
      "    \"cuda_mem_allocated\": 19.11302661895752,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1703,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.060481503229594835,\n",
      "    \"samples_seen\": 32398,\n",
      "    \"gradnorm\": 6.53125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:14.527060\"\n",
      "}\u001b[0m\n",
      "Epoch: 9, Step: 1035, Rank: 0, loss = 0.05126953125\n",
      "\n",
      "Epoch 9:  95%|█████████▌| 99/104 [01:30<00:04,  1.11it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1035,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 39.3242997053411,\n",
      "    \"lr\": 1.8125447647421302e-09,\n",
      "    \"cuda_mem_allocated\": 25.104228973388672,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1654,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.051390568319226115,\n",
      "    \"samples_seen\": 32430,\n",
      "    \"gradnorm\": 6.53125,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:15.381309\"\n",
      "}\u001b[0m\n",
      "Epoch: 9, Step: 1036, Rank: 0, loss = 0.0654296875\n",
      "\n",
      "Epoch 9:  96%|█████████▌| 100/104 [01:31<00:03,  1.09it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1036,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 37.049199641481785,\n",
      "    \"lr\": 8.05588971406479e-10,\n",
      "    \"cuda_mem_allocated\": 19.125236988067627,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1729,\n",
      "    \"batch_size\": 32,\n",
      "    \"total_loss\": 0.0653556969346443,\n",
      "    \"samples_seen\": 32462,\n",
      "    \"gradnorm\": 6.09375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:16.351718\"\n",
      "}\u001b[0m\n",
      "Epoch: 9, Step: 1037, Rank: 0, loss = 0.034912109375\n",
      "\n",
      "Epoch 9:  97%|█████████▋| 101/104 [01:32<00:02,  1.10it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1037,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 38.63047662905826,\n",
      "    \"lr\": 8.05588971406479e-10,\n",
      "    \"cuda_mem_allocated\": 25.06400775909424,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1707,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.03485647334504979,\n",
      "    \"samples_seen\": 32493,\n",
      "    \"gradnorm\": 6.09375,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:17.222806\"\n",
      "}\u001b[0m\n",
      "Epoch: 9, Step: 1038, Rank: 0, loss = 0.047119140625\n",
      "\n",
      "Epoch 9:  98%|█████████▊| 102/104 [01:33<00:01,  1.08it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1038,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 36.56536093989443,\n",
      "    \"lr\": 2.0139927093487666e-10,\n",
      "    \"cuda_mem_allocated\": 19.118773460388184,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1678,\n",
      "    \"batch_size\": 30,\n",
      "    \"total_loss\": 0.04707985697258641,\n",
      "    \"samples_seen\": 32523,\n",
      "    \"gradnorm\": 5.5625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:18.204879\"\n",
      "}\u001b[0m\n",
      "Epoch: 9, Step: 1039, Rank: 0, loss = 0.04833984375\n",
      "\n",
      "Epoch 9:  99%|█████████▉| 103/104 [01:34<00:01,  1.11s/it]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1039,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 21.687677613080563,\n",
      "    \"lr\": 2.0139927093487666e-10,\n",
      "    \"cuda_mem_allocated\": 25.099918365478516,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 1726,\n",
      "    \"batch_size\": 31,\n",
      "    \"total_loss\": 0.048377752027809966,\n",
      "    \"samples_seen\": 32554,\n",
      "    \"gradnorm\": 5.5625,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:19.723962\"\n",
      "}\u001b[0m\n",
      "Epoch: 9, Step: 1040, Rank: 0, loss = 0.0361328125\n",
      "\n",
      "Epoch 9: 100%|██████████| 104/104 [01:35<00:00,  1.05it/s]\u001b[A\u001b[92m{\n",
      "    \"epoch\": 9,\n",
      "    \"step\": 1040,\n",
      "    \"rank\": 0,\n",
      "    \"overall_throughput\": 66.72804703556801,\n",
      "    \"lr\": 0.0,\n",
      "    \"cuda_mem_allocated\": 18.51857280731201,\n",
      "    \"cuda_malloc_retries\": 0,\n",
      "    \"num_loss_counted_tokens\": 876,\n",
      "    \"batch_size\": 16,\n",
      "    \"total_loss\": 0.036101598173515985,\n",
      "    \"samples_seen\": 32570,\n",
      "    \"gradnorm\": 6.5,\n",
      "    \"total_samples\": 3257,\n",
      "    \"timestamp\": \"2025-04-12T10:16:20.310772\"\n",
      "}\u001b[0m\n",
      "\u001b[93mSaving model in huggingface format at: samples_32570\u001b[0m\n",
      "[10:16:44] INFO     The model is bigger than the maximum     accelerator.py:2924\n",
      "                    size per checkpoint (5GB) and is going                      \n",
      "                    to be split in 2 checkpoint shards. You                     \n",
      "                    can find where each parameters has been                     \n",
      "                    saved in the index located at                               \n",
      "                    /root/.local/share/instructlab/checkpoin                    \n",
      "                    ts/hf_format/samples_32570/model.safeten                    \n",
      "                    sors.index.json.                                            \n",
      "\u001b[93mModel saved in /root/.local/share/instructlab/checkpoints/hf_format/samples_32570\u001b[0m\n",
      "           INFO     saving took 23.73954939842224 seconds           utils.py:879\n",
      "Epoch 9: 100%|██████████| 104/104 [01:59<00:00,  1.15s/it]\n",
      "\u001b[92mOperation completed successfully! 🎉\u001b[0m\n",
      "Waiting for process to exit, 60s...\n",
      "DEBUG 2025-04-12 10:16:47,833 filelock:331: Attempting to acquire lock 133318148152592 on /root/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock\n",
      "DEBUG 2025-04-12 10:16:47,833 filelock:334: Lock 133318148152592 acquired on /root/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock\n",
      "DEBUG 2025-04-12 10:16:47,833 filelock:364: Attempting to release lock 133318148152592 on /root/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock\n",
      "DEBUG 2025-04-12 10:16:47,833 filelock:367: Lock 133318148152592 released on /root/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock\n",
      "DEBUG 2025-04-12 10:16:47,837 filelock:331: Attempting to acquire lock 133318148155536 on /root/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock\n",
      "DEBUG 2025-04-12 10:16:47,837 filelock:334: Lock 133318148155536 acquired on /root/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock\n",
      "DEBUG 2025-04-12 10:16:47,837 filelock:364: Attempting to release lock 133318148155536 on /root/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock\n",
      "DEBUG 2025-04-12 10:16:47,837 filelock:367: Lock 133318148155536 released on /root/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock\n",
      "Moving the trained model to the directory: data/cybersecurity/new_model/\n",
      "mv: cannot stat '/root/.local/share/instructlab/checkpoints/ggml-model-f16.gguf': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "data_path=\"data/\"+ use_case+\"/ilab_generated/\"\n",
    "train_data=data_path+\"train_gen.jsonl\"\n",
    "model_path=\"models/meta-llama/Llama-3.2-3B-Instruct\"\n",
    "##model_path='/root/.cache/instructlab/models/instructlab/granite-7b-lab'\n",
    "trained_model_path=\"data/\"+ use_case+\"/new_model/\"\n",
    "train_name= 'train_gen.jsonl'\n",
    "test_name= 'test_gen.jsonl'\n",
    "\n",
    "##'Simple (Fast)', 'Accelerated GPU'\n",
    "file_cnt=0\n",
    "for filename in os.listdir(data_path):\n",
    "    if filename[:15]=='train_gen.jsonl': file_cnt+=1\n",
    "    elif filename[:14]=='test_gen.jsonl': file_cnt+=1\n",
    "if file_cnt < 2 or os.path.getsize(gen_directory+train_name) < 5 or os.path.getsize(gen_directory+test_name) < 5:\n",
    "    print(\"ERROR: train_gen.jsonl and/or test.jsonl are not present or too small\")\n",
    "\n",
    "if not os.path.exists(trained_model_path):\n",
    "    print(\"Create directory: \" + trained_model_path)\n",
    "    !mkdir {trained_model_path}\n",
    "ep=int(epoch.value)\n",
    "its=int(it.value)\n",
    "train_pipe.value='Accelerated GPU'\n",
    "if train_pipe.value=='Simple with GPU':\n",
    "    print(\"Train with simple pipeline with a GPU\")\n",
    "    shell_command = f\"ilab model train --pipeline simple --device cuda --model-path {model_path} --data-path {data_path} --num-epochs {ep} --iters {its}\"\n",
    "elif train_pipe.value=='Accelerated GPU':\n",
    "    print(\"Train accelerated with a GPU\")\n",
    "    #shell_command = f\"ilab model train --pipeline accelerated --device cuda --model-path {model_path} --data-path {train_data} --num-epochs {ep} --iters {its}\"\n",
    "    shell_command = f\"ilab -v -v model train --pipeline accelerated --device cuda --model-path {'/content/ilab/'+model_path} --data-path {'/content/ilab/'+train_data} --num-epochs 10 --iters 20 --disable-accelerate-full-state-at-epoch\"\n",
    "\n",
    "print(\"Running: !\"+shell_command)\n",
    "!{shell_command}\n",
    "# if train_pipe.value=='Accelerated GPU':\n",
    "#     print(\"Run ilab model evaluate\")\n",
    "#     !ilab model evaluate --benchmark mmlu\n",
    "#Move the model to the use_case/new_model directory\n",
    "print(\"Moving the trained model to the directory: \"+trained_model_path)\n",
    "!mv /root/.local/share/instructlab/checkpoints/ggml-model-f16.gguf {trained_model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 468784,
     "status": "ok",
     "timestamp": 1744455042976,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "u6EspfHDOp6T",
    "outputId": "d87bc71b-3b80-4e34-ae68-0489caeb311e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied '/root/.local/share/instructlab/checkpoints' to '/content/drive/MyDrive/instructlab_checkpoints'\n"
     ]
    }
   ],
   "source": [
    "# prompt: I need a code that copies the folder /root/.local/share/instructlab/checkpoints to my Google Drive\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define source and destination directories\n",
    "source_dir = \"/root/.local/share/instructlab/checkpoints\"\n",
    "destination_dir = \"/content/drive/MyDrive/instructlab_checkpoints\"  # Or any other path in your Google Drive\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "# Copy the folder and its contents\n",
    "try:\n",
    "    shutil.copytree(source_dir, destination_dir, dirs_exist_ok=True)\n",
    "    print(f\"Successfully copied '{source_dir}' to '{destination_dir}'\")\n",
    "except OSError as e:\n",
    "    print(f\"Error copying folder: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 28483,
     "status": "ok",
     "timestamp": 1744454567203,
     "user": {
      "displayName": "Boris Hnila",
      "userId": "15564967862299816713"
     },
     "user_tz": -120
    },
    "id": "PdjzEscjO2r0",
    "outputId": "f491d148-689b-44f1-ec93-73d8c73f80b6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SogwbM1vxgk",
    "tags": []
   },
   "source": [
    "# Section 4. Inferencing with the Model\n",
    "\n",
    "You have now completed InstructLab training. You can run this section to ask questions to both the base and InstructLab trained models and to compare answers.\n",
    "\n",
    "This third notebook section showcases the generation of synthetic data utilizing InstructLab. It subsequently demonstrates how a large language model (LLM) can be effectively trained on this synthetic dataset. In current notebook, Both the pre-trained LLM and the LLM trained on the generated synthetic data are evaluated against a predefined set of questions to assess their respective performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWNj4u9Vvxgl",
    "tags": []
   },
   "source": [
    "<a id=\"IL3_3\"></a>\n",
    "## Run Interactive Q&A Session with Base and Trained Models to Evaluate Performance\n",
    "\n",
    "Run both base and trained models to compare results with interactive questions and and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoD1Ngrv7ni3"
   },
   "outputs": [],
   "source": [
    "print(\"Do you want to run interactive question on the base and trained models?\")\n",
    "display(questions)\n",
    "print(\"After making your choice, please select and run the following cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AK5l-lIE7oPF"
   },
   "source": [
    "The following are sample questions derived from the data used to generate synthetic data, which was then employed to train the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVp-BeByvxgl"
   },
   "outputs": [],
   "source": [
    "# Define a Function to Perform Inference on Base and Trained Models\n",
    "def model_inference(base_model_path, trained_model_path):\n",
    "    _DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "    Current conversation:\n",
    "    Human: {input}\n",
    "    AI:\"\"\"\n",
    "    base_llm = LlamaCpp(model_path=base_model_path,\n",
    "                   verbose=False,\n",
    "                   n_gpu_layers=25,\n",
    "                   max_tokens=90,\n",
    "                   temperature=0,\n",
    "                   top_k=1\n",
    "                  )\n",
    "    trained_llm = LlamaCpp(model_path=trained_model_path,\n",
    "                   verbose=False,\n",
    "                   n_gpu_layers=25,\n",
    "                   max_tokens=90,\n",
    "                   temperature=0,\n",
    "                   top_k=1\n",
    "                  )\n",
    "    PROMPT = PromptTemplate( input_variables=[\"input\"],\n",
    "                            template=_DEFAULT_TEMPLATE\n",
    "                            )\n",
    "    chain1 = PROMPT | base_llm | StrOutputParser()\n",
    "    chain2 = PROMPT | trained_llm | StrOutputParser()\n",
    "    while True:\n",
    "        question = input(\"Ask me a question (type 'exit' to end): \")\n",
    "        if question.lower() == 'exit':\n",
    "            print(\"Exiting this Q&A session.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"You asked: \", question)\n",
    "            answer1 = chain1.invoke(question)\n",
    "            answer1= answer1.split('Human',1)[0]\n",
    "            print (\"Base Model Answer: \",answer1)\n",
    "            answer2 = chain2.invoke(question)\n",
    "            answer2= answer2.split('Human',1)[0]\n",
    "            print (\"Trained Model Answer: \",answer2)\n",
    "\n",
    "##Display Sample Questions\n",
    "base_model = notebook_dir +\"/models/granite-7b-lab-Q4_K_M.gguf\"\n",
    "trained_model = trained_model_path + \"ggml-model-f16.gguf\"\n",
    "if questions.value=='Yes':\n",
    "  with open(notebook_dir+'/data/' + use_case + '/questions.txt') as f:\n",
    "      for line in f.readlines():\n",
    "          display(widgets.HTML(Norm+line))\n",
    "  print(\"Processing may take several minutes on the first run...\")\n",
    "  model_inference(base_model, trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5l5m7L_LDV1"
   },
   "source": [
    "# Section 5. Download the Trained Model\n",
    " Now that we have a model trained on our dataset, we can download the trained model for futher testing and use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kx65h6u17SW_"
   },
   "outputs": [],
   "source": [
    "print(\"Do you want to download the trained model to your local machine?\")\n",
    "display(download)\n",
    "print(\"After making your selection, please select and run the following cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwwZ4P6n7TDu"
   },
   "source": [
    "Select and run the next cell to download if selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZsjOChUK7gj"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "if download.value=='Yes':\n",
    "  files.download(trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3K4lhpzTvxgl",
    "tags": []
   },
   "source": [
    "<a id=\"IL3_conclusion\"></a>\n",
    "# Conclusion\n",
    "\n",
    "This notebook demonstrated utilizing InstructLab for introducing datasets, data generation, model training, and model creation. This notebook produced an InstructLab trained model that was available for inferecing and downloading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63hBXXa0vxgl",
    "tags": []
   },
   "source": [
    "<a id=\"IL3_learn\"></a>\n",
    "# Learn More\n",
    "\n",
    "InstructLab uses a novel synthetic data-based alignment tuning method for Large Language Models introduced in this [paper](https://arxiv.org/abs/2403.01081).\n",
    "\n",
    "This notebook is based on the InstructLab CLI repository available [here](https://github.com/instructlab/instructlab).\n",
    "\n",
    "Contact us by email to ask questions, discuss potential use cases, or schedule a technical deep dive. The contact email is IBM.Research.JupyterLab@ibm.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poB7nDmcvxgl"
   },
   "source": [
    "© 2025 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "https://github.com/KenOcheltree/ilab-colab/blob/main/running_instructlab_on_GPU.ipynb",
     "timestamp": 1743963809040
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01c1d7674c674814965f15c257155ea0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsModel",
      "_options_labels": [
       "1",
       "3",
       "5",
       "10",
       "20",
       "50",
       "100",
       "200"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ToggleButtonsView",
      "button_style": "",
      "description": "Iterations:",
      "description_tooltip": null,
      "disabled": false,
      "icons": [],
      "index": 4,
      "layout": "IPY_MODEL_5c2e9fc3b8614403b7b5eafd28ed0d49",
      "style": "IPY_MODEL_e745a9162f7441acbb580f00144517d5",
      "tooltips": []
     }
    },
    "160d470d643e421e8acddf35fafc830f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsModel",
      "_options_labels": [
       "Yes",
       "No"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ToggleButtonsView",
      "button_style": "",
      "description": "Live Q&A:",
      "description_tooltip": null,
      "disabled": false,
      "icons": [],
      "index": 1,
      "layout": "IPY_MODEL_def0878813934f31a02af400a3424e83",
      "style": "IPY_MODEL_e7973af9f3894e38816567095419ae47",
      "tooltips": []
     }
    },
    "26d6c19fe1bc406983b928881d3e237a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsModel",
      "_options_labels": [
       "Yes",
       "No"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ToggleButtonsView",
      "button_style": "",
      "description": "Download:",
      "description_tooltip": null,
      "disabled": false,
      "icons": [],
      "index": 1,
      "layout": "IPY_MODEL_a84167deb1ef4c5ca4ba5c2a250dc911",
      "style": "IPY_MODEL_819c4c8a9d984ebab8683fe7178f8cdc",
      "tooltips": []
     }
    },
    "27f83a1a921649999712e0e395240ce1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29053a1b03234e2ca82e72aedd369305": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3742b3381d4a4f2f97ec523a3f504315": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsModel",
      "_options_labels": [
       "Simple with GPU",
       "Accelerated GPU"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ToggleButtonsView",
      "button_style": "",
      "description": "Processing",
      "description_tooltip": null,
      "disabled": false,
      "icons": [],
      "index": 1,
      "layout": "IPY_MODEL_27f83a1a921649999712e0e395240ce1",
      "style": "IPY_MODEL_78f1c61cc82b4338a45f21c434bcdb0c",
      "tooltips": []
     }
    },
    "43e810ff41b34cc4b0fe54ead08af316": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_width": "auto",
      "description_width": "",
      "font_weight": ""
     }
    },
    "48b32b0ad9dc4596b01fa811ceef5dd7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c2e9fc3b8614403b7b5eafd28ed0d49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ce6823c7dda4dd9ad234f77d498ae83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "762e524f95c24b5e9c2d27dc7dc03397": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsModel",
      "_options_labels": [
       "cybersecurity"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ToggleButtonsView",
      "button_style": "",
      "description": "Dataset:",
      "description_tooltip": null,
      "disabled": false,
      "icons": [],
      "index": 0,
      "layout": "IPY_MODEL_48b32b0ad9dc4596b01fa811ceef5dd7",
      "style": "IPY_MODEL_43e810ff41b34cc4b0fe54ead08af316",
      "tooltips": []
     }
    },
    "78f1c61cc82b4338a45f21c434bcdb0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_width": "auto",
      "description_width": "",
      "font_weight": ""
     }
    },
    "7d5516d8ecd044e9a476ace2247890fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsModel",
      "_options_labels": [
       "Simple",
       "Full with GPU"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ToggleButtonsView",
      "button_style": "",
      "description": "Processing:",
      "description_tooltip": null,
      "disabled": false,
      "icons": [],
      "index": 1,
      "layout": "IPY_MODEL_942cca6f88024528b7f68e9f4a9639c0",
      "style": "IPY_MODEL_bf59ab4e421c4630b253ee2038135760",
      "tooltips": []
     }
    },
    "7daefb64b8be42ef9d41d6f4877d3201": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_width": "auto",
      "description_width": "",
      "font_weight": ""
     }
    },
    "819c4c8a9d984ebab8683fe7178f8cdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_width": "auto",
      "description_width": "",
      "font_weight": ""
     }
    },
    "93c8387fae0e4c929296b9e37ee26147": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_width": "auto",
      "description_width": "",
      "font_weight": ""
     }
    },
    "942cca6f88024528b7f68e9f4a9639c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a84167deb1ef4c5ca4ba5c2a250dc911": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac4c3dd67d69464a8b42848a0039455b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsModel",
      "_options_labels": [
       "Simple",
       "Full with GPU"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ToggleButtonsView",
      "button_style": "",
      "description": "Processing:",
      "description_tooltip": null,
      "disabled": false,
      "icons": [],
      "index": 1,
      "layout": "IPY_MODEL_6ce6823c7dda4dd9ad234f77d498ae83",
      "style": "IPY_MODEL_d50074581d1b4733b05bb0b5d4a859cb",
      "tooltips": []
     }
    },
    "b88988ceee1a4bb7bb51570ca29c7710": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsModel",
      "_options_labels": [
       "Default (>450)",
       ">15",
       ">50",
       ">200",
       ">500",
       ">1000"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ToggleButtonsView",
      "button_style": "",
      "description": "# of QNAs:",
      "description_tooltip": null,
      "disabled": false,
      "icons": [],
      "index": 2,
      "layout": "IPY_MODEL_29053a1b03234e2ca82e72aedd369305",
      "style": "IPY_MODEL_7daefb64b8be42ef9d41d6f4877d3201",
      "tooltips": []
     }
    },
    "bf59ab4e421c4630b253ee2038135760": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_width": "auto",
      "description_width": "",
      "font_weight": ""
     }
    },
    "cb9a8f915633416ea6afb8041d6c9325": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d04db31d3e9e46a69f6332c03562f4fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_width": "auto",
      "description_width": "",
      "font_weight": ""
     }
    },
    "d50074581d1b4733b05bb0b5d4a859cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_width": "auto",
      "description_width": "",
      "font_weight": ""
     }
    },
    "de1abbcbf54942909e9e976f1cdb6284": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsModel",
      "_options_labels": [
       "Default (>450)",
       ">15",
       ">50",
       ">200",
       ">500",
       ">1000"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ToggleButtonsView",
      "button_style": "",
      "description": "# of QNAs:",
      "description_tooltip": null,
      "disabled": false,
      "icons": [],
      "index": 2,
      "layout": "IPY_MODEL_e349a0d750634cf08e27de9e9199a900",
      "style": "IPY_MODEL_d04db31d3e9e46a69f6332c03562f4fe",
      "tooltips": []
     }
    },
    "def0878813934f31a02af400a3424e83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e349a0d750634cf08e27de9e9199a900": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e745a9162f7441acbb580f00144517d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_width": "auto",
      "description_width": "",
      "font_weight": ""
     }
    },
    "e7973af9f3894e38816567095419ae47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_width": "auto",
      "description_width": "",
      "font_weight": ""
     }
    },
    "efdae66b8ff84194b5b3f42869d5729d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ToggleButtonsModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ToggleButtonsModel",
      "_options_labels": [
       "1",
       "2",
       "3",
       "4",
       "5",
       "10",
       "15"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ToggleButtonsView",
      "button_style": "",
      "description": "Epochs:",
      "description_tooltip": null,
      "disabled": false,
      "icons": [],
      "index": 5,
      "layout": "IPY_MODEL_cb9a8f915633416ea6afb8041d6c9325",
      "style": "IPY_MODEL_93c8387fae0e4c929296b9e37ee26147",
      "tooltips": []
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
